# 1 "gemm_8x8_blocking_pack_ab_avx.c"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 365 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "gemm_8x8_blocking_pack_ab_avx.c" 2


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 3 4


# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 


 
# 117 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



# 129 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 





# 162 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



# 200 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



# 216 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4


# 231 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
















    #ifdef __ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__
        
        
    #endif



    #ifdef __ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__
        
        
        #define __TV_OS_VERSION_MAX_ALLOWED 120000 
        
        
    #endif



    #ifdef __ENVIRONMENT_WATCH_OS_VERSION_MIN_REQUIRED__
        
        
        #define __WATCH_OS_VERSION_MAX_ALLOWED 50000
        
        
    #endif



    #ifdef __ENVIRONMENT_BRIDGE_OS_VERSION_MIN_REQUIRED__
        
        
        #define __BRIDGE_OS_VERSION_MAX_ALLOWED 20000
        
        
    #endif






    #if 1
        
    #else
        
    #endif










    
    
        #define __IPHONE_OS_VERSION_MAX_ALLOWED     120100
    
    
    




    
        #if 1
            
            
            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_0    __attribute__((availability(ios,introduced=2.0,deprecated=2.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=2.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_1    __attribute__((availability(ios,introduced=2.0,deprecated=2.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=2.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_2    __attribute__((availability(ios,introduced=2.0,deprecated=2.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_2_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=2.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_0    __attribute__((availability(ios,introduced=2.0,deprecated=3.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=3.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_1    __attribute__((availability(ios,introduced=2.0,deprecated=3.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=3.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_2    __attribute__((availability(ios,introduced=2.0,deprecated=3.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_3_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=3.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_0    __attribute__((availability(ios,introduced=2.0,deprecated=4.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=4.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_1    __attribute__((availability(ios,introduced=2.0,deprecated=4.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=4.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_2    __attribute__((availability(ios,introduced=2.0,deprecated=4.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=4.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_3    __attribute__((availability(ios,introduced=2.0,deprecated=4.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_4_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=4.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=2.0,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=2.0,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=2.0,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=2.0,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=2.0,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=2.0,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=2.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=2.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=2.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=2.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=2.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=2.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=2.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=2.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=2.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=2.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=2.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=2.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=2.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=2.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=2.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=2.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=2.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=2.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=2.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=2.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=2.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_2_1                    __attribute__((availability(ios,introduced=2.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_2_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=2.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_2_2    __attribute__((availability(ios,introduced=2.2,deprecated=2.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_2_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=2.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_0    __attribute__((availability(ios,introduced=2.2,deprecated=3.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=3.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_1    __attribute__((availability(ios,introduced=2.2,deprecated=3.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=3.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_2    __attribute__((availability(ios,introduced=2.2,deprecated=3.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_3_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=3.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_0    __attribute__((availability(ios,introduced=2.2,deprecated=4.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=4.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_1    __attribute__((availability(ios,introduced=2.2,deprecated=4.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=4.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_2    __attribute__((availability(ios,introduced=2.2,deprecated=4.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=4.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_3    __attribute__((availability(ios,introduced=2.2,deprecated=4.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_4_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=4.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=2.2,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=2.2,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=2.2,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=2.2,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=2.2,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=2.2,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=2.2,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=2.2,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=2.2,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=2.2,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=2.2,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=2.2,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=2.2,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=2.2,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=2.2,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=2.2,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=2.2,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=2.2,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=2.2,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=2.2,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=2.2,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=2.2,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=2.2,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=2.2,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=2.2,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=2.2,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=2.2,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_2_2_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=2.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_3_0                    __attribute__((availability(ios,introduced=3.0)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_3_0_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=3.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_3_1    __attribute__((availability(ios,introduced=3.1,deprecated=3.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_3_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=3.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_3_2    __attribute__((availability(ios,introduced=3.1,deprecated=3.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_3_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=3.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_0    __attribute__((availability(ios,introduced=3.1,deprecated=4.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=4.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_1    __attribute__((availability(ios,introduced=3.1,deprecated=4.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=4.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_2    __attribute__((availability(ios,introduced=3.1,deprecated=4.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=4.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_3    __attribute__((availability(ios,introduced=3.1,deprecated=4.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_4_3_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=4.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=3.1,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=3.1,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=3.1,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=3.1,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=3.1,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=3.1,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=3.1,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=3.1,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=3.1,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=3.1,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=3.1,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=3.1,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=3.1,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=3.1,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=3.1,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=3.1,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=3.1,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=3.1,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=3.1,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=3.1,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=3.1,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=3.1,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=3.1,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=3.1,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=3.1,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=3.1,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=3.1,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_3_1_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=3.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_3_2                    __attribute__((availability(ios,introduced=3.2)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_3_2_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=3.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_0    __attribute__((availability(ios,introduced=4.0,deprecated=4.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=4.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_1    __attribute__((availability(ios,introduced=4.0,deprecated=4.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=4.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_2    __attribute__((availability(ios,introduced=4.0,deprecated=4.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=4.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_3    __attribute__((availability(ios,introduced=4.0,deprecated=4.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_4_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=4.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=4.0,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=4.0,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=4.0,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=4.0,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=4.0,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=4.0,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=4.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=4.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=4.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=4.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=4.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=4.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=4.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=4.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=4.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=4.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=4.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=4.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=4.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=4.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=4.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=4.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=4.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=4.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=4.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=4.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=4.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_1                    __attribute__((availability(ios,introduced=4.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=4.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_4_2    __attribute__((availability(ios,introduced=4.2,deprecated=4.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_4_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=4.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_4_3    __attribute__((availability(ios,introduced=4.2,deprecated=4.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_4_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=4.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=4.2,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=4.2,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=4.2,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=4.2,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=4.2,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=4.2,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=4.2,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=4.2,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=4.2,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=4.2,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=4.2,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=4.2,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=4.2,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=4.2,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=4.2,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=4.2,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=4.2,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=4.2,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=4.2,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=4.2,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=4.2,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=4.2,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=4.2,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=4.2,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=4.2,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=4.2,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=4.2,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_4_2_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=4.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_3                    __attribute__((availability(ios,introduced=4.3)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_4_3_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=4.3)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_5_0    __attribute__((availability(ios,introduced=5.0,deprecated=5.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_5_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=5.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_5_1    __attribute__((availability(ios,introduced=5.0,deprecated=5.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_5_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=5.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=5.0,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=5.0,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=5.0,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=5.0,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=5.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=5.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=5.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=5.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=5.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=5.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=5.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=5.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=5.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=5.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=5.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=5.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=5.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=5.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=5.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=5.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=5.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=5.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=5.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=5.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=5.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_5_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=5.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_5_1                    __attribute__((availability(ios,introduced=5.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_5_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=5.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_6_0    __attribute__((availability(ios,introduced=6.0,deprecated=6.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_6_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=6.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_6_1    __attribute__((availability(ios,introduced=6.0,deprecated=6.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_6_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=6.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=6.0,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=6.0,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=6.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=6.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=6.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=6.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=6.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=6.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=6.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=6.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=6.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=6.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=6.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=6.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=6.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=6.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=6.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=6.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=6.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=6.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=6.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=6.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=6.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_6_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=6.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_6_1                    __attribute__((availability(ios,introduced=6.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_6_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=6.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_7_0    __attribute__((availability(ios,introduced=7.0,deprecated=7.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_7_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=7.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_7_1    __attribute__((availability(ios,introduced=7.0,deprecated=7.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_7_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=7.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=7.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=7.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=7.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=7.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=7.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=7.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=7.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=7.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=7.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=7.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=7.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=7.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=7.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=7.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=7.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=7.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=7.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=7.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=7.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=7.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=7.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_7_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=7.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_7_1                    __attribute__((availability(ios,introduced=7.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_7_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=7.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_0    __attribute__((availability(ios,introduced=8.0,deprecated=8.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=8.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_1    __attribute__((availability(ios,introduced=8.0,deprecated=8.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=8.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=8.0,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=8.0,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=8.0,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=8.0,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=8.0,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=8.0,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=8.0,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=8.0,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=8.0,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=8.0,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=8.0,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=8.0,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=8.0,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=8.0,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=8.0,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=8.0,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=8.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=8.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=8.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_1                    __attribute__((availability(ios,introduced=8.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=8.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_2    __attribute__((availability(ios,introduced=8.2,deprecated=8.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=8.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_3    __attribute__((availability(ios,introduced=8.2,deprecated=8.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=8.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=8.2,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=8.2,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=8.2,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=8.2,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=8.2,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=8.2,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=8.2,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=8.2,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=8.2,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=8.2,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=8.2,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=8.2,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=8.2,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=8.2,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=8.2,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=8.2,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.2,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_2_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=8.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_3                    __attribute__((availability(ios,introduced=8.3)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_3_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=8.3)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_8_4    __attribute__((availability(ios,introduced=8.4,deprecated=8.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_8_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=8.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_0    __attribute__((availability(ios,introduced=8.4,deprecated=9.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=9.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=8.4,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=8.4,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=8.4,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=8.4,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=8.4,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=8.4,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=8.4,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=8.4,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=8.4,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=8.4,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=8.4,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=8.4,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=8.4,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=8.4,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=8.4,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_8_4_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=8.4)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_0                    __attribute__((availability(ios,introduced=9.0)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_0_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=9.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_1    __attribute__((availability(ios,introduced=9.1,deprecated=9.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=9.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_2    __attribute__((availability(ios,introduced=9.1,deprecated=9.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_2_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=9.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=9.1,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=9.1,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=9.1,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=9.1,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=9.1,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=9.1,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=9.1,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=9.1,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=9.1,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=9.1,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=9.1,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=9.1,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.1,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_1_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=9.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_2                    __attribute__((availability(ios,introduced=9.2)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_2_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=9.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_9_3    __attribute__((availability(ios,introduced=9.3,deprecated=9.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_9_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=9.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_0    __attribute__((availability(ios,introduced=9.3,deprecated=10.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=10.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=9.3,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=9.3,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=9.3,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=9.3,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=9.3,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=9.3,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=9.3,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=9.3,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=9.3,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=9.3,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=9.3,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_9_3_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=9.3)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_0                    __attribute__((availability(ios,introduced=10.0)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_0_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=10.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_1    __attribute__((availability(ios,introduced=10.1,deprecated=10.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_1_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=10.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_2    __attribute__((availability(ios,introduced=10.1,deprecated=10.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_2_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=10.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=10.1,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=10.1,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=10.1,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=10.1,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=10.1,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=10.1,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=10.1,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=10.1,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=10.1,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_1_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=10.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_2                    __attribute__((availability(ios,introduced=10.2)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_2_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=10.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_10_3    __attribute__((availability(ios,introduced=10.3,deprecated=10.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_10_3_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=10.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_0    __attribute__((availability(ios,introduced=10.3,deprecated=11.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_0_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=11.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=10.3,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=10.3,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=10.3,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=10.3,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=10.3,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=10.3,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=10.3,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_10_3_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=10.3)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_0                    __attribute__((availability(ios,introduced=11.0)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_0_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=11.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_1    __attribute__((availability(ios,introduced=11.1,deprecated=11.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_1_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=11.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_2    __attribute__((availability(ios,introduced=11.1,deprecated=11.2)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_2_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=11.2,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=11.1,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=11.1,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=11.1,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=11.1,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=11.1,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_1_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=11.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_2                    __attribute__((availability(ios,introduced=11.2)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_2_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=11.2)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_11_3    __attribute__((availability(ios,introduced=11.3,deprecated=11.3)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_11_3_MSG(_msg)    __attribute__((availability(ios,introduced=11.3,deprecated=11.3,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_11_4    __attribute__((availability(ios,introduced=11.3,deprecated=11.4)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_11_4_MSG(_msg)    __attribute__((availability(ios,introduced=11.3,deprecated=11.4,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=11.3,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=11.3,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=11.3,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=11.3,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_11_3_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=11.3)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_4                    __attribute__((availability(ios,introduced=11.4)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_11_4_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=11.4)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_12_0_DEP__IPHONE_12_0    __attribute__((availability(ios,introduced=12.0,deprecated=12.0)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_12_0_DEP__IPHONE_12_0_MSG(_msg)    __attribute__((availability(ios,introduced=12.0,deprecated=12.0,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_12_0_DEP__IPHONE_12_1    __attribute__((availability(ios,introduced=12.0,deprecated=12.1)))
            
                    #define __AVAILABILITY_INTERNAL__IPHONE_12_0_DEP__IPHONE_12_1_MSG(_msg)    __attribute__((availability(ios,introduced=12.0,deprecated=12.1,message=_msg)))
            


            #define __AVAILABILITY_INTERNAL__IPHONE_12_0_DEP__IPHONE_NA               __attribute__((availability(ios,introduced=12.0)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_12_1                    __attribute__((availability(ios,introduced=12.1)))
            
            #if 1
                    
            #else
                    
            #endif
            
            #define __AVAILABILITY_INTERNAL__IPHONE_12_1_DEP__IPHONE_NA_MSG(_msg)     __attribute__((availability(ios,introduced=12.1)))
            
            #define __AVAILABILITY_INTERNAL__IPHONE_NA_DEP__IPHONE_NA                __attribute__((availability(ios,unavailable)))
            
        #endif
    

    
# 37694 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4


# 40084 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101302
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101302
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101300
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101300
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101202
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101202
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101200
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101200
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101103
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101103
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101100
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101100
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 101002
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 101002
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_10        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 1090
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 1090
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_8        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 1070
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 1070
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_6        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 1050
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 1050
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_4        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 1030
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 1030
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_2        __attribute__((unavailable))
        




        #if __MAC_OS_X_VERSION_MAX_ALLOWED < 1010
            
        #elif __MAC_OS_X_VERSION_MIN_REQUIRED < 1010
            
        #else
            
        #endif
        
            #define __AVAILABILITY_INTERNAL__MAC_10_0        __attribute__((unavailable))
        




        #define __AVAILABILITY_INTERNAL__MAC_NA             __attribute__((unavailable))
        





            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_1              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_1              
            
        #endif
        







            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_2              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_2              __AVAILABILITY_INTERNAL__MAC_10_2
            
        #endif
        
# 40289 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_3              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_3              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_3              
            
        #endif
        
# 40310 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_4              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_4              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_4              __AVAILABILITY_INTERNAL__MAC_10_4
            
        #endif
        
# 40335 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_5              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_5              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_5              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_5              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_5              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_5              
            
        #endif
        
# 40364 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_6              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_6              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_6              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_6              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_6              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_6              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_6              __AVAILABILITY_INTERNAL__MAC_10_6
            
        #endif
        
# 40397 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_7              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_7              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_7              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_7              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_7              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_7              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_7              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_7              
            
        #endif
        
# 40434 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_8              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_8              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_8              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_8              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_8              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_8              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_8              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_8              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_8              __AVAILABILITY_INTERNAL__MAC_10_8
            
        #endif
        
# 40475 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_9              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_9              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_9              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_9              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_9              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_9              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_9              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_9              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_9              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_9              
            
        #endif
        
# 40520 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_10              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_10              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_10              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_10              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_10              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_10              __AVAILABILITY_INTERNAL__MAC_10_10
            
        #endif
        
# 40569 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_10_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_10_2              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_10_2              
            
        #endif
        
# 40622 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_10_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_10_3              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
        #endif
        
# 40679 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_11              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_11              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_11              
            
        #endif
        
# 40740 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_11_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_11_2              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
        #endif
        
# 40805 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_11_3              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_11_3              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_11_3              
            
        #endif
        
# 40874 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_11_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_11_4              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
        #endif
        
# 40947 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_12              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_12              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_12              
            
        #endif
        
# 41024 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_12_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_12_1              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
        #endif
        
# 41105 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_12_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_12_2              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_12_2              
            
        #endif
        
# 41190 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_12_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_12_4              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
        #endif
        
# 41279 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_13              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_13              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_13              
            
        #endif
        
# 41372 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_13_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_10_13_1              __AVAILABILITY_INTERNAL__MAC_10_13_1
            
        #endif
        
# 41469 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_13_2              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_10_13_2              __AVAILABILITY_INTERNAL__MAC_10_13_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_2_DEP__MAC_10_13_2              
            
        #endif
        
# 41570 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_13_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_2_DEP__MAC_10_13_4              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_4_DEP__MAC_10_13_4              __AVAILABILITY_INTERNAL__MAC_10_13_4
            
        #endif
        
# 41675 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_13_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_2_DEP__MAC_10_14              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_4_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_13_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_14_DEP__MAC_10_14              __AVAILABILITY_INTERNAL__MAC_10_14
            
        #endif
        
# 41784 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4
            #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_0
            
            #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_6
            
            #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_8
            
            #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_10
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_10_3
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_11_2
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_11_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_12_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_12_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_13_1
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_2_DEP__MAC_10_14_1              
            
            #define __AVAILABILITY_INTERNAL__MAC_10_13_4_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_13_4
            
            #define __AVAILABILITY_INTERNAL__MAC_10_14_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_14
            
            #define __AVAILABILITY_INTERNAL__MAC_10_14_1_DEP__MAC_10_14_1              __AVAILABILITY_INTERNAL__MAC_10_14_1
            
        #endif
        
        #define __AVAILABILITY_INTERNAL__MAC_10_0_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_0
        
        #define __AVAILABILITY_INTERNAL__MAC_10_1_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_2_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_2
        
        #define __AVAILABILITY_INTERNAL__MAC_10_3_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_4_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_4
        
        #define __AVAILABILITY_INTERNAL__MAC_10_5_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_6_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_6
        
        #define __AVAILABILITY_INTERNAL__MAC_10_7_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_8_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_8
        
        #define __AVAILABILITY_INTERNAL__MAC_10_9_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_10_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_10
        
        #define __AVAILABILITY_INTERNAL__MAC_10_10_2_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_10_3_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_10_3
        
        #define __AVAILABILITY_INTERNAL__MAC_10_11_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_11_2_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_11_2
        
        #define __AVAILABILITY_INTERNAL__MAC_10_11_3_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_11_4_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_11_4
        
        #define __AVAILABILITY_INTERNAL__MAC_10_12_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_12_1_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_12_1
        
        #define __AVAILABILITY_INTERNAL__MAC_10_12_2_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_12_4_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_12_4
        
        #define __AVAILABILITY_INTERNAL__MAC_10_13_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_13_1_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_13_1
        
        #define __AVAILABILITY_INTERNAL__MAC_10_13_2_DEP__MAC_NA_MSG(_msg)   
        
        #define __AVAILABILITY_INTERNAL__MAC_10_13_4_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_13_4
        
        #define __AVAILABILITY_INTERNAL__MAC_10_14_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_14
        
        #define __AVAILABILITY_INTERNAL__MAC_10_14_1_DEP__MAC_NA_MSG(_msg)   __AVAILABILITY_INTERNAL__MAC_10_14_1
        
        #define __AVAILABILITY_INTERNAL__MAC_NA_DEP__MAC_NA_MSG(_msg)     __attribute__((unavailable))
    









 #if 1

    
    
    #define __API_AVAILABLE_PLATFORM_macosx(x) macosx,introduced=x
    
    #define __API_AVAILABLE_PLATFORM_watchos(x) watchos,introduced=x
    
    

    
    #define __API_AVAILABLE1(x) __attribute__((availability(__API_AVAILABLE_PLATFORM_##x)))
    
    #define __API_AVAILABLE3(x,y,z)  __attribute__((availability(__API_AVAILABLE_PLATFORM_##x))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##y))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##z)))
    
    #define __API_AVAILABLE5(x,y,z,t,b) __attribute__((availability(__API_AVAILABLE_PLATFORM_##x))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##y))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##z))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##t))) __attribute__((availability(__API_AVAILABLE_PLATFORM_##b)))
    
    #define __API_AVAILABLE_GET_MACRO(_1,_2,_3,_4,_5,_6,NAME,...) NAME

    
    
    #define __API_DEPRECATED_PLATFORM_macosx(x,y) macosx,introduced=x,deprecated=y
    
    #define __API_DEPRECATED_PLATFORM_watchos(x,y) watchos,introduced=x,deprecated=y
    
    

    
    #define __API_DEPRECATED_MSG2(msg,x) __attribute__((availability(__API_DEPRECATED_PLATFORM_##x,message=msg)))
    
    #define __API_DEPRECATED_MSG4(msg,x,y,z) __attribute__((availability(__API_DEPRECATED_PLATFORM_##x,message=msg))) __attribute__((availability(__API_DEPRECATED_PLATFORM_##y,message=msg))) __attribute__((availability(__API_DEPRECATED_PLATFORM_##z,message=msg)))
    
    #define __API_DEPRECATED_MSG6(msg,x,y,z,t,b) __API_DEPRECATED_MSG4(msg,x,y,z) __attribute__((availability(__API_DEPRECATED_PLATFORM_##t,message=msg))) __attribute__((availability(__API_DEPRECATED_PLATFORM_##b,message=msg)))
    
    #define __API_DEPRECATED_MSG_GET_MACRO(_1,_2,_3,_4,_5,_6,_7,NAME,...) NAME

    
        #define __API_R(rep,x) __attribute__((availability(__API_DEPRECATED_PLATFORM_##x,replacement=rep)))
    



    
    #define __API_DEPRECATED_REP3(rep,x,y) __API_R(rep,x) __API_R(rep,y)
    
    #define __API_DEPRECATED_REP5(rep,x,y,z,t) __API_DEPRECATED_REP3(rep,x,y) __API_R(rep,z) __API_R(rep,t)
    
    #define __API_DEPRECATED_REP7(rep,x,y,z,t,b,m) __API_DEPRECATED_REP5(rep,x,y,z,t) __API_R(rep,b) __API_R(rep,m)
    

    







    
    #define __API_UNAVAILABLE_PLATFORM_macosx macosx,unavailable
    
    #define __API_UNAVAILABLE_PLATFORM_watchos watchos,unavailable
    
    

    
    #define __API_UNAVAILABLE1(x) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##x)))
    
    #define __API_UNAVAILABLE3(x,y,z) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##x))) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##y))) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##z)))
    
    #define __API_UNAVAILABLE5(x,y,z,t,b) __API_UNAVAILABLE3(x,y,z) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##t))) __attribute__((availability(__API_UNAVAILABLE_PLATFORM_##b)))
    
    #define __API_UNAVAILABLE_GET_MACRO(_1,_2,_3,_4,_5,_6,NAME,...) NAME
 
# 42000 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4


# 42011 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/AvailabilityInternal.h" 3 4

 





    #define __swift_compiler_version_at_least(...) 1





 

  #define __SPI_AVAILABLE(...)



# 233 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 2 3 4


    #define __OSX_AVAILABLE_STARTING(_osx, _ios) __AVAILABILITY_INTERNAL##_ios
    

    #define __OSX_AVAILABLE_BUT_DEPRECATED_MSG(_osxIntro, _osxDep, _iosIntro, _iosDep, _msg)                                                      __AVAILABILITY_INTERNAL##_iosIntro##_DEP##_iosDep##_MSG(_msg)


# 253 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



  #if 1
    
    #define __OS_AVAILABILITY_MSG(_target, _availability, _msg)  __attribute__((availability(_target,_availability,message=_msg)))
  
# 270 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4




  #if 1
    
    #define __IOS_EXTENSION_UNAVAILABLE(_msg)  __OS_AVAILABILITY_MSG(ios_app_extension,unavailable,_msg)
  




    #define __OS_AVAILABILITY_MSG(macosx_app_extension,unavailable,_msg)
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) __OS_AVAILABILITY_MSG(ios,deprecated=_dep,_msg)
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) __OS_AVAILABILITY_MSG(tvos,deprecated=_dep,_msg)
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) __OS_AVAILABILITY_MSG(watchos,deprecated=_dep,_msg)
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         __OS_AVAILABILITY_MSG(swift,unavailable,_msg)
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define __API_DEPRECATED_WITH_REPLACEMENT(...) __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 62 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4



# 65 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4
























# 103 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4









# 131 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4

















































	#define __deprecated_msg(_msg) __attribute__((deprecated(_msg)))





	#define __deprecated_enum_msg(_msg) __deprecated_msg(_msg)






























# 236 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4













































# 294 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4



# 311 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4















# 341 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4




















































# 404 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4










# 448 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4









# 473 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4










# 505 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4


# 527 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4


# 537 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4
















# 561 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4


# 571 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4






































# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_symbol_aliasing.h" 1 3 4
# 26 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_symbol_aliasing.h" 3 4

































































































































































































































































































































































































# 609 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 2 3 4











# 637 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4
















# 664 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 3 4











# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_posix_availability.h" 1 3 4
# 26 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_posix_availability.h" 3 4

















































# 675 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/cdefs.h" 2 3 4





















































































































# 63 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types.h" 3 4







# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_types.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_types.h" 3 4










typedef __signed char		__int8_t;



typedef unsigned char		__uint8_t;
typedef	short			__int16_t;
typedef	unsigned short		__uint16_t;
typedef int			__int32_t;
typedef unsigned int		__uint32_t;
typedef long long		__int64_t;
typedef unsigned long long	__uint64_t;

typedef long			__darwin_intptr_t;
typedef unsigned int		__darwin_natural_t;


# 68 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_types.h" 3 4


typedef int			__darwin_ct_rune_t;	





typedef union {
	char		__mbstate8[128];
	long long	_mbstateL;			
} __mbstate_t;

typedef __mbstate_t		__darwin_mbstate_t;	


typedef long int	__darwin_ptrdiff_t;	







typedef long unsigned int		__darwin_size_t;	





typedef __builtin_va_list	__darwin_va_list;	





typedef int		__darwin_wchar_t;	




typedef __darwin_wchar_t	__darwin_rune_t;	


typedef int		__darwin_wint_t;	




typedef unsigned long		__darwin_clock_t;	
typedef __uint32_t		__darwin_socklen_t;	
typedef long			__darwin_ssize_t;	
typedef long			__darwin_time_t;	


# 33 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_types.h" 2 3 4





# 34 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types.h" 2 3 4








# 54 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types.h" 3 4

typedef	__int64_t	__darwin_blkcnt_t;	
typedef	__int32_t	__darwin_blksize_t;	
typedef __int32_t	__darwin_dev_t;		
typedef unsigned int	__darwin_fsblkcnt_t;	
typedef unsigned int	__darwin_fsfilcnt_t;	
typedef __uint32_t	__darwin_gid_t;		
typedef __uint32_t	__darwin_id_t;		
typedef __uint64_t	__darwin_ino64_t;	

typedef __darwin_ino64_t __darwin_ino_t;	



typedef __darwin_natural_t __darwin_mach_port_name_t; 
typedef __darwin_mach_port_name_t __darwin_mach_port_t; 
typedef __uint16_t	__darwin_mode_t;	
typedef __int64_t	__darwin_off_t;		
typedef __int32_t	__darwin_pid_t;		
typedef __uint32_t	__darwin_sigset_t;	
typedef __int32_t	__darwin_suseconds_t;	
typedef __uint32_t	__darwin_uid_t;		
typedef __uint32_t	__darwin_useconds_t;	
typedef	unsigned char	__darwin_uuid_t[16];
typedef	char	__darwin_uuid_string_t[37];


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_pthread/_pthread_types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_pthread/_pthread_types.h" 3 4







// pthread opaque structures

# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_pthread/_pthread_types.h" 3 4

struct __darwin_pthread_handler_rec {
	void (*__routine)(void *);	// Routine to call
	void *__arg;			// Argument to pass
	struct __darwin_pthread_handler_rec *__next;
};

struct _opaque_pthread_attr_t {
	long __sig;
	char __opaque[56];
};

struct _opaque_pthread_cond_t {
	long __sig;
	char __opaque[40];
};

struct _opaque_pthread_condattr_t {
	long __sig;
	char __opaque[8];
};

struct _opaque_pthread_mutex_t {
	long __sig;
	char __opaque[56];
};

struct _opaque_pthread_mutexattr_t {
	long __sig;
	char __opaque[8];
};

struct _opaque_pthread_once_t {
	long __sig;
	char __opaque[8];
};

struct _opaque_pthread_rwlock_t {
	long __sig;
	char __opaque[192];
};

struct _opaque_pthread_rwlockattr_t {
	long __sig;
	char __opaque[16];
};

struct _opaque_pthread_t {
	long __sig;
	struct __darwin_pthread_handler_rec  *__cleanup_stack;
	char __opaque[8176];
};

typedef struct _opaque_pthread_attr_t __darwin_pthread_attr_t;
typedef struct _opaque_pthread_cond_t __darwin_pthread_cond_t;
typedef struct _opaque_pthread_condattr_t __darwin_pthread_condattr_t;
typedef unsigned long __darwin_pthread_key_t;
typedef struct _opaque_pthread_mutex_t __darwin_pthread_mutex_t;
typedef struct _opaque_pthread_mutexattr_t __darwin_pthread_mutexattr_t;
typedef struct _opaque_pthread_once_t __darwin_pthread_once_t;
typedef struct _opaque_pthread_rwlock_t __darwin_pthread_rwlock_t;
typedef struct _opaque_pthread_rwlockattr_t __darwin_pthread_rwlockattr_t;
typedef struct _opaque_pthread_t *__darwin_pthread_t;


# 81 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types.h" 2 3 4









# 28 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types.h" 2 3 4



# 39 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types.h" 3 4

typedef	int		__darwin_nl_item;
typedef	int		__darwin_wctrans_t;

typedef	__uint32_t	__darwin_wctype_t;


























# 65 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4

# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 3 4



# 62 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 3 4

 















typedef enum {
	P_ALL,
	P_PID,
	P_PGID
} idtype_t;






# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_pid_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_pid_t.h" 3 4




typedef __darwin_pid_t        pid_t; 

# 90 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_id_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_id_t.h" 3 4




typedef __darwin_id_t	id_t;		

# 91 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 2 3 4


# 108 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4



# 67 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4







# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/appleapiopts.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/appleapiopts.h" 3 4





































# 74 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 75 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4








# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/signal.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/signal.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/signal.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/signal.h" 3 4












typedef int sig_atomic_t; 






# 33 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/signal.h" 2 3 4





# 83 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4


# 124 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4








# 142 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_mcontext.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_mcontext.h" 3 4



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_mcontext.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_mcontext.h" 3 4








# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/machine/_structs.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/machine/_structs.h" 3 4







# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4










# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/types.h" 3 4









# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 3 4





# 64 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 3 4













# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int8_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int8_t.h" 3 4



typedef	__signed char		int8_t;

# 77 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int16_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int16_t.h" 3 4



typedef	short			int16_t;

# 78 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int32_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int32_t.h" 3 4



typedef	int			int32_t;

# 79 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int64_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_int64_t.h" 3 4



typedef	long long		int64_t;

# 80 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int8_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int8_t.h" 3 4



typedef	unsigned char		u_int8_t;

# 82 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int16_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int16_t.h" 3 4



typedef	unsigned short			u_int16_t;

# 83 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int32_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int32_t.h" 3 4



typedef	unsigned int		u_int32_t;

# 84 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int64_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_u_int64_t.h" 3 4



typedef	unsigned long long	u_int64_t;

# 85 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4


typedef int64_t			register_t;





# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_intptr_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_intptr_t.h" 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/types.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/types.h" 3 4





# 31 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_intptr_t.h" 2 3 4

typedef __darwin_intptr_t	intptr_t;

# 93 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_uintptr_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_uintptr_t.h" 3 4



typedef unsigned long		uintptr_t;

# 94 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/types.h" 2 3 4



typedef u_int64_t		user_addr_t;	
typedef u_int64_t		user_size_t;	
typedef int64_t			user_ssize_t;
typedef int64_t			user_long_t;
typedef u_int64_t		user_ulong_t;
typedef int64_t			user_time_t;
typedef int64_t			user_off_t;







typedef u_int64_t		syscall_arg_t;


# 36 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/types.h" 2 3 4





# 37 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 2 3 4









struct __darwin_i386_thread_state
{
    unsigned int	__eax;
    unsigned int	__ebx;
    unsigned int	__ecx;
    unsigned int	__edx;
    unsigned int	__edi;
    unsigned int	__esi;
    unsigned int	__ebp;
    unsigned int	__esp;
    unsigned int	__ss;
    unsigned int	__eflags;
    unsigned int	__eip;
    unsigned int	__cs;
    unsigned int	__ds;
    unsigned int	__es;
    unsigned int	__fs;
    unsigned int	__gs;
};

# 87 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4





struct __darwin_fp_control
{
    unsigned short		__invalid	:1,
    				__denorm	:1,
				__zdiv		:1,
				__ovrfl		:1,
				__undfl		:1,
				__precis	:1,
						:2,
				__pc		:2,





				__rc		:2,






						:1,
						:3;
};
typedef struct __darwin_fp_control	__darwin_fp_control_t;

# 143 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4







struct __darwin_fp_status
{
    unsigned short		__invalid	:1,
    				__denorm	:1,
				__zdiv		:1,
				__ovrfl		:1,
				__undfl		:1,
				__precis	:1,
				__stkflt	:1,
				__errsumm	:1,
				__c0		:1,
				__c1		:1,
				__c2		:1,
				__tos		:3,
				__c3		:1,
				__busy		:1;
};
typedef struct __darwin_fp_status	__darwin_fp_status_t;

# 189 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4
				




struct __darwin_mmst_reg
{
	char	__mmst_reg[10];
	char	__mmst_rsrv[6];
};














struct __darwin_xmm_reg
{
	char		__xmm_reg[16];
};












struct __darwin_ymm_reg
{
	char		__ymm_reg[32];
};












struct __darwin_zmm_reg
{
	char		__zmm_reg[64];
};










struct __darwin_opmask_reg
{
	char		__opmask_reg[8];
};


















struct __darwin_i386_float_state
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		
	__uint16_t		__fpu_rsrv2;		
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		
	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	char			__fpu_rsrv4[14*16];	
	int 			__fpu_reserved1;
};


struct __darwin_i386_avx_state
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		
	__uint16_t		__fpu_rsrv2;		
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		
	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	char			__fpu_rsrv4[14*16];	
	int 			__fpu_reserved1;
	char			__avx_reserved1[64];
	struct __darwin_xmm_reg		__fpu_ymmh0;		
	struct __darwin_xmm_reg		__fpu_ymmh1;		
	struct __darwin_xmm_reg		__fpu_ymmh2;		
	struct __darwin_xmm_reg		__fpu_ymmh3;		
	struct __darwin_xmm_reg		__fpu_ymmh4;		
	struct __darwin_xmm_reg		__fpu_ymmh5;		
	struct __darwin_xmm_reg		__fpu_ymmh6;		
	struct __darwin_xmm_reg		__fpu_ymmh7;		
};


struct __darwin_i386_avx512_state
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		
	__uint16_t		__fpu_rsrv2;		
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		
	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	char			__fpu_rsrv4[14*16];	
	int 			__fpu_reserved1;
	char			__avx_reserved1[64];
	struct __darwin_xmm_reg		__fpu_ymmh0;		
	struct __darwin_xmm_reg		__fpu_ymmh1;		
	struct __darwin_xmm_reg		__fpu_ymmh2;		
	struct __darwin_xmm_reg		__fpu_ymmh3;		
	struct __darwin_xmm_reg		__fpu_ymmh4;		
	struct __darwin_xmm_reg		__fpu_ymmh5;		
	struct __darwin_xmm_reg		__fpu_ymmh6;		
	struct __darwin_xmm_reg		__fpu_ymmh7;		
	struct __darwin_opmask_reg	__fpu_k0;		
	struct __darwin_opmask_reg	__fpu_k1;		
	struct __darwin_opmask_reg	__fpu_k2;		
	struct __darwin_opmask_reg	__fpu_k3;		
	struct __darwin_opmask_reg	__fpu_k4;		
	struct __darwin_opmask_reg	__fpu_k5;		
	struct __darwin_opmask_reg	__fpu_k6;		
	struct __darwin_opmask_reg	__fpu_k7;		
	struct __darwin_ymm_reg		__fpu_zmmh0;		
	struct __darwin_ymm_reg		__fpu_zmmh1;		
	struct __darwin_ymm_reg		__fpu_zmmh2;		
	struct __darwin_ymm_reg		__fpu_zmmh3;		
	struct __darwin_ymm_reg		__fpu_zmmh4;		
	struct __darwin_ymm_reg		__fpu_zmmh5;		
	struct __darwin_ymm_reg		__fpu_zmmh6;		
	struct __darwin_ymm_reg		__fpu_zmmh7;		
};


# 572 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4



struct __darwin_i386_exception_state
{
	__uint16_t	__trapno;
	__uint16_t	__cpu;
	__uint32_t	__err;
	__uint32_t	__faultvaddr;
};

# 592 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4



struct __darwin_x86_debug_state32
{
	unsigned int	__dr0;
	unsigned int	__dr1;
	unsigned int	__dr2;
	unsigned int	__dr3;
	unsigned int	__dr4;
	unsigned int	__dr5;
	unsigned int	__dr6;
	unsigned int	__dr7;
};

# 620 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4







struct __darwin_x86_thread_state64
{
	__uint64_t	__rax;
	__uint64_t	__rbx;
	__uint64_t	__rcx;
	__uint64_t	__rdx;
	__uint64_t	__rdi;
	__uint64_t	__rsi;
	__uint64_t	__rbp;
	__uint64_t	__rsp;
	__uint64_t	__r8;
	__uint64_t	__r9;
	__uint64_t	__r10;
	__uint64_t	__r11;
	__uint64_t	__r12;
	__uint64_t	__r13;
	__uint64_t	__r14;
	__uint64_t	__r15;
	__uint64_t	__rip;
	__uint64_t	__rflags;
	__uint64_t	__cs;
	__uint64_t	__fs;
	__uint64_t	__gs;
};

# 678 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4




struct __darwin_x86_float_state64
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		

	
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		

	__uint16_t		__fpu_rsrv2;		

	
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		

	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	struct __darwin_xmm_reg		__fpu_xmm8;		
	struct __darwin_xmm_reg		__fpu_xmm9;		
	struct __darwin_xmm_reg		__fpu_xmm10;		
	struct __darwin_xmm_reg		__fpu_xmm11;		
	struct __darwin_xmm_reg		__fpu_xmm12;		
	struct __darwin_xmm_reg		__fpu_xmm13;		
	struct __darwin_xmm_reg		__fpu_xmm14;		
	struct __darwin_xmm_reg		__fpu_xmm15;		
	char			__fpu_rsrv4[6*16];	
	int 			__fpu_reserved1;
};


struct __darwin_x86_avx_state64
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		

	
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		

	__uint16_t		__fpu_rsrv2;		

	
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		

	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	struct __darwin_xmm_reg		__fpu_xmm8;		
	struct __darwin_xmm_reg		__fpu_xmm9;		
	struct __darwin_xmm_reg		__fpu_xmm10;		
	struct __darwin_xmm_reg		__fpu_xmm11;		
	struct __darwin_xmm_reg		__fpu_xmm12;		
	struct __darwin_xmm_reg		__fpu_xmm13;		
	struct __darwin_xmm_reg		__fpu_xmm14;		
	struct __darwin_xmm_reg		__fpu_xmm15;		
	char			__fpu_rsrv4[6*16];	
	int 			__fpu_reserved1;
	char			__avx_reserved1[64];
	struct __darwin_xmm_reg		__fpu_ymmh0;		
	struct __darwin_xmm_reg		__fpu_ymmh1;		
	struct __darwin_xmm_reg		__fpu_ymmh2;		
	struct __darwin_xmm_reg		__fpu_ymmh3;		
	struct __darwin_xmm_reg		__fpu_ymmh4;		
	struct __darwin_xmm_reg		__fpu_ymmh5;		
	struct __darwin_xmm_reg		__fpu_ymmh6;		
	struct __darwin_xmm_reg		__fpu_ymmh7;		
	struct __darwin_xmm_reg		__fpu_ymmh8;		
	struct __darwin_xmm_reg		__fpu_ymmh9;		
	struct __darwin_xmm_reg		__fpu_ymmh10;		
	struct __darwin_xmm_reg		__fpu_ymmh11;		
	struct __darwin_xmm_reg		__fpu_ymmh12;		
	struct __darwin_xmm_reg		__fpu_ymmh13;		
	struct __darwin_xmm_reg		__fpu_ymmh14;		
	struct __darwin_xmm_reg		__fpu_ymmh15;		
};


struct __darwin_x86_avx512_state64
{
	int 			__fpu_reserved[2];
	struct __darwin_fp_control	__fpu_fcw;		
	struct __darwin_fp_status	__fpu_fsw;		
	__uint8_t		__fpu_ftw;		
	__uint8_t		__fpu_rsrv1;		 
	__uint16_t		__fpu_fop;		

	
	__uint32_t		__fpu_ip;		
	__uint16_t		__fpu_cs;		

	__uint16_t		__fpu_rsrv2;		

	
	__uint32_t		__fpu_dp;		
	__uint16_t		__fpu_ds;		

	__uint16_t		__fpu_rsrv3;		
	__uint32_t		__fpu_mxcsr;		
	__uint32_t		__fpu_mxcsrmask;	
	struct __darwin_mmst_reg	__fpu_stmm0;		
	struct __darwin_mmst_reg	__fpu_stmm1;		
	struct __darwin_mmst_reg	__fpu_stmm2;		
	struct __darwin_mmst_reg	__fpu_stmm3;		
	struct __darwin_mmst_reg	__fpu_stmm4;		
	struct __darwin_mmst_reg	__fpu_stmm5;		
	struct __darwin_mmst_reg	__fpu_stmm6;		
	struct __darwin_mmst_reg	__fpu_stmm7;		
	struct __darwin_xmm_reg		__fpu_xmm0;		
	struct __darwin_xmm_reg		__fpu_xmm1;		
	struct __darwin_xmm_reg		__fpu_xmm2;		
	struct __darwin_xmm_reg		__fpu_xmm3;		
	struct __darwin_xmm_reg		__fpu_xmm4;		
	struct __darwin_xmm_reg		__fpu_xmm5;		
	struct __darwin_xmm_reg		__fpu_xmm6;		
	struct __darwin_xmm_reg		__fpu_xmm7;		
	struct __darwin_xmm_reg		__fpu_xmm8;		
	struct __darwin_xmm_reg		__fpu_xmm9;		
	struct __darwin_xmm_reg		__fpu_xmm10;		
	struct __darwin_xmm_reg		__fpu_xmm11;		
	struct __darwin_xmm_reg		__fpu_xmm12;		
	struct __darwin_xmm_reg		__fpu_xmm13;		
	struct __darwin_xmm_reg		__fpu_xmm14;		
	struct __darwin_xmm_reg		__fpu_xmm15;		
	char			__fpu_rsrv4[6*16];	
	int 			__fpu_reserved1;
	char			__avx_reserved1[64];
	struct __darwin_xmm_reg		__fpu_ymmh0;		
	struct __darwin_xmm_reg		__fpu_ymmh1;		
	struct __darwin_xmm_reg		__fpu_ymmh2;		
	struct __darwin_xmm_reg		__fpu_ymmh3;		
	struct __darwin_xmm_reg		__fpu_ymmh4;		
	struct __darwin_xmm_reg		__fpu_ymmh5;		
	struct __darwin_xmm_reg		__fpu_ymmh6;		
	struct __darwin_xmm_reg		__fpu_ymmh7;		
	struct __darwin_xmm_reg		__fpu_ymmh8;		
	struct __darwin_xmm_reg		__fpu_ymmh9;		
	struct __darwin_xmm_reg		__fpu_ymmh10;		
	struct __darwin_xmm_reg		__fpu_ymmh11;		
	struct __darwin_xmm_reg		__fpu_ymmh12;		
	struct __darwin_xmm_reg		__fpu_ymmh13;		
	struct __darwin_xmm_reg		__fpu_ymmh14;		
	struct __darwin_xmm_reg		__fpu_ymmh15;		
	struct __darwin_opmask_reg	__fpu_k0;		
	struct __darwin_opmask_reg	__fpu_k1;		
	struct __darwin_opmask_reg	__fpu_k2;		
	struct __darwin_opmask_reg	__fpu_k3;		
	struct __darwin_opmask_reg	__fpu_k4;		
	struct __darwin_opmask_reg	__fpu_k5;		
	struct __darwin_opmask_reg	__fpu_k6;		
	struct __darwin_opmask_reg	__fpu_k7;		
	struct __darwin_ymm_reg		__fpu_zmmh0;		
	struct __darwin_ymm_reg		__fpu_zmmh1;		
	struct __darwin_ymm_reg		__fpu_zmmh2;		
	struct __darwin_ymm_reg		__fpu_zmmh3;		
	struct __darwin_ymm_reg		__fpu_zmmh4;		
	struct __darwin_ymm_reg		__fpu_zmmh5;		
	struct __darwin_ymm_reg		__fpu_zmmh6;		
	struct __darwin_ymm_reg		__fpu_zmmh7;		
	struct __darwin_ymm_reg		__fpu_zmmh8;		
	struct __darwin_ymm_reg		__fpu_zmmh9;		
	struct __darwin_ymm_reg		__fpu_zmmh10;		
	struct __darwin_ymm_reg		__fpu_zmmh11;		
	struct __darwin_ymm_reg		__fpu_zmmh12;		
	struct __darwin_ymm_reg		__fpu_zmmh13;		
	struct __darwin_ymm_reg		__fpu_zmmh14;		
	struct __darwin_ymm_reg		__fpu_zmmh15;		
	struct __darwin_zmm_reg		__fpu_zmm16;		
	struct __darwin_zmm_reg		__fpu_zmm17;		
	struct __darwin_zmm_reg		__fpu_zmm18;		
	struct __darwin_zmm_reg		__fpu_zmm19;		
	struct __darwin_zmm_reg		__fpu_zmm20;		
	struct __darwin_zmm_reg		__fpu_zmm21;		
	struct __darwin_zmm_reg		__fpu_zmm22;		
	struct __darwin_zmm_reg		__fpu_zmm23;		
	struct __darwin_zmm_reg		__fpu_zmm24;		
	struct __darwin_zmm_reg		__fpu_zmm25;		
	struct __darwin_zmm_reg		__fpu_zmm26;		
	struct __darwin_zmm_reg		__fpu_zmm27;		
	struct __darwin_zmm_reg		__fpu_zmm28;		
	struct __darwin_zmm_reg		__fpu_zmm29;		
	struct __darwin_zmm_reg		__fpu_zmm30;		
	struct __darwin_zmm_reg		__fpu_zmm31;		
};


# 1137 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4



struct __darwin_x86_exception_state64
{
    __uint16_t	__trapno;
    __uint16_t	__cpu;
    __uint32_t	__err;
    __uint64_t	__faultvaddr;
};

# 1157 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4



struct __darwin_x86_debug_state64
{
	__uint64_t	__dr0;
	__uint64_t	__dr1;
	__uint64_t	__dr2;
	__uint64_t	__dr3;
	__uint64_t	__dr4;
	__uint64_t	__dr5;
	__uint64_t	__dr6;
	__uint64_t	__dr7;
};

# 1185 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/i386/_structs.h" 3 4



struct __darwin_x86_cpmu_state64
{
	__uint64_t __ctrs[16];
};









# 34 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/mach/machine/_structs.h" 2 3 4





# 35 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_mcontext.h" 2 3 4




struct __darwin_mcontext32
{
	struct __darwin_i386_exception_state	__es;
	struct __darwin_i386_thread_state	__ss;
	struct __darwin_i386_float_state	__fs;
};


struct __darwin_mcontext_avx32
{
	struct __darwin_i386_exception_state	__es;
	struct __darwin_i386_thread_state	__ss;
	struct __darwin_i386_avx_state		__fs;
};



struct __darwin_mcontext_avx512_32
{
	struct __darwin_i386_exception_state	__es;
	struct __darwin_i386_thread_state	__ss;
	struct __darwin_i386_avx512_state	__fs;
};



# 93 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_mcontext.h" 3 4




struct __darwin_mcontext64
{
	struct __darwin_x86_exception_state64	__es;
	struct __darwin_x86_thread_state64	__ss;
	struct __darwin_x86_float_state64	__fs;
};


struct __darwin_mcontext_avx64
{
	struct __darwin_x86_exception_state64	__es;
	struct __darwin_x86_thread_state64	__ss;
	struct __darwin_x86_avx_state64		__fs;
};



struct __darwin_mcontext_avx512_64
{
	struct __darwin_x86_exception_state64	__es;
	struct __darwin_x86_thread_state64	__ss;
	struct __darwin_x86_avx512_state64	__fs;
};



# 151 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/_mcontext.h" 3 4





typedef struct __darwin_mcontext64	*mcontext_t;








# 30 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_mcontext.h" 2 3 4
# 147 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_pthread/_pthread_attr_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_pthread/_pthread_attr_t.h" 3 4




typedef __darwin_pthread_attr_t pthread_attr_t;

# 149 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_sigaltstack.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_sigaltstack.h" 3 4















struct __darwin_sigaltstack
{
	void            *ss_sp;	        
	__darwin_size_t ss_size;        
	int             ss_flags;       
};
typedef struct __darwin_sigaltstack	stack_t; 


# 151 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ucontext.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ucontext.h" 3 4













# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_mcontext.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/_mcontext.h" 3 4


# 40 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ucontext.h" 2 3 4


struct __darwin_ucontext
{
	int                     uc_onstack;
	__darwin_sigset_t       uc_sigmask;     
	struct __darwin_sigaltstack     uc_stack;       
	struct __darwin_ucontext        *uc_link;       
	__darwin_size_t	        uc_mcsize;      
	struct __darwin_mcontext64        *uc_mcontext;   



};


typedef struct __darwin_ucontext	ucontext_t;     	


# 152 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_sigset_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_sigset_t.h" 3 4




typedef __darwin_sigset_t		sigset_t;

# 155 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_size_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_size_t.h" 3 4




typedef __darwin_size_t        size_t; 

# 156 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_uid_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_uid_t.h" 3 4




typedef __darwin_uid_t        uid_t; 

# 157 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 2 3 4

union sigval {
	
	int	sival_int;
	void	*sival_ptr;
};





struct sigevent {
	int				sigev_notify;				
	int				sigev_signo;				
	union sigval	sigev_value;				
	void			(*sigev_notify_function)(union sigval);	  
	pthread_attr_t	*sigev_notify_attributes;	
};


typedef struct __siginfo {
	int	si_signo;		
	int	si_errno;		
	int	si_code;		
	pid_t	si_pid;			
	uid_t	si_uid;			
	int	si_status;		
	void	*si_addr;		
	union sigval si_value;		
	long	si_band;		
	unsigned long	__pad[7];	
} siginfo_t;



# 200 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4






# 216 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4



# 229 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4






















# 259 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4










union __sigaction_u {
	void    (*__sa_handler)(int);
	void    (*__sa_sigaction)(int, struct __siginfo *,
		       void *);
};


struct	__sigaction {
	union __sigaction_u __sigaction_u;  
	void    (*sa_tramp)(void *, int, int, siginfo_t *, void *);
	sigset_t sa_mask;		
	int	sa_flags;		
};




struct	sigaction {
	union __sigaction_u __sigaction_u;  
	sigset_t sa_mask;		
	int	sa_flags;		
};








# 307 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/signal.h" 3 4
























typedef	void (*sig_t)(int);	
















struct	sigvec {
	void	(*sv_handler)(int);	
	int	sv_mask;		
	int	sv_flags;		
};














struct	sigstack {
	char	*ss_sp;			
	int	ss_onstack;		
};



















void	(*signal(int, void (*)(int)))(int);


# 110 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 3 4



# 62 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 3 4











# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/stdint.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/stdint.h" 3 4










// C99 7.18.3 Limits of other integer types
//
//  Footnote 219, 220: C++ implementations should define these macros only when
//  __STDC_LIMIT_MACROS is defined before <stdint.h> is included.
//
//  Footnote 222: C++ implementations should define these macros only when
//  __STDC_CONSTANT_MACROS is defined before <stdint.h> is included.
//
// C++11 [cstdint.syn]p2:
//
//  The macros defined by <cstdint> are provided unconditionally. In particular,
//  the symbols __STDC_LIMIT_MACROS and __STDC_CONSTANT_MACROS (mentioned in
//  footnotes 219, 220, and 222 in the C standard) play no role in C++.
//
// C11 removed the problematic footnotes.
//
// Work around this inconsistency by always defining those macros in C++ mode,
// so that a C library implementation which follows the C99 standard can be
// used in C++.

# 62 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/stdint.h" 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 1 3 4























# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint8_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint8_t.h" 3 4




typedef unsigned char uint8_t;

# 24 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint16_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint16_t.h" 3 4




typedef unsigned short uint16_t;

# 25 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint32_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint32_t.h" 3 4




typedef unsigned int uint32_t;

# 26 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint64_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uint64_t.h" 3 4




typedef unsigned long long uint64_t;

# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4


typedef int8_t           int_least8_t;
typedef int16_t         int_least16_t;
typedef int32_t         int_least32_t;
typedef int64_t         int_least64_t;
typedef uint8_t         uint_least8_t;
typedef uint16_t       uint_least16_t;
typedef uint32_t       uint_least32_t;
typedef uint64_t       uint_least64_t;



typedef int8_t            int_fast8_t;
typedef int16_t          int_fast16_t;
typedef int32_t          int_fast32_t;
typedef int64_t          int_fast64_t;
typedef uint8_t          uint_fast8_t;
typedef uint16_t        uint_fast16_t;
typedef uint32_t        uint_fast32_t;
typedef uint64_t        uint_fast64_t;











# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_intmax_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_intmax_t.h" 3 4





typedef long int intmax_t;

# 59 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uintmax_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/_types/_uintmax_t.h" 3 4





typedef long unsigned int uintmax_t;

# 60 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdint.h" 2 3 4



















   































































































































# 64 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/stdint.h" 2 3 4











# 73 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 2 3 4



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 76 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_timeval.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_timeval.h" 3 4







struct timeval
{
	__darwin_time_t	        tv_sec;	        
	__darwin_suseconds_t    tv_usec;        
};

# 81 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 2 3 4








typedef __uint64_t	rlim_t;






























































struct	rusage {
	struct timeval ru_utime;	
	struct timeval ru_stime;	



	




	long	ru_maxrss;		

	long	ru_ixrss;		
	long	ru_idrss;		
	long	ru_isrss;		
	long	ru_minflt;		
	long	ru_majflt;		
	long	ru_nswap;		
	long	ru_inblock;		
	long	ru_oublock;		
	long	ru_msgsnd;		
	long	ru_msgrcv;		
	long	ru_nsignals;		
	long	ru_nvcsw;		
	long	ru_nivcsw;		


};












typedef void *rusage_info_t;

struct rusage_info_v0 {
	uint8_t  ri_uuid[16];
	uint64_t ri_user_time;
	uint64_t ri_system_time;
	uint64_t ri_pkg_idle_wkups;
	uint64_t ri_interrupt_wkups;
	uint64_t ri_pageins;
	uint64_t ri_wired_size;
	uint64_t ri_resident_size;
	uint64_t ri_phys_footprint;
	uint64_t ri_proc_start_abstime;
	uint64_t ri_proc_exit_abstime;
};

struct rusage_info_v1 {
	uint8_t  ri_uuid[16];
	uint64_t ri_user_time;
	uint64_t ri_system_time;
	uint64_t ri_pkg_idle_wkups;
	uint64_t ri_interrupt_wkups;
	uint64_t ri_pageins;
	uint64_t ri_wired_size;
	uint64_t ri_resident_size;
	uint64_t ri_phys_footprint;
	uint64_t ri_proc_start_abstime;
	uint64_t ri_proc_exit_abstime;
	uint64_t ri_child_user_time;
	uint64_t ri_child_system_time;
	uint64_t ri_child_pkg_idle_wkups;
	uint64_t ri_child_interrupt_wkups;
	uint64_t ri_child_pageins;
	uint64_t ri_child_elapsed_abstime;
};

struct rusage_info_v2 {
	uint8_t  ri_uuid[16];
	uint64_t ri_user_time;
	uint64_t ri_system_time;
	uint64_t ri_pkg_idle_wkups;
	uint64_t ri_interrupt_wkups;
	uint64_t ri_pageins;
	uint64_t ri_wired_size;
	uint64_t ri_resident_size;
	uint64_t ri_phys_footprint;
	uint64_t ri_proc_start_abstime;
	uint64_t ri_proc_exit_abstime;
	uint64_t ri_child_user_time;
	uint64_t ri_child_system_time;
	uint64_t ri_child_pkg_idle_wkups;
	uint64_t ri_child_interrupt_wkups;
	uint64_t ri_child_pageins;
	uint64_t ri_child_elapsed_abstime;
	uint64_t ri_diskio_bytesread;
	uint64_t ri_diskio_byteswritten;
};

struct rusage_info_v3 {
	uint8_t  ri_uuid[16];
	uint64_t ri_user_time;
	uint64_t ri_system_time;
	uint64_t ri_pkg_idle_wkups;
	uint64_t ri_interrupt_wkups;
	uint64_t ri_pageins;
	uint64_t ri_wired_size;
	uint64_t ri_resident_size;
	uint64_t ri_phys_footprint;
	uint64_t ri_proc_start_abstime;
	uint64_t ri_proc_exit_abstime;
	uint64_t ri_child_user_time;
	uint64_t ri_child_system_time;
	uint64_t ri_child_pkg_idle_wkups;
	uint64_t ri_child_interrupt_wkups;
	uint64_t ri_child_pageins;
	uint64_t ri_child_elapsed_abstime;
	uint64_t ri_diskio_bytesread;
	uint64_t ri_diskio_byteswritten;
	uint64_t ri_cpu_time_qos_default;
	uint64_t ri_cpu_time_qos_maintenance;
	uint64_t ri_cpu_time_qos_background;
	uint64_t ri_cpu_time_qos_utility;
	uint64_t ri_cpu_time_qos_legacy;
	uint64_t ri_cpu_time_qos_user_initiated;
	uint64_t ri_cpu_time_qos_user_interactive;
	uint64_t ri_billed_system_time;
	uint64_t ri_serviced_system_time;
};

struct rusage_info_v4 {
	uint8_t  ri_uuid[16];
	uint64_t ri_user_time;
	uint64_t ri_system_time;
	uint64_t ri_pkg_idle_wkups;
	uint64_t ri_interrupt_wkups;
	uint64_t ri_pageins;
	uint64_t ri_wired_size;
	uint64_t ri_resident_size;
	uint64_t ri_phys_footprint;
	uint64_t ri_proc_start_abstime;
	uint64_t ri_proc_exit_abstime;
	uint64_t ri_child_user_time;
	uint64_t ri_child_system_time;
	uint64_t ri_child_pkg_idle_wkups;
	uint64_t ri_child_interrupt_wkups;
	uint64_t ri_child_pageins;
	uint64_t ri_child_elapsed_abstime;
	uint64_t ri_diskio_bytesread;
	uint64_t ri_diskio_byteswritten;
	uint64_t ri_cpu_time_qos_default;
	uint64_t ri_cpu_time_qos_maintenance;
	uint64_t ri_cpu_time_qos_background;
	uint64_t ri_cpu_time_qos_utility;
	uint64_t ri_cpu_time_qos_legacy;
	uint64_t ri_cpu_time_qos_user_initiated;
	uint64_t ri_cpu_time_qos_user_interactive;
	uint64_t ri_billed_system_time;
	uint64_t ri_serviced_system_time;
	uint64_t ri_logical_writes;
	uint64_t ri_lifetime_max_phys_footprint;
	uint64_t ri_instructions;
	uint64_t ri_cycles;
	uint64_t ri_billed_energy;
	uint64_t ri_serviced_energy;
        uint64_t ri_interval_max_phys_footprint; 
	// 1 reserve counter(s) remaining for future extension
	uint64_t ri_unused[1];
};

typedef struct rusage_info_v4 rusage_info_current;























# 361 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/resource.h" 3 4





struct rlimit {
	rlim_t	rlim_cur;		
	rlim_t	rlim_max;		
};































struct proc_rlimit_control_wakeupmon {
	uint32_t wm_flags;
	int32_t wm_rate;
};
































int	getpriority(int, id_t);

int	getiopolicy_np(int, int) __OSX_AVAILABLE_STARTING(1050, 20000);

int	getrlimit(int, struct rlimit *) __asm("_" "x" );
int	getrusage(int, struct rusage *);
int	setpriority(int, id_t, int);

int	setiopolicy_np(int, int, int) __OSX_AVAILABLE_STARTING(1050, 20000);

int	setrlimit(int, __const struct rlimit *) __asm("_" "x" );



# 111 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 2 3 4












































































# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/endian.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/endian.h" 3 4









# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/endian.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/endian.h" 3 4





# 64 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/endian.h" 3 4




































# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 3 4



# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 3 4


# 88 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 3 4












# 129 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/_OSByteOrder.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/_OSByteOrder.h" 3 4



























# 62 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/_OSByteOrder.h" 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/i386/_OSByteOrder.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/i386/_OSByteOrder.h" 3 4






# 41 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/i386/_OSByteOrder.h" 3 4



static __inline
__uint16_t
_OSSwapInt16(
    __uint16_t        _data
)
{
    return ((__uint16_t)((_data << 8) | (_data >> 8)));
}

static __inline
__uint32_t
_OSSwapInt32(
    __uint32_t        _data
)
{

    return __builtin_bswap32(_data);




}


static __inline
__uint64_t
_OSSwapInt64(
    __uint64_t        _data
)
{
    return __builtin_bswap64(_data);
}


# 103 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/i386/_OSByteOrder.h" 3 4


# 67 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/_OSByteOrder.h" 2 3 4














# 129 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/libkern/_OSByteOrder.h" 3 4


# 131 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_endian.h" 2 3 4













# 100 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/i386/endian.h" 2 3 4


# 36 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/machine/endian.h" 2 3 4





# 187 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/wait.h" 2 3 4







union wait {
	int	w_status;		
	


	struct {

		unsigned int	w_Termsig:7,	
				w_Coredump:1,	
				w_Retcode:8,	
				w_Filler:16;	







	} w_T;
	




	struct {

		unsigned int	w_Stopval:8,	
				w_Stopsig:8,	
				w_Filler:16;	






	} w_S;
};

















pid_t	wait(int *) __asm("_" "x" );
pid_t	waitpid(pid_t, int *, int) __asm("_" "x" );

int	waitid(idtype_t, id_t, siginfo_t *, int) __asm("_" "x" );


pid_t	wait3(int *, int, struct rusage *);
pid_t	wait4(pid_t, int *, int, struct rusage *);

           

# 67 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4

# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/alloca.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/alloca.h" 3 4










void	*alloca(size_t);		











# 69 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4









# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ct_rune_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ct_rune_t.h" 3 4





typedef __darwin_ct_rune_t ct_rune_t;

# 78 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_rune_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_rune_t.h" 3 4




typedef __darwin_rune_t rune_t; 

# 79 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_wchar_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_wchar_t.h" 3 4







typedef __darwin_wchar_t wchar_t;

# 82 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4

typedef struct {
	int quot;		
	int rem;		
} div_t;

typedef struct {
	long quot;		
	long rem;		
} ldiv_t;


typedef struct {
	long long quot;
	long long rem;
} lldiv_t;



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_null.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_null.h" 3 4


# 101 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4

















extern int __mb_cur_max;










# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/malloc/_malloc.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/malloc/_malloc.h" 3 4












# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 34 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/malloc/_malloc.h" 2 3 4






void	*malloc(size_t __size) __attribute__((__warn_unused_result__)) __attribute__((alloc_size(1)));
void	*calloc(size_t __count, size_t __size) __attribute__((__warn_unused_result__)) __attribute__((alloc_size(1,2)));
void	 free(void *);
void	*realloc(void *__ptr, size_t __size) __attribute__((__warn_unused_result__)) __attribute__((alloc_size(2)));

void	*valloc(size_t) __attribute__((alloc_size(1)));

int 	 posix_memalign(void **__memptr, size_t __alignment, size_t __size) __OSX_AVAILABLE_STARTING(1060, 30000);




# 129 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4


void	 abort(void) __attribute__((noreturn));
int	 abs(int) __attribute__((__const));
int	 atexit(void (* _Nonnull)(void));
double	 atof(__const char *);
int	 atoi(__const char *);
long	 atol(__const char *);

long long
	 atoll(__const char *);

void	*bsearch(__const void *__key, __const void *__base, size_t __nel,
	    size_t __width, int (* _Nonnull __compar)(__const void *, __const void *));

div_t	 div(int, int) __attribute__((__const));
void	 exit(int) __attribute__((noreturn));

char	*getenv(__const char *);
long	 labs(long) __attribute__((__const));
ldiv_t	 ldiv(long, long) __attribute__((__const));

long long
	 llabs(long long);
lldiv_t	 lldiv(long long, long long);


int	 mblen(__const char *__s, size_t __n);
size_t	 mbstowcs(wchar_t * restrict , __const char * restrict, size_t);
int	 mbtowc(wchar_t * restrict, __const char * restrict, size_t);

void	 qsort(void *__base, size_t __nel, size_t __width,
	    int (* _Nonnull __compar)(__const void *, __const void *));
int	 rand(void) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));

void	 srand(unsigned) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
double	 strtod(__const char *, char **) __asm("_" "x" );
float	 strtof(__const char *, char **) __asm("_" "x" );
long	 strtol(__const char *__str, char **__endptr, int __base);
long double
	 strtold(__const char *, char **);

long long 
	 strtoll(__const char *__str, char **__endptr, int __base);

unsigned long
	 strtoul(__const char *__str, char **__endptr, int __base);

unsigned long long
	 strtoull(__const char *__str, char **__endptr, int __base);








__attribute__((__availability__(swift, unavailable, message="Use posix_spawn APIs or NSTask instead.")))
 __IOS_PROHIBITED
__WATCHOS_PROHIBITED __TVOS_PROHIBITED
int	 system(__const char *) __asm("_" "x" );



size_t	 wcstombs(char * restrict, __const wchar_t * restrict, size_t);
int	 wctomb(char *, wchar_t);


void	_Exit(int) __attribute__((noreturn));
long	 a64l(__const char *);
double	 drand48(void);
char	*ecvt(double, int, int *restrict, int *restrict); 
double	 erand48(unsigned short[3]);
char	*fcvt(double, int, int *restrict, int *restrict); 
char	*gcvt(double, int, char *); 
int	 getsubopt(char **, char * __const *, char **);
int	 grantpt(int);

char	*initstate(unsigned, char *, size_t); 



long	 jrand48(unsigned short[3]) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
char	*l64a(long);
void	 lcong48(unsigned short[7]);
long	 lrand48(void) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
char	*mktemp(char *);
int	 mkstemp(char *);
long	 mrand48(void) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
long	 nrand48(unsigned short[3]) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
int	 posix_openpt(int);
char	*ptsname(int);


int ptsname_r(int fildes, char *buffer, size_t buflen) ;


int	 putenv(char *) __asm("_" "x" );
long	 random(void) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));
int	 rand_r(unsigned *) __attribute__((__availability__(swift, unavailable, message="Use arc4random instead.")));

char	*realpath(__const char * restrict, char * restrict) __asm("_" "x" "$DARWIN_EXTSN");



unsigned short
	*seed48(unsigned short[3]);
int	 setenv(__const char * __name, __const char * __value, int __overwrite) __asm("_" "x" );

void	 setkey(__const char *) __asm("_" "x" );



char	*setstate(__const char *);
void	 srand48(long);

void	 srandom(unsigned);



int	 unlockpt(int);

int	 unsetenv(__const char *) __asm("_" "x" );









# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_dev_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_dev_t.h" 3 4




typedef __darwin_dev_t        dev_t;	 

# 262 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_mode_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_mode_t.h" 3 4




typedef	__darwin_mode_t		mode_t;

# 263 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/stdlib.h" 2 3 4


uint32_t arc4random(void);
void	 arc4random_addrandom(unsigned char * , int )
    __OSX_AVAILABLE(10.0)
    __IOS_DEPRECATED(2.0, 10.0, "use arc4random_stir")
    __TVOS_DEPRECATED(2.0, 10.0, "use arc4random_stir")
    __WATCHOS_DEPRECATED(1.0, 3.0, "use arc4random_stir");
void	 arc4random_buf(void * __buf, size_t __nbytes) __OSX_AVAILABLE_STARTING(1070, 40300);
void	 arc4random_stir(void);
uint32_t
	 arc4random_uniform(uint32_t __upper_bound) __OSX_AVAILABLE_STARTING(1070, 40300);

int	 atexit_b(void (^ _Nonnull)(void)) __OSX_AVAILABLE_STARTING(1060, 30200);
void	*bsearch_b(__const void *__key, __const void *__base, size_t __nel,
	    size_t __width, int (^ _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);


	 
char	*cgetcap(char *, __const char *, int);
int	 cgetclose(void);
int	 cgetent(char **, char **, __const char *);
int	 cgetfirst(char **, char **);
int	 cgetmatch(__const char *, __const char *);
int	 cgetnext(char **, char **);
int	 cgetnum(char *, __const char *, long *);
int	 cgetset(__const char *);
int	 cgetstr(char *, __const char *, char **);
int	 cgetustr(char *, __const char *, char **);

int	 daemon(int, int) __asm("_" "x" "$1050") __OSX_AVAILABLE_BUT_DEPRECATED_MSG(1000, 1050, 20000, 20000, "Use posix_spawn APIs instead.") __WATCHOS_PROHIBITED __TVOS_PROHIBITED;
char	*devname(dev_t, mode_t);
char	*devname_r(dev_t, mode_t, char *buf, int len);
char	*getbsize(int *, long *);
int	 getloadavg(double [], int);
__const char
	*getprogname(void);

int	 heapsort(void *__base, size_t __nel, size_t __width,
	    int (* _Nonnull __compar)(__const void *, __const void *));

int	 heapsort_b(void *__base, size_t __nel, size_t __width,
	    int (^ _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);

int	 mergesort(void *__base, size_t __nel, size_t __width,
	    int (* _Nonnull __compar)(__const void *, __const void *));

int	 mergesort_b(void *__base, size_t __nel, size_t __width,
	    int (^ _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);

void	 psort(void *__base, size_t __nel, size_t __width,
	    int (* _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);

void	 psort_b(void *__base, size_t __nel, size_t __width,
	    int (^ _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);

void	 psort_r(void *__base, size_t __nel, size_t __width, void *,
	    int (* _Nonnull __compar)(void *, __const void *, __const void *))  __OSX_AVAILABLE_STARTING(1060, 30200);

void	 qsort_b(void *__base, size_t __nel, size_t __width,
	    int (^ _Nonnull __compar)(__const void *, __const void *)) __OSX_AVAILABLE_STARTING(1060, 30200);

void	 qsort_r(void *__base, size_t __nel, size_t __width, void *,
	    int (* _Nonnull __compar)(void *, __const void *, __const void *));
int	 radixsort(__const unsigned char **__base, int __nel, __const unsigned char *__table,
	    unsigned __endbyte);
void	 setprogname(__const char *);
int	 sradixsort(__const unsigned char **__base, int __nel, __const unsigned char *__table,
	    unsigned __endbyte);
void	 sranddev(void);
void	 srandomdev(void);
void	*reallocf(void *__ptr, size_t __size) __attribute__((alloc_size(2)));

long long
	 strtoq(__const char *__str, char **__endptr, int __base);
unsigned long long
	 strtouq(__const char *__str, char **__endptr, int __base);

extern char *suboptarg;		







           






# 3 "gemm_8x8_blocking_pack_ab_avx.c" 2
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 3 4


# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 3 4








# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 64 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 2 3 4






void	*memchr(__const void *__s, int __c, size_t __n);
int	 memcmp(__const void *__s1, __const void *__s2, size_t __n);
void	*memcpy(void *__dst, __const void *__src, size_t __n);
void	*memmove(void *__dst, __const void *__src, size_t __len);
void	*memset(void *__b, int __c, size_t __len);
char	*strcat(char *__s1, __const char *__s2);
char	*strchr(__const char *__s, int __c);
int	 strcmp(__const char *__s1, __const char *__s2);
int	 strcoll(__const char *__s1, __const char *__s2);
char	*strcpy(char *__dst, __const char *__src);
size_t	 strcspn(__const char *__s, __const char *__charset);
char	*strerror(int __errnum) __asm("_" "x" );
size_t	 strlen(__const char *__s);
char	*strncat(char *__s1, __const char *__s2, size_t __n);
int	 strncmp(__const char *__s1, __const char *__s2, size_t __n);
char	*strncpy(char *__dst, __const char *__src, size_t __n);
char	*strpbrk(__const char *__s, __const char *__charset);
char	*strrchr(__const char *__s, int __c);
size_t	 strspn(__const char *__s, __const char *__charset);
char	*strstr(__const char *__big, __const char *__little);
char	*strtok(char *__str, __const char *__sep);
size_t	 strxfrm(char *__s1, __const char *__s2, size_t __n);











             
char	*strtok_r(char *__str, __const char *__sep, char **__lasts);










             
int	 strerror_r(int __errnum, char *__strerrbuf, size_t __buflen);
char	*strdup(__const char *__s1);
void	*memccpy(void *__dst, __const void *__src, int __c, size_t __n);










             
char	*stpcpy(char *__dst, __const char *__src);
char    *stpncpy(char *__dst, __const char *__src, size_t __n) __OSX_AVAILABLE_STARTING(1070, 40300);
char	*strndup(__const char *__s1, size_t __n) __OSX_AVAILABLE_STARTING(1070, 40300);
size_t   strnlen(__const char *__s1, size_t __n) __OSX_AVAILABLE_STARTING(1070, 40300);
char	*strsignal(int __sig);


















# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ssize_t.h" 1 3 4
# 27 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/sys/_types/_ssize_t.h" 3 4




typedef __darwin_ssize_t        ssize_t; 

# 153 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 2 3 4


void	*memmem(__const void *__big, size_t __big_len, __const void *__little, size_t __little_len) __OSX_AVAILABLE_STARTING(1070, 40300);
void     memset_pattern4(void *__b, __const void *__pattern4, size_t __len) __OSX_AVAILABLE_STARTING(1050, 30000);
void     memset_pattern8(void *__b, __const void *__pattern8, size_t __len) __OSX_AVAILABLE_STARTING(1050, 30000);
void     memset_pattern16(void *__b, __const void *__pattern16, size_t __len) __OSX_AVAILABLE_STARTING(1050, 30000);

char	*strcasestr(__const char *__big, __const char *__little);
char	*strnstr(__const char *__big, __const char *__little, size_t __len);
size_t	 strlcat(char *__dst, __const char *__source, size_t __size);
size_t	 strlcpy(char *__dst, __const char *__source, size_t __size);
void	 strmode(int __mode, char *__bp);
char	*strsep(char **__stringp, __const char *__delim);


void	 swab(__const void * restrict, void * restrict, ssize_t);

__OSX_AVAILABLE(10.12.1) __attribute__((availability(ios,introduced=10.1)))
__attribute__((availability(tvos,introduced=10.0.1))) __attribute__((availability(watchos,introduced=3.1)))
int	timingsafe_bcmp(__const void *__b1, __const void *__b2, size_t __len);








# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 3 4


# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 3 4









# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 65 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 2 3 4





int	 bcmp(__const void *, __const void *, size_t) ___POSIX_C_DEPRECATED_STARTING_##200112L;
void	 bcopy(__const void *, void *, size_t) ___POSIX_C_DEPRECATED_STARTING_##200112L;
void	 bzero(void *, size_t) ___POSIX_C_DEPRECATED_STARTING_##200112L;
char	*index(__const char *, int) ___POSIX_C_DEPRECATED_STARTING_##200112L;
char	*rindex(__const char *, int) ___POSIX_C_DEPRECATED_STARTING_##200112L;


int	 ffs(int);
int	 strcasecmp(__const char *, __const char *);
int	 strncasecmp(__const char *, __const char *, size_t);




             
int	 ffsl(long) __OSX_AVAILABLE_STARTING(1050, 20000);
int	 ffsll(long long) __OSX_AVAILABLE_STARTING(1090, 70000);
int	 fls(int) __OSX_AVAILABLE_STARTING(1050, 20000);
int	 flsl(long) __OSX_AVAILABLE_STARTING(1050, 20000);
int	 flsll(long long) __OSX_AVAILABLE_STARTING(1090, 70000);



# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 3 4


# 56 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 3 4



# 93 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_strings.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_strings.h" 3 4











# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 33 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_strings.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_common.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_common.h" 3 4






# 37 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_common.h" 3 4





# 34 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_strings.h" 2 3 4

























# 98 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/strings.h" 2 3 4





# 181 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 2 3 4










# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_string.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_string.h" 3 4











# 1 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

 

# 267 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4
    #define __attribute__((availability(_target, _availability)))
    





  #if 1
    
    #define   
  




    #define 
    








  #if 1
    
    #define __OSX_AVAILABLE(_vers)               __attribute__((availability(macosx,introduced=_vers)))
    
  #endif







  #define __OSX_AVAILABLE(_vers)









  #if 1
    
    #define __IOS_PROHIBITED                     __attribute__((availability(ios,unavailable)))
    
    #define __IOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(ios,introduced=_start))) 
  







  #define __IOS_PROHIBITED







  #define __IOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __TVOS_PROHIBITED                     __attribute__((availability(tvos,unavailable)))
    
    #define __TVOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(tvos,introduced=_start))) 
  







  #define __TVOS_PROHIBITED







  #define __TVOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __WATCHOS_PROHIBITED                     __attribute__((availability(watchos,unavailable)))
    
    #define __WATCHOS_DEPRECATED(_start, _dep, _msg) __attribute__((availability(watchos,introduced=_start))) 
  







  #define __WATCHOS_PROHIBITED







  #define __WATCHOS_DEPRECATED(_start, _dep, _msg)





  #if 1
    
    #define __SWIFT_UNAVAILABLE_MSG(_msg)         
  







  #define __SWIFT_UNAVAILABLE_MSG(_msg)



# 421 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4



 #if 1

    
# 438 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    


    
# 457 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/Availability.h" 3 4

    
    #define  __API_DEPRECATED_REP1(__VA_ARGS__)

    







    
 #else 

    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)
 


    


    
    
    #define __API_DEPRECATED_MSG_GET_MACRO(...,__API_DEPRECATED_MSG7,__API_DEPRECATED_MSG6,__API_DEPRECATED_MSG5,__API_DEPRECATED_MSG4,__API_DEPRECATED_MSG3,__API_DEPRECATED_MSG2,__API_DEPRECATED_MSG1, 0)(...)
    
    #define __API_UNAVAILABLE_GET_MACRO(...,__API_UNAVAILABLE6,__API_UNAVAILABLE5,__API_UNAVAILABLE4,__API_UNAVAILABLE3,__API_UNAVAILABLE2,__API_UNAVAILABLE1, 0)(...)









# 33 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/secure/_string.h" 2 3 4

















































































































# 191 "/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/string.h" 2 3 4



# 4 "gemm_8x8_blocking_pack_ab_avx.c" 2
# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 3 4







# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/mmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/mmintrin.h" 3 4





typedef long long __m64 __attribute__((__vector_size__(8)));

typedef long long __v1di __attribute__((__vector_size__(8)));
typedef int __v2si __attribute__((__vector_size__(8)));
typedef short __v4hi __attribute__((__vector_size__(8)));
typedef char __v8qi __attribute__((__vector_size__(8)));




/// \brief Clears the MMX state by setting the state of the x87 stack registers
///    to empty.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> EMMS </c> instruction.
///
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_empty(void)
{
    __builtin_ia32_emms();
}

/// \brief Constructs a 64-bit integer vector, setting the lower 32 bits to the
///    value of the 32-bit integer parameter and setting the upper 32 bits to 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.
///
/// \param __i
///    A 32-bit integer value.
/// \returns A 64-bit integer vector. The lower 32 bits contain the value of the
///    parameter. The upper 32 bits are set to 0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cvtsi32_si64(int __i)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
}

/// \brief Returns the lower 32 bits of a 64-bit integer vector as a 32-bit
///    __signed integer.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector.
/// \returns A 32-bit __signed integer value containing the lower 32 bits of the
///    parameter.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cvtsi64_si32(__m64 __m)
{
    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
}

/// \brief Casts a 64-bit __signed integer value into a 64-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVD </c> instruction.
///
/// \param __i
///    A 64-bit __signed integer.
/// \returns A 64-bit integer vector containing the same bitwise pattern as the
///    parameter.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cvtsi64_m64(long long __i)
{
    return (__m64)__i;
}

/// \brief Casts a 64-bit integer vector into a 64-bit __signed integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector.
/// \returns A 64-bit __signed integer containing the same bitwise pattern as the
///    parameter.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cvtm64_si64(__m64 __m)
{
    return (long long)__m;
}

/// \brief Converts 16-bit __signed integers from both 64-bit integer vector
///    parameters of [4 x i16] into 8-bit __signed integer values, and constructs
///    a 64-bit integer vector of [8 x i8] as the result. Positive values
///    greater than 0x7F are saturated to 0x7F. Negative values less than 0x80
///    are saturated to 0x80.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PACKSSWB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a
///    16-bit __signed integer and is converted to an 8-bit __signed integer with
///    saturation. Positive values greater than 0x7F are saturated to 0x7F.
///    Negative values less than 0x80 are saturated to 0x80. The converted
///    [4 x i8] values are written to the lower 32 bits of the result.
/// \param __m2
///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a
///    16-bit __signed integer and is converted to an 8-bit __signed integer with
///    saturation. Positive values greater than 0x7F are saturated to 0x7F.
///    Negative values less than 0x80 are saturated to 0x80. The converted
///    [4 x i8] values are written to the upper 32 bits of the result.
/// \returns A 64-bit integer vector of [8 x i8] containing the converted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_packs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Converts 32-bit __signed integers from both 64-bit integer vector
///    parameters of [2 x i32] into 16-bit __signed integer values, and constructs
///    a 64-bit integer vector of [4 x i16] as the result. Positive values
///    greater than 0x7FFF are saturated to 0x7FFF. Negative values less than
///    0x8000 are saturated to 0x8000.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PACKSSDW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32]. Each 32-bit element is treated as a
///    32-bit __signed integer and is converted to a 16-bit __signed integer with
///    saturation. Positive values greater than 0x7FFF are saturated to 0x7FFF.
///    Negative values less than 0x8000 are saturated to 0x8000. The converted
///    [2 x i16] values are written to the lower 32 bits of the result.
/// \param __m2
///    A 64-bit integer vector of [2 x i32]. Each 32-bit element is treated as a
///    32-bit __signed integer and is converted to a 16-bit __signed integer with
///    saturation. Positive values greater than 0x7FFF are saturated to 0x7FFF.
///    Negative values less than 0x8000 are saturated to 0x8000. The converted
///    [2 x i16] values are written to the upper 32 bits of the result.
/// \returns A 64-bit integer vector of [4 x i16] containing the converted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_packs_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
}

/// \brief Converts 16-bit __signed integers from both 64-bit integer vector
///    parameters of [4 x i16] into 8-bit unsigned integer values, and
///    constructs a 64-bit integer vector of [8 x i8] as the result. Values
///    greater than 0xFF are saturated to 0xFF. Values less than 0 are saturated
///    to 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PACKUSWB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a
///    16-bit __signed integer and is converted to an 8-bit unsigned integer with
///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less
///    than 0 are saturated to 0. The converted [4 x i8] values are written to
///    the lower 32 bits of the result.
/// \param __m2
///    A 64-bit integer vector of [4 x i16]. Each 16-bit element is treated as a
///    16-bit __signed integer and is converted to an 8-bit unsigned integer with
///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less
///    than 0 are saturated to 0. The converted [4 x i8] values are written to
///    the upper 32 bits of the result.
/// \returns A 64-bit integer vector of [8 x i8] containing the converted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_packs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Unpacks the upper 32 bits from two 64-bit integer vectors of [8 x i8]
///    and interleaves them into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKHBW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8]. \n
///    Bits [39:32] are written to bits [7:0] of the result. \n
///    Bits [47:40] are written to bits [23:16] of the result. \n
///    Bits [55:48] are written to bits [39:32] of the result. \n
///    Bits [63:56] are written to bits [55:48] of the result.
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
///    Bits [39:32] are written to bits [15:8] of the result. \n
///    Bits [47:40] are written to bits [31:24] of the result. \n
///    Bits [55:48] are written to bits [47:40] of the result. \n
///    Bits [63:56] are written to bits [63:56] of the result.
/// \returns A 64-bit integer vector of [8 x i8] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Unpacks the upper 32 bits from two 64-bit integer vectors of
///    [4 x i16] and interleaves them into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKHWD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
///    Bits [47:32] are written to bits [15:0] of the result. \n
///    Bits [63:48] are written to bits [47:32] of the result.
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
///    Bits [47:32] are written to bits [31:16] of the result. \n
///    Bits [63:48] are written to bits [63:48] of the result.
/// \returns A 64-bit integer vector of [4 x i16] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Unpacks the upper 32 bits from two 64-bit integer vectors of
///    [2 x i32] and interleaves them into a 64-bit integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKHDQ </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32]. The upper 32 bits are written to
///    the lower 32 bits of the result.
/// \param __m2
///    A 64-bit integer vector of [2 x i32]. The upper 32 bits are written to
///    the upper 32 bits of the result.
/// \returns A 64-bit integer vector of [2 x i32] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
}

/// \brief Unpacks the lower 32 bits from two 64-bit integer vectors of [8 x i8]
///    and interleaves them into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKLBW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
///    Bits [7:0] are written to bits [7:0] of the result. \n
///    Bits [15:8] are written to bits [23:16] of the result. \n
///    Bits [23:16] are written to bits [39:32] of the result. \n
///    Bits [31:24] are written to bits [55:48] of the result.
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
///    Bits [7:0] are written to bits [15:8] of the result. \n
///    Bits [15:8] are written to bits [31:24] of the result. \n
///    Bits [23:16] are written to bits [47:40] of the result. \n
///    Bits [31:24] are written to bits [63:56] of the result.
/// \returns A 64-bit integer vector of [8 x i8] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Unpacks the lower 32 bits from two 64-bit integer vectors of
///    [4 x i16] and interleaves them into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKLWD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
///    Bits [15:0] are written to bits [15:0] of the result. \n
///    Bits [31:16] are written to bits [47:32] of the result.
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
///    Bits [15:0] are written to bits [31:16] of the result. \n
///    Bits [31:16] are written to bits [63:48] of the result.
/// \returns A 64-bit integer vector of [4 x i16] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Unpacks the lower 32 bits from two 64-bit integer vectors of
///    [2 x i32] and interleaves them into a 64-bit integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PUNPCKLDQ </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32]. The lower 32 bits are written to
///    the lower 32 bits of the result.
/// \param __m2
///    A 64-bit integer vector of [2 x i32]. The lower 32 bits are written to
///    the upper 32 bits of the result.
/// \returns A 64-bit integer vector of [2 x i32] containing the interleaved
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
}

/// \brief Adds each 8-bit integer element of the first 64-bit integer vector
///    of [8 x i8] to the corresponding 8-bit integer element of the second
///    64-bit integer vector of [8 x i8]. The lower 8 bits of the results are
///    packed into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
/// \returns A 64-bit integer vector of [8 x i8] containing the sums of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_add_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Adds each 16-bit integer element of the first 64-bit integer vector
///    of [4 x i16] to the corresponding 16-bit integer element of the second
///    64-bit integer vector of [4 x i16]. The lower 16 bits of the results are
///    packed into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the sums of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_add_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Adds each 32-bit integer element of the first 64-bit integer vector
///    of [2 x i32] to the corresponding 32-bit integer element of the second
///    64-bit integer vector of [2 x i32]. The lower 32 bits of the results are
///    packed into a 64-bit integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32].
/// \param __m2
///    A 64-bit integer vector of [2 x i32].
/// \returns A 64-bit integer vector of [2 x i32] containing the sums of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_add_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
}

/// \brief Adds each 8-bit __signed integer element of the first 64-bit integer
///    vector of [8 x i8] to the corresponding 8-bit __signed integer element of
///    the second 64-bit integer vector of [8 x i8]. Positive sums greater than
///    0x7F are saturated to 0x7F. Negative sums less than 0x80 are saturated to
///    0x80. The results are packed into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDSB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
/// \returns A 64-bit integer vector of [8 x i8] containing the saturated sums
///    of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_adds_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Adds each 16-bit __signed integer element of the first 64-bit integer
///    vector of [4 x i16] to the corresponding 16-bit __signed integer element of
///    the second 64-bit integer vector of [4 x i16]. Positive sums greater than
///    0x7FFF are saturated to 0x7FFF. Negative sums less than 0x8000 are
///    saturated to 0x8000. The results are packed into a 64-bit integer vector
///    of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDSW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the saturated sums
///    of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_adds_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Adds each 8-bit unsigned integer element of the first 64-bit integer
///    vector of [8 x i8] to the corresponding 8-bit unsigned integer element of
///    the second 64-bit integer vector of [8 x i8]. Sums greater than 0xFF are
///    saturated to 0xFF. The results are packed into a 64-bit integer vector of
///    [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDUSB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
/// \returns A 64-bit integer vector of [8 x i8] containing the saturated
///    unsigned sums of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_adds_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Adds each 16-bit unsigned integer element of the first 64-bit integer
///    vector of [4 x i16] to the corresponding 16-bit unsigned integer element
///    of the second 64-bit integer vector of [4 x i16]. Sums greater than
///    0xFFFF are saturated to 0xFFFF. The results are packed into a 64-bit
///    integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDUSW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the saturated
///    unsigned sums of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_adds_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Subtracts each 8-bit integer element of the second 64-bit integer
///    vector of [8 x i8] from the corresponding 8-bit integer element of the
///    first 64-bit integer vector of [8 x i8]. The lower 8 bits of the results
///    are packed into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [8 x i8] containing the subtrahends.
/// \returns A 64-bit integer vector of [8 x i8] containing the differences of
///    both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sub_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Subtracts each 16-bit integer element of the second 64-bit integer
///    vector of [4 x i16] from the corresponding 16-bit integer element of the
///    first 64-bit integer vector of [4 x i16]. The lower 16 bits of the
///    results are packed into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [4 x i16] containing the subtrahends.
/// \returns A 64-bit integer vector of [4 x i16] containing the differences of
///    both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sub_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Subtracts each 32-bit integer element of the second 64-bit integer
///    vector of [2 x i32] from the corresponding 32-bit integer element of the
///    first 64-bit integer vector of [2 x i32]. The lower 32 bits of the
///    results are packed into a 64-bit integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [2 x i32] containing the subtrahends.
/// \returns A 64-bit integer vector of [2 x i32] containing the differences of
///    both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sub_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);
}

/// \brief Subtracts each 8-bit __signed integer element of the second 64-bit
///    integer vector of [8 x i8] from the corresponding 8-bit __signed integer
///    element of the first 64-bit integer vector of [8 x i8]. Positive results
///    greater than 0x7F are saturated to 0x7F. Negative results less than 0x80
///    are saturated to 0x80. The results are packed into a 64-bit integer
///    vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBSB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [8 x i8] containing the subtrahends.
/// \returns A 64-bit integer vector of [8 x i8] containing the saturated
///    differences of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_subs_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Subtracts each 16-bit __signed integer element of the second 64-bit
///    integer vector of [4 x i16] from the corresponding 16-bit __signed integer
///    element of the first 64-bit integer vector of [4 x i16]. Positive results
///    greater than 0x7FFF are saturated to 0x7FFF. Negative results less than
///    0x8000 are saturated to 0x8000. The results are packed into a 64-bit
///    integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBSW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [4 x i16] containing the subtrahends.
/// \returns A 64-bit integer vector of [4 x i16] containing the saturated
///    differences of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_subs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Subtracts each 8-bit unsigned integer element of the second 64-bit
///    integer vector of [8 x i8] from the corresponding 8-bit unsigned integer
///    element of the first 64-bit integer vector of [8 x i8].
///
///    If an element of the first vector is less than the corresponding element
///    of the second vector, the result is saturated to 0. The results are
///    packed into a 64-bit integer vector of [8 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBUSB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [8 x i8] containing the subtrahends.
/// \returns A 64-bit integer vector of [8 x i8] containing the saturated
///    differences of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_subs_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Subtracts each 16-bit unsigned integer element of the second 64-bit
///    integer vector of [4 x i16] from the corresponding 16-bit unsigned
///    integer element of the first 64-bit integer vector of [4 x i16].
///
///    If an element of the first vector is less than the corresponding element
///    of the second vector, the result is saturated to 0. The results are
///    packed into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBUSW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16] containing the minuends.
/// \param __m2
///    A 64-bit integer vector of [4 x i16] containing the subtrahends.
/// \returns A 64-bit integer vector of [4 x i16] containing the saturated
///    differences of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_subs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Multiplies each 16-bit __signed integer element of the first 64-bit
///    integer vector of [4 x i16] by the corresponding 16-bit __signed integer
///    element of the second 64-bit integer vector of [4 x i16] and get four
///    32-bit products. Adds adjacent pairs of products to get two 32-bit sums.
///    The lower 32 bits of these two sums are packed into a 64-bit integer
///    vector of [2 x i32].
///
///    For example, bits [15:0] of both parameters are multiplied, bits [31:16]
///    of both parameters are multiplied, and the sum of both results is written
///    to bits [31:0] of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMADDWD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [2 x i32] containing the sums of
///    products of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_madd_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Multiplies each 16-bit __signed integer element of the first 64-bit
///    integer vector of [4 x i16] by the corresponding 16-bit __signed integer
///    element of the second 64-bit integer vector of [4 x i16]. Packs the upper
///    16 bits of the 32-bit products into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMULHW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the upper 16 bits
///    of the products of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_mulhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Multiplies each 16-bit __signed integer element of the first 64-bit
///    integer vector of [4 x i16] by the corresponding 16-bit __signed integer
///    element of the second 64-bit integer vector of [4 x i16]. Packs the lower
///    16 bits of the 32-bit products into a 64-bit integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMULLW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the lower 16 bits
///    of the products of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_mullo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Left-shifts each 16-bit __signed integer element of the first
///    parameter, which is a 64-bit integer vector of [4 x i16], by the number
///    of bits specified by the second parameter, which is a 64-bit integer. The
///    lower 16 bits of the results are packed into a 64-bit integer vector of
///    [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [4 x i16] containing the left-shifted
///    values. If \a __count is greater or equal to 16, the result is set to all
///    0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sll_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);
}

/// \brief Left-shifts each 16-bit __signed integer element of a 64-bit integer
///    vector of [4 x i16] by the number of bits specified by a 32-bit integer.
///    The lower 16 bits of the results are packed into a 64-bit integer vector
///    of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [4 x i16] containing the left-shifted
///    values. If \a __count is greater or equal to 16, the result is set to all
///    0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_slli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);
}

/// \brief Left-shifts each 32-bit __signed integer element of the first
///    parameter, which is a 64-bit integer vector of [2 x i32], by the number
///    of bits specified by the second parameter, which is a 64-bit integer. The
///    lower 32 bits of the results are packed into a 64-bit integer vector of
///    [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [2 x i32] containing the left-shifted
///    values. If \a __count is greater or equal to 32, the result is set to all
///    0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sll_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);
}

/// \brief Left-shifts each 32-bit __signed integer element of a 64-bit integer
///    vector of [2 x i32] by the number of bits specified by a 32-bit integer.
///    The lower 32 bits of the results are packed into a 64-bit integer vector
///    of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [2 x i32] containing the left-shifted
///    values. If \a __count is greater or equal to 32, the result is set to all
///    0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_slli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);
}

/// \brief Left-shifts the first 64-bit integer parameter by the number of bits
///    specified by the second 64-bit integer parameter. The lower 64 bits of
///    result are returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLQ </c> instruction.
///
/// \param __m
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector containing the left-shifted value. If
///     \a __count is greater or equal to 64, the result is set to 0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sll_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllq((__v1di)__m, __count);
}

/// \brief Left-shifts the first parameter, which is a 64-bit integer, by the
///    number of bits specified by the second parameter, which is a 32-bit
///    integer. The lower 64 bits of result are returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSLLQ </c> instruction.
///
/// \param __m
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector containing the left-shifted value. If
///     \a __count is greater or equal to 64, the result is set to 0.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_slli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllqi((__v1di)__m, __count);
}

/// \brief Right-shifts each 16-bit integer element of the first parameter,
///    which is a 64-bit integer vector of [4 x i16], by the number of bits
///    specified by the second parameter, which is a 64-bit integer.
///
///    High-order bits are filled with the sign bit of the initial value of each
///    16-bit element. The 16-bit results are packed into a 64-bit integer
///    vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRAW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [4 x i16] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sra_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);
}

/// \brief Right-shifts each 16-bit integer element of a 64-bit integer vector
///    of [4 x i16] by the number of bits specified by a 32-bit integer.
///
///    High-order bits are filled with the sign bit of the initial value of each
///    16-bit element. The 16-bit results are packed into a 64-bit integer
///    vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRAW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [4 x i16] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srai_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);
}

/// \brief Right-shifts each 32-bit integer element of the first parameter,
///    which is a 64-bit integer vector of [2 x i32], by the number of bits
///    specified by the second parameter, which is a 64-bit integer.
///
///    High-order bits are filled with the sign bit of the initial value of each
///    32-bit element. The 32-bit results are packed into a 64-bit integer
///    vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRAD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [2 x i32] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_sra_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);
}

/// \brief Right-shifts each 32-bit integer element of a 64-bit integer vector
///    of [2 x i32] by the number of bits specified by a 32-bit integer.
///
///    High-order bits are filled with the sign bit of the initial value of each
///    32-bit element. The 32-bit results are packed into a 64-bit integer
///    vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRAD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [2 x i32] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srai_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);
}

/// \brief Right-shifts each 16-bit integer element of the first parameter,
///    which is a 64-bit integer vector of [4 x i16], by the number of bits
///    specified by the second parameter, which is a 64-bit integer.
///
///    High-order bits are cleared. The 16-bit results are packed into a 64-bit
///    integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [4 x i16] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srl_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);
}

/// \brief Right-shifts each 16-bit integer element of a 64-bit integer vector
///    of [4 x i16] by the number of bits specified by a 32-bit integer.
///
///    High-order bits are cleared. The 16-bit results are packed into a 64-bit
///    integer vector of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLW </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [4 x i16].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [4 x i16] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);
}

/// \brief Right-shifts each 32-bit integer element of the first parameter,
///    which is a 64-bit integer vector of [2 x i32], by the number of bits
///    specified by the second parameter, which is a 64-bit integer.
///
///    High-order bits are cleared. The 32-bit results are packed into a 64-bit
///    integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector of [2 x i32] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srl_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);
}

/// \brief Right-shifts each 32-bit integer element of a 64-bit integer vector
///    of [2 x i32] by the number of bits specified by a 32-bit integer.
///
///    High-order bits are cleared. The 32-bit results are packed into a 64-bit
///    integer vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLD </c> instruction.
///
/// \param __m
///    A 64-bit integer vector of [2 x i32].
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector of [2 x i32] containing the right-shifted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);
}

/// \brief Right-shifts the first 64-bit integer parameter by the number of bits
///    specified by the second 64-bit integer parameter.
///
///    High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLQ </c> instruction.
///
/// \param __m
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \param __count
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \returns A 64-bit integer vector containing the right-shifted value.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srl_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlq((__v1di)__m, __count);
}

/// \brief Right-shifts the first parameter, which is a 64-bit integer, by the
///    number of bits specified by the second parameter, which is a 32-bit
///    integer.
///
///    High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSRLQ </c> instruction.
///
/// \param __m
///    A 64-bit integer vector interpreted as a single 64-bit integer.
/// \param __count
///    A 32-bit integer value.
/// \returns A 64-bit integer vector containing the right-shifted value.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_srli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlqi((__v1di)__m, __count);
}

/// \brief Performs a bitwise AND of two 64-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PAND </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector.
/// \param __m2
///    A 64-bit integer vector.
/// \returns A 64-bit integer vector containing the bitwise AND of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_and_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pand((__v1di)__m1, (__v1di)__m2);
}

/// \brief Performs a bitwise NOT of the first 64-bit integer vector, and then
///    performs a bitwise AND of the intermediate result and the second 64-bit
///    integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PANDN </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector. The one's complement of this parameter is used
///    in the bitwise AND.
/// \param __m2
///    A 64-bit integer vector.
/// \returns A 64-bit integer vector containing the bitwise AND of the second
///    parameter and the one's complement of the first parameter.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_andnot_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pandn((__v1di)__m1, (__v1di)__m2);
}

/// \brief Performs a bitwise OR of two 64-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> POR </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector.
/// \param __m2
///    A 64-bit integer vector.
/// \returns A 64-bit integer vector containing the bitwise OR of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_or_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_por((__v1di)__m1, (__v1di)__m2);
}

/// \brief Performs a bitwise exclusive OR of two 64-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PXOR </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector.
/// \param __m2
///    A 64-bit integer vector.
/// \returns A 64-bit integer vector containing the bitwise exclusive OR of both
///    parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_xor_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pxor((__v1di)__m1, (__v1di)__m2);
}

/// \brief Compares the 8-bit integer elements of two 64-bit integer vectors of
///    [8 x i8] to determine if the element of the first vector is equal to the
///    corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPEQB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
/// \returns A 64-bit integer vector of [8 x i8] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Compares the 16-bit integer elements of two 64-bit integer vectors of
///    [4 x i16] to determine if the element of the first vector is equal to the
///    corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFFFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPEQW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Compares the 32-bit integer elements of two 64-bit integer vectors of
///    [2 x i32] to determine if the element of the first vector is equal to the
///    corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFFFFFFFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPEQD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32].
/// \param __m2
///    A 64-bit integer vector of [2 x i32].
/// \returns A 64-bit integer vector of [2 x i32] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);
}

/// \brief Compares the 8-bit integer elements of two 64-bit integer vectors of
///    [8 x i8] to determine if the element of the first vector is greater than
///    the corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPGTB </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [8 x i8].
/// \param __m2
///    A 64-bit integer vector of [8 x i8].
/// \returns A 64-bit integer vector of [8 x i8] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);
}

/// \brief Compares the 16-bit integer elements of two 64-bit integer vectors of
///    [4 x i16] to determine if the element of the first vector is greater than
///    the corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFFFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPGTW </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [4 x i16].
/// \param __m2
///    A 64-bit integer vector of [4 x i16].
/// \returns A 64-bit integer vector of [4 x i16] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);
}

/// \brief Compares the 32-bit integer elements of two 64-bit integer vectors of
///    [2 x i32] to determine if the element of the first vector is greater than
///    the corresponding element of the second vector.
///
///    The comparison yields 0 for false, 0xFFFFFFFF for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PCMPGTD </c> instruction.
///
/// \param __m1
///    A 64-bit integer vector of [2 x i32].
/// \param __m2
///    A 64-bit integer vector of [2 x i32].
/// \returns A 64-bit integer vector of [2 x i32] containing the comparison
///    results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);
}

/// \brief Constructs a 64-bit integer vector initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.
///
/// \returns An initialized 64-bit integer vector with all elements set to zero.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_setzero_si64(void)
{
    return (__m64){ 0LL };
}

/// \brief Constructs a 64-bit integer vector initialized with the specified
///    32-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __i1
///    A 32-bit integer value used to initialize the upper 32 bits of the
///    result.
/// \param __i0
///    A 32-bit integer value used to initialize the lower 32 bits of the
///    result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set_pi32(int __i1, int __i0)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);
}

/// \brief Constructs a 64-bit integer vector initialized with the specified
///    16-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __s3
///    A 16-bit integer value used to initialize bits [63:48] of the result.
/// \param __s2
///    A 16-bit integer value used to initialize bits [47:32] of the result.
/// \param __s1
///    A 16-bit integer value used to initialize bits [31:16] of the result.
/// \param __s0
///    A 16-bit integer value used to initialize bits [15:0] of the result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)
{
    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);
}

/// \brief Constructs a 64-bit integer vector initialized with the specified
///    8-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __b7
///    An 8-bit integer value used to initialize bits [63:56] of the result.
/// \param __b6
///    An 8-bit integer value used to initialize bits [55:48] of the result.
/// \param __b5
///    An 8-bit integer value used to initialize bits [47:40] of the result.
/// \param __b4
///    An 8-bit integer value used to initialize bits [39:32] of the result.
/// \param __b3
///    An 8-bit integer value used to initialize bits [31:24] of the result.
/// \param __b2
///    An 8-bit integer value used to initialize bits [23:16] of the result.
/// \param __b1
///    An 8-bit integer value used to initialize bits [15:8] of the result.
/// \param __b0
///    An 8-bit integer value used to initialize bits [7:0] of the result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,
            char __b1, char __b0)
{
    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,
                                               __b4, __b5, __b6, __b7);
}

/// \brief Constructs a 64-bit integer vector of [2 x i32], with each of the
///    32-bit integer vector elements set to the specified 32-bit integer
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSHUFD / PSHUFD </c> instruction.
///
/// \param __i
///    A 32-bit integer value used to initialize each vector element of the
///    result.
/// \returns An initialized 64-bit integer vector of [2 x i32].
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set1_pi32(int __i)
{
    return _mm_set_pi32(__i, __i);
}

/// \brief Constructs a 64-bit integer vector of [4 x i16], with each of the
///    16-bit integer vector elements set to the specified 16-bit integer
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSHUFLW / PSHUFLW </c> instruction.
///
/// \param __w
///    A 16-bit integer value used to initialize each vector element of the
///    result.
/// \returns An initialized 64-bit integer vector of [4 x i16].
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set1_pi16(short __w)
{
    return _mm_set_pi16(__w, __w, __w, __w);
}

/// \brief Constructs a 64-bit integer vector of [8 x i8], with each of the
///    8-bit integer vector elements set to the specified 8-bit integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLBW + VPSHUFLW / PUNPCKLBW +
///    PSHUFLW </c> instruction.
///
/// \param __b
///    An 8-bit integer value used to initialize each vector element of the
///    result.
/// \returns An initialized 64-bit integer vector of [8 x i8].
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_set1_pi8(char __b)
{
    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);
}

/// \brief Constructs a 64-bit integer vector, initialized in reverse order with
///    the specified 32-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __i0
///    A 32-bit integer value used to initialize the lower 32 bits of the
///    result.
/// \param __i1
///    A 32-bit integer value used to initialize the upper 32 bits of the
///    result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_setr_pi32(int __i0, int __i1)
{
    return _mm_set_pi32(__i1, __i0);
}

/// \brief Constructs a 64-bit integer vector, initialized in reverse order with
///    the specified 16-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __w0
///    A 16-bit integer value used to initialize bits [15:0] of the result.
/// \param __w1
///    A 16-bit integer value used to initialize bits [31:16] of the result.
/// \param __w2
///    A 16-bit integer value used to initialize bits [47:32] of the result.
/// \param __w3
///    A 16-bit integer value used to initialize bits [63:48] of the result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)
{
    return _mm_set_pi16(__w3, __w2, __w1, __w0);
}

/// \brief Constructs a 64-bit integer vector, initialized in reverse order with
///    the specified 8-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __b0
///    An 8-bit integer value used to initialize bits [7:0] of the result.
/// \param __b1
///    An 8-bit integer value used to initialize bits [15:8] of the result.
/// \param __b2
///    An 8-bit integer value used to initialize bits [23:16] of the result.
/// \param __b3
///    An 8-bit integer value used to initialize bits [31:24] of the result.
/// \param __b4
///    An 8-bit integer value used to initialize bits [39:32] of the result.
/// \param __b5
///    An 8-bit integer value used to initialize bits [47:40] of the result.
/// \param __b6
///    An 8-bit integer value used to initialize bits [55:48] of the result.
/// \param __b7
///    An 8-bit integer value used to initialize bits [63:56] of the result.
/// \returns An initialized 64-bit integer vector.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
             char __b6, char __b7)
{
    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}





# 1568 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/mmintrin.h" 3 4




# 29 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 3 4







typedef int __v4si __attribute__((__vector_size__(16)));
typedef float __v4sf __attribute__((__vector_size__(16)));
typedef float __m128 __attribute__((__vector_size__(16)));


typedef unsigned int __v4su __attribute__((__vector_size__(16)));





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/mm_malloc.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/mm_malloc.h" 3 4











extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);










static __inline__ void *__attribute__((__always_inline__, __nodebug__,
                                       __malloc__))
_mm_malloc(size_t __size, size_t __align)
{
  if (__align == 1) {
    return malloc(__size);
  }

  if (!(__align & (__align - 1)) && __align < sizeof(void *))
    __align = sizeof(void *);

  void *__mallocedMemory;





  if (posix_memalign(&__mallocedMemory, __align, __size))
    return 0;


  return __mallocedMemory;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_free(void *__p)
{
  free(__p);
}



# 40 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 2 3 4





/// \brief Adds the 32-bit float values in the low-order bits of the operands.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSS / ADDSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The lower 32 bits of this operand are used in the calculation.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The lower 32 bits of this operand are used in the calculation.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the sum
///    of the lower 32 bits of both operands. The upper 96 bits are copied from
///    the upper 96 bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_add_ss(__m128 __a, __m128 __b)
{
  __a[0] += __b[0];
  return __a;
}

/// \brief Adds two 128-bit vectors of [4 x float], and returns the results of
///    the addition.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDPS / ADDPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \returns A 128-bit vector of [4 x float] containing the sums of both
///    operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_add_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a + (__v4sf)__b);
}

/// \brief Subtracts the 32-bit float value in the low-order bits of the second
///    operand from the corresponding value in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBSS / SUBSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the minuend. The lower 32 bits
///    of this operand are used in the calculation.
/// \param __b
///    A 128-bit vector of [4 x float] containing the subtrahend. The lower 32
///    bits of this operand are used in the calculation.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    difference of the lower 32 bits of both operands. The upper 96 bits are
///    copied from the upper 96 bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_sub_ss(__m128 __a, __m128 __b)
{
  __a[0] -= __b[0];
  return __a;
}

/// \brief Subtracts each of the values of the second operand from the first
///    operand, both of which are 128-bit vectors of [4 x float] and returns
///    the results of the subtraction.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBPS / SUBPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the minuend.
/// \param __b
///    A 128-bit vector of [4 x float] containing the subtrahend.
/// \returns A 128-bit vector of [4 x float] containing the differences between
///    both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_sub_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a - (__v4sf)__b);
}

/// \brief Multiplies two 32-bit float values in the low-order bits of the
///    operands.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULSS / MULSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The lower 32 bits of this operand are used in the calculation.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The lower 32 bits of this operand are used in the calculation.
/// \returns A 128-bit vector of [4 x float] containing the product of the lower
///    32 bits of both operands. The upper 96 bits are copied from the upper 96
///    bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_mul_ss(__m128 __a, __m128 __b)
{
  __a[0] *= __b[0];
  return __a;
}

/// \brief Multiplies two 128-bit vectors of [4 x float] and returns the
///    results of the multiplication.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULPS / MULPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \returns A 128-bit vector of [4 x float] containing the products of both
///    operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_mul_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a * (__v4sf)__b);
}

/// \brief Divides the value in the low-order 32 bits of the first operand by
///    the corresponding value in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVSS / DIVSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the dividend. The lower 32
///    bits of this operand are used in the calculation.
/// \param __b
///    A 128-bit vector of [4 x float] containing the divisor. The lower 32 bits
///    of this operand are used in the calculation.
/// \returns A 128-bit vector of [4 x float] containing the quotients of the
///    lower 32 bits of both operands. The upper 96 bits are copied from the
///    upper 96 bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_div_ss(__m128 __a, __m128 __b)
{
  __a[0] /= __b[0];
  return __a;
}

/// \brief Divides two 128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVPS / DIVPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the dividend.
/// \param __b
///    A 128-bit vector of [4 x float] containing the divisor.
/// \returns A 128-bit vector of [4 x float] containing the quotients of both
///    operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_div_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a / (__v4sf)__b);
}

/// \brief Calculates the square root of the value stored in the low-order bits
///    of a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTSS / SQRTSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the calculation.
/// \returns A 128-bit vector of [4 x float] containing the square root of the
///    value in the low-order bits of the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_sqrt_ss(__m128 __a)
{
  __m128 __c = __builtin_ia32_sqrtss((__v4sf)__a);
  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
}

/// \brief Calculates the square roots of the values stored in a 128-bit vector
///    of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTPS / SQRTPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the square roots of the
///    values in the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_sqrt_ps(__m128 __a)
{
  return __builtin_ia32_sqrtps((__v4sf)__a);
}

/// \brief Calculates the approximate reciprocal of the value stored in the
///    low-order bits of a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRCPSS / RCPSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the calculation.
/// \returns A 128-bit vector of [4 x float] containing the approximate
///    reciprocal of the value in the low-order bits of the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_rcp_ss(__m128 __a)
{
  __m128 __c = __builtin_ia32_rcpss((__v4sf)__a);
  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
}

/// \brief Calculates the approximate reciprocals of the values stored in a
///    128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRCPPS / RCPPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the approximate
///    reciprocals of the values in the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_rcp_ps(__m128 __a)
{
  return __builtin_ia32_rcpps((__v4sf)__a);
}

/// \brief Calculates the approximate reciprocal of the square root of the value
///    stored in the low-order bits of a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRSQRTSS / RSQRTSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the calculation.
/// \returns A 128-bit vector of [4 x float] containing the approximate
///    reciprocal of the square root of the value in the low-order bits of the
///    operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_rsqrt_ss(__m128 __a)
{
  __m128 __c = __builtin_ia32_rsqrtss((__v4sf)__a);
  return (__m128) { __c[0], __a[1], __a[2], __a[3] };
}

/// \brief Calculates the approximate reciprocals of the square roots of the
///    values stored in a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRSQRTPS / RSQRTPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the approximate
///    reciprocals of the square roots of the values in the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_rsqrt_ps(__m128 __a)
{
  return __builtin_ia32_rsqrtps((__v4sf)__a);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands and returns the lesser value in the low-order bits of the
///    vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINSS / MINSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    minimum value between both operands. The upper 96 bits are copied from
///    the upper 96 bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_min_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 128-bit vectors of [4 x float] and returns the lesser
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINPS / MINPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands.
/// \returns A 128-bit vector of [4 x float] containing the minimum values
///    between both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_min_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands and returns the greater value in the low-order bits of a 128-bit
///    vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXSS / MAXSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    maximum value between both operands. The upper 96 bits are copied from
///    the upper 96 bits of the first source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_max_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 128-bit vectors of [4 x float] and returns the greater
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXPS / MAXPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands.
/// \returns A 128-bit vector of [4 x float] containing the maximum values
///    between both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_max_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs a bitwise AND of two 128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDPS / ANDPS </c> instructions.
///
/// \param __a
///    A 128-bit vector containing one of the source operands.
/// \param __b
///    A 128-bit vector containing one of the source operands.
/// \returns A 128-bit vector of [4 x float] containing the bitwise AND of the
///    values between both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_and_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a & (__v4su)__b);
}

/// \brief Performs a bitwise AND of two 128-bit vectors of [4 x float], using
///    the one's complement of the values contained in the first source
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDNPS / ANDNPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the first source operand. The
///    one's complement of this value is used in the bitwise AND.
/// \param __b
///    A 128-bit vector of [4 x float] containing the second source operand.
/// \returns A 128-bit vector of [4 x float] containing the bitwise AND of the
///    one's complement of the first operand and the values in the second
///    operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_andnot_ps(__m128 __a, __m128 __b)
{
  return (__m128)(~(__v4su)__a & (__v4su)__b);
}

/// \brief Performs a bitwise OR of two 128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VORPS / ORPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \returns A 128-bit vector of [4 x float] containing the bitwise OR of the
///    values between both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_or_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a | (__v4su)__b);
}

/// \brief Performs a bitwise exclusive OR of two 128-bit vectors of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
/// \returns A 128-bit vector of [4 x float] containing the bitwise exclusive OR
///    of the values between both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_xor_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a ^ (__v4su)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands for equality and returns the result of the comparison in the
///    low-order bits of a vector [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPEQSS / CMPEQSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpeq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] for equality.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPEQPS / CMPEQPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpeq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is less than the
///    corresponding value in the second operand and returns the result of the
///    comparison in the low-order bits of a vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTSS / CMPLTSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmplt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are less than those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTPS / CMPLTPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmplt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is less than or
///    equal to the corresponding value in the second operand and returns the
///    result of the comparison in the low-order bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLESS / CMPLESS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmple_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpless((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are less than or equal to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLEPS / CMPLEPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmple_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is greater than
///    the corresponding value in the second operand and returns the result of
///    the comparison in the low-order bits of a vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTSS / CMPLTSS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpgt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are greater than those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTPS / CMPLTPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpgt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__b, (__v4sf)__a);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is greater than
///    or equal to the corresponding value in the second operand and returns
///    the result of the comparison in the low-order bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLESS / CMPLESS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are greater than or equal to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLEPS / CMPLEPS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__b, (__v4sf)__a);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands for inequality and returns the result of the comparison in the
///    low-order bits of a vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNEQSS / CMPNEQSS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpneq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] for inequality.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNEQPS / CMPNEQPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpneq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is not less than
///    the corresponding value in the second operand and returns the result of
///    the comparison in the low-order bits of a vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTSS / CMPNLTSS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnlt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are not less than those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTPS / CMPNLTPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnlt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is not less than
///    or equal to the corresponding value in the second operand and returns
///    the result of the comparison in the low-order bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLESS / CMPNLESS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnle_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnless((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are not less than or equal to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLEPS / CMPNLEPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnle_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is not greater
///    than the corresponding value in the second operand and returns the
///    result of the comparison in the low-order bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTSS / CMPNLTSS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpngt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are not greater than those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTPS / CMPNLTPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpngt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__b, (__v4sf)__a);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is not greater
///    than or equal to the corresponding value in the second operand and
///    returns the result of the comparison in the low-order bits of a vector
///    of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLESS / CMPNLESS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are not greater than or equal to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLEPS / CMPNLEPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpnge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__b, (__v4sf)__a);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is ordered with
///    respect to the corresponding value in the second operand and returns the
///    result of the comparison in the low-order bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPORDSS / CMPORDSS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are ordered with respect to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPORDPS / CMPORDPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the value in the first operand is unordered
///    with respect to the corresponding value in the second operand and
///    returns the result of the comparison in the low-order bits of a vector
///    of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPUNORDSS / CMPUNORDSS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the operands. The lower
///    32 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [4 x float] containing the comparison results
///    in the low-order bits.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpunord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordss((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares each of the corresponding 32-bit float values of the
///    128-bit vectors of [4 x float] to determine if the values in the first
///    operand are unordered with respect to those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPUNORDPS / CMPUNORDPS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x float] containing the comparison results.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cmpunord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands for equality and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comieq((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the first operand is less than the second
///    operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comilt((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the first operand is less than or equal to the
///    second operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comile((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the first operand is greater than the second
///    operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comigt((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the first operand is greater than or equal to
///    the second operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comige((__v4sf)__a, (__v4sf)__b);
}

/// \brief Compares two 32-bit float values in the low-order bits of both
///    operands to determine if the first operand is not equal to the second
///    operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISS / COMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_comineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comineq((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine equality and returns
///    the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomieq((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine if the first operand is
///    less than the second operand and returns the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomilt((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine if the first operand is
///    less than or equal to the second operand and returns the result of the
///    comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomile((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine if the first operand is
///    greater than the second operand and returns the result of the
///    comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomigt((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine if the first operand is
///    greater than or equal to the second operand and returns the result of
///    the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomige((__v4sf)__a, (__v4sf)__b);
}

/// \brief Performs an unordered comparison of two 32-bit float values using
///    the low-order bits of both operands to determine inequality and returns
///    the result of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISS / UCOMISS </c> instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the comparison.
/// \returns An integer containing the comparison results.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_ucomineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomineq((__v4sf)__a, (__v4sf)__b);
}

/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 32-bit integer.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 32-bit integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtss_si32(__m128 __a)
{
  return __builtin_ia32_cvtss2si((__v4sf)__a);
}

/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 32-bit integer.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 32-bit integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvt_ss2si(__m128 __a)
{
  return _mm_cvtss_si32(__a);
}



/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 64-bit integer.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSS2SI / CVTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 64-bit integer containing the converted value.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtss_si64(__m128 __a)
{
  return __builtin_ia32_cvtss2si64((__v4sf)__a);
}



/// \brief Converts two low-order float values in a 128-bit vector of
///    [4 x float] into a 64-bit vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPS2PI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 64-bit integer vector containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvtps2pi((__v4sf)__a);
}

/// \brief Converts two low-order float values in a 128-bit vector of
///    [4 x float] into a 64-bit vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPS2PI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 64-bit integer vector containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvt_ps2pi(__m128 __a)
{
  return _mm_cvtps_pi32(__a);
}

/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 32-bit integer, truncating the result when it is
///    inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 32-bit integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvttss_si32(__m128 __a)
{
  return __builtin_ia32_cvttss2si((__v4sf)__a);
}

/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 32-bit integer, truncating the result when it is
///    inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 32-bit integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtt_ss2si(__m128 __a)
{
  return _mm_cvttss_si32(__a);
}


/// \brief Converts a float value contained in the lower 32 bits of a vector of
///    [4 x float] into a 64-bit integer, truncating the result when it is
///    inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTSS2SI / CVTTSS2SI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the conversion.
/// \returns A 64-bit integer containing the converted value.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvttss_si64(__m128 __a)
{
  return __builtin_ia32_cvttss2si64((__v4sf)__a);
}


/// \brief Converts two low-order float values in a 128-bit vector of
///    [4 x float] into a 64-bit vector of [2 x i32], truncating the result
///    when it is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTTPS2PI / VTTPS2PI </c>
///   instructions.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 64-bit integer vector containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvttps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvttps2pi((__v4sf)__a);
}

/// \brief Converts two low-order float values in a 128-bit vector of [4 x
///    float] into a 64-bit vector of [2 x i32], truncating the result when it
///    is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTTPS2PI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 64-bit integer vector containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtt_ps2pi(__m128 __a)
{
  return _mm_cvttps_pi32(__a);
}

/// \brief Converts a 32-bit __signed integer value into a floating point value
///    and writes it to the lower 32 bits of the destination. The remaining
///    higher order elements of the destination vector are copied from the
///    corresponding elements in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 32-bit __signed integer operand containing the value to be converted.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    converted value of the second operand. The upper 96 bits are copied from
///    the upper 96 bits of the first operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtsi32_ss(__m128 __a, int __b)
{
  __a[0] = __b;
  return __a;
}

/// \brief Converts a 32-bit __signed integer value into a floating point value
///    and writes it to the lower 32 bits of the destination. The remaining
///    higher order elements of the destination are copied from the
///    corresponding elements in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 32-bit __signed integer operand containing the value to be converted.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    converted value of the second operand. The upper 96 bits are copied from
///    the upper 96 bits of the first operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvt_si2ss(__m128 __a, int __b)
{
  return _mm_cvtsi32_ss(__a, __b);
}



/// \brief Converts a 64-bit __signed integer value into a floating point value
///    and writes it to the lower 32 bits of the destination. The remaining
///    higher order elements of the destination are copied from the
///    corresponding elements in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSI2SS / CVTSI2SS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 64-bit __signed integer operand containing the value to be converted.
/// \returns A 128-bit vector of [4 x float] whose lower 32 bits contain the
///    converted value of the second operand. The upper 96 bits are copied from
///    the upper 96 bits of the first operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtsi64_ss(__m128 __a, long long __b)
{
  __a[0] = __b;
  return __a;
}



/// \brief Converts two elements of a 64-bit vector of [2 x i32] into two
///    floating point values and writes them to the lower 64-bits of the
///    destination. The remaining higher order elements of the destination are
///    copied from the corresponding elements in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 64-bit vector of [2 x i32]. The elements in this vector are converted
///    and written to the corresponding low-order elements in the destination.
/// \returns A 128-bit vector of [4 x float] whose lower 64 bits contain the
///    converted value of the second operand. The upper 64 bits are copied from
///    the upper 64 bits of the first operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpi32_ps(__m128 __a, __m64 __b)
{
  return __builtin_ia32_cvtpi2ps((__v4sf)__a, (__v2si)__b);
}

/// \brief Converts two elements of a 64-bit vector of [2 x i32] into two
///    floating point values and writes them to the lower 64-bits of the
///    destination. The remaining higher order elements of the destination are
///    copied from the corresponding elements in the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 64-bit vector of [2 x i32]. The elements in this vector are converted
///    and written to the corresponding low-order elements in the destination.
/// \returns A 128-bit vector of [4 x float] whose lower 64 bits contain the
///    converted value from the second operand. The upper 64 bits are copied
///    from the upper 64 bits of the first operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvt_pi2ps(__m128 __a, __m64 __b)
{
  return _mm_cvtpi32_ps(__a, __b);
}

/// \brief Extracts a float value contained in the lower 32 bits of a vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower 32 bits of this operand are
///    used in the extraction.
/// \returns A 32-bit float containing the extracted value.
static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtss_f32(__m128 __a)
{
  return __a[0];
}

/// \brief Loads two packed float values from the address \a __p into the
///     high-order bits of a 128-bit vector of [4 x float]. The low-order bits
///     are copied from the low-order bits of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. Bits [63:0] are written to bits [63:0]
///    of the destination.
/// \param __p
///    A pointer to two packed float values. Bits [63:0] are written to bits
///    [127:64] of the destination.
/// \returns A 128-bit vector of [4 x float] containing the moved values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_loadh_pi(__m128 __a, __const __m64 *__p)
{
  typedef float __mm_loadh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadh_pi_struct {
    __mm_loadh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadh_pi_v2f32 __b = ((struct __mm_loadh_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 0, 1, 4, 5);
}

/// \brief Loads two packed float values from the address \a __p into the
///    low-order bits of a 128-bit vector of [4 x float]. The high-order bits
///    are copied from the high-order bits of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. Bits [127:64] are written to bits
///    [127:64] of the destination.
/// \param __p
///    A pointer to two packed float values. Bits [63:0] are written to bits
///    [63:0] of the destination.
/// \returns A 128-bit vector of [4 x float] containing the moved values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_loadl_pi(__m128 __a, __const __m64 *__p)
{
  typedef float __mm_loadl_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadl_pi_struct {
    __mm_loadl_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadl_pi_v2f32 __b = ((struct __mm_loadl_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 4, 5, 2, 3);
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]. The lower
///    32 bits of the vector are initialized with the single-precision
///    floating-point value loaded from a specified memory location. The upper
///    96 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.
///
/// \param __p
///    A pointer to a 32-bit memory location containing a single-precision
///    floating-point value.
/// \returns An initialized 128-bit floating-point vector of [4 x float]. The
///    lower 32 bits contain the value loaded from the memory location. The
///    upper 96 bits are set to zero.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_load_ss(__const float *__p)
{
  struct __mm_load_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((struct __mm_load_ss_struct*)__p)->__u;
  return (__m128){ __u, 0, 0, 0 };
}

/// \brief Loads a 32-bit float value and duplicates it to all four vector
///    elements of a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS + shuffling </c>
///    instruction.
///
/// \param __p
///    A pointer to a float value to be loaded and duplicated.
/// \returns A 128-bit vector of [4 x float] containing the loaded and
///    duplicated values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_load1_ps(__const float *__p)
{
  struct __mm_load1_ps_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((struct __mm_load1_ps_struct*)__p)->__u;
  return (__m128){ __u, __u, __u, __u };
}



/// \brief Loads a 128-bit floating-point vector of [4 x float] from an aligned
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location has to be 128-bit aligned.
/// \returns A 128-bit vector of [4 x float] containing the loaded valus.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_load_ps(__const float *__p)
{
  return *(__m128*)__p;
}

/// \brief Loads a 128-bit floating-point vector of [4 x float] from an
///    unaligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location does not have to be aligned.
/// \returns A 128-bit vector of [4 x float] containing the loaded values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_loadu_ps(__const float *__p)
{
  struct __loadu_ps {
    __m128 __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_ps*)__p)->__v;
}

/// \brief Loads four packed float values, in reverse order, from an aligned
///    memory location to 32-bit elements in a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS + shuffling </c>
///    instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location has to be 128-bit aligned.
/// \returns A 128-bit vector of [4 x float] containing the moved values, loaded
///    in reverse order.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_loadr_ps(__const float *__p)
{
  __m128 __a = _mm_load_ps(__p);
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
}

/// \brief Create a 128-bit vector of [4 x float] with undefined values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 128-bit vector of [4 x float] containing undefined values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_undefined_ps(void)
{
  return (__m128)__builtin_ia32_undef128();
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]. The lower
///    32 bits of the vector are initialized with the specified single-precision
///    floating-point value. The upper 96 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.
///
/// \param __w
///    A single-precision floating-point value used to initialize the lower 32
///    bits of the result.
/// \returns An initialized 128-bit floating-point vector of [4 x float]. The
///    lower 32 bits contain the value provided in the source operand. The
///    upper 96 bits are set to zero.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_set_ss(float __w)
{
  return (__m128){ __w, 0, 0, 0 };
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float], with each
///    of the four single-precision floating-point vector elements set to the
///    specified single-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS / PERMILPS </c> instruction.
///
/// \param __w
///    A single-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_set1_ps(float __w)
{
  return (__m128){ __w, __w, __w, __w };
}


/// \brief Constructs a 128-bit floating-point vector of [4 x float], with each
///    of the four single-precision floating-point vector elements set to the
///    specified single-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS / PERMILPS </c> instruction.
///
/// \param __w
///    A single-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_set_ps1(float __w)
{
    return _mm_set1_ps(__w);
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]
///    initialized with the specified single-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __z
///    A single-precision floating-point value used to initialize bits [127:96]
///    of the result.
/// \param __y
///    A single-precision floating-point value used to initialize bits [95:64]
///    of the result.
/// \param __x
///    A single-precision floating-point value used to initialize bits [63:32]
///    of the result.
/// \param __w
///    A single-precision floating-point value used to initialize bits [31:0]
///    of the result.
/// \returns An initialized 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_set_ps(float __z, float __y, float __x, float __w)
{
  return (__m128){ __w, __x, __y, __z };
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float],
///    initialized in reverse order with the specified 32-bit single-precision
///    float-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __z
///    A single-precision floating-point value used to initialize bits [31:0]
///    of the result.
/// \param __y
///    A single-precision floating-point value used to initialize bits [63:32]
///    of the result.
/// \param __x
///    A single-precision floating-point value used to initialize bits [95:64]
///    of the result.
/// \param __w
///    A single-precision floating-point value used to initialize bits [127:96]
///    of the result.
/// \returns An initialized 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_setr_ps(float __z, float __y, float __x, float __w)
{
  return (__m128){ __z, __y, __x, __w };
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float] initialized
///    to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.
///
/// \returns An initialized 128-bit floating-point vector of [4 x float] with
///    all elements set to zero.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_setzero_ps(void)
{
  return (__m128){ 0, 0, 0, 0 };
}

/// \brief Stores the upper 64 bits of a 128-bit vector of [4 x float] to a
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPEXTRQ / MOVQ </c> instruction.
///
/// \param __p
///    A pointer to a 64-bit memory location.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_storeh_pi(__m64 *__p, __m128 __a)
{
  __builtin_ia32_storehps((__v2si *)__p, (__v4sf)__a);
}

/// \brief Stores the lower 64 bits of a 128-bit vector of [4 x float] to a
///     memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVLPS / MOVLPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_storel_pi(__m64 *__p, __m128 __a)
{
  __builtin_ia32_storelps((__v2si *)__p, (__v4sf)__a);
}

/// \brief Stores the lower 32 bits of a 128-bit vector of [4 x float] to a
///     memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.
///
/// \param __p
///    A pointer to a 32-bit memory location.
/// \param __a
///    A 128-bit vector of [4 x float] containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_store_ss(float *__p, __m128 __a)
{
  struct __mm_store_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_ss_struct*)__p)->__u = __a[0];
}

/// \brief Stores a 128-bit vector of [4 x float] to an unaligned memory
///    location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location does not have to be aligned.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_storeu_ps(float *__p, __m128 __a)
{
  struct __storeu_ps {
    __m128 __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}

/// \brief Stores a 128-bit vector of [4 x float] into an aligned memory
///    location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location has to be 16-byte aligned.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_store_ps(float *__p, __m128 __a)
{
  *(__m128*)__p = __a;
}

/// \brief Stores the lower 32 bits of a 128-bit vector of [4 x float] into
///    four contiguous elements in an aligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to <c> VMOVAPS / MOVAPS + shuffling </c>
///    instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location.
/// \param __a
///    A 128-bit vector of [4 x float] whose lower 32 bits are stored to each
///    of the four contiguous elements pointed by \a __p.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_store1_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 0, 0);
  _mm_store_ps(__p, __a);
}

/// \brief Stores the lower 32 bits of a 128-bit vector of [4 x float] into
///    four contiguous elements in an aligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to <c> VMOVAPS / MOVAPS + shuffling </c>
///    instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location.
/// \param __a
///    A 128-bit vector of [4 x float] whose lower 32 bits are stored to each
///    of the four contiguous elements pointed by \a __p.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_store_ps1(float *__p, __m128 __a)
{
  return _mm_store1_ps(__p, __a);
}

/// \brief Stores float values from a 128-bit vector of [4 x float] to an
///    aligned memory location in reverse order.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS + shuffling </c>
///    instruction.
///
/// \param __p
///    A pointer to a 128-bit memory location. The address of the memory
///    location has to be 128-bit aligned.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_storer_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
  _mm_store_ps(__p, __a);
}












/// \brief Loads one cache line of data from the specified address to a location
///    closer to the processor.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// void _mm_prefetch(__const void * a, __const int sel);
/// \endcode
///
/// This intrinsic corresponds to the <c> PREFETCHNTA </c> instruction.
///
/// \param a
///    A pointer to a memory location containing a cache line of data.
/// \param sel
///    A predefined integer constant specifying the type of prefetch
///    operation: \n
///    0: Move data using the non-temporal access (NTA) hint. The
///    PREFETCHNTA instruction will be generated. \n
///    3: Move data using the T0 hint. The PREFETCHT0 instruction will
///    be generated. \n
///    2: Move data using the T1 hint. The PREFETCHT1 instruction will
///    be generated. \n
///    1: Move data using the T2 hint. The PREFETCHT2 instruction will
///    be generated.




/// \brief Stores a 64-bit integer in the specified aligned memory location. To
///    minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MOVNTQ </c> instruction.
///
/// \param __p
///    A pointer to an aligned memory location used to store the register value.
/// \param __a
///    A 64-bit integer containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_stream_pi(__m64 *__p, __m64 __a)
{
  __builtin_ia32_movntq(__p, __a);
}

/// \brief Moves packed float values from a 128-bit vector of [4 x float] to a
///    128-bit aligned memory location. To minimize caching, the data is flagged
///    as non-temporal (unlikely to be used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit aligned memory location that will receive the
///    single-precision floating-point values.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be moved.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_stream_ps(float *__p, __m128 __a)
{
  __builtin_nontemporal_store((__v4sf)__a, (__v4sf*)__p);
}





/// \brief Forces strong memory ordering (serialization) between store
///    instructions preceding this instruction and store instructions following
///    this instruction, ensuring the system completes all previous stores
///    before executing subsequent stores.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> SFENCE </c> instruction.
///
void _mm_sfence(void);





/// \brief Extracts 16-bit element from a 64-bit vector of [4 x i16] and
///    returns it, as specified by the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_extract_pi16(__m64 a, int n);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPEXTRW / PEXTRW </c> instruction.
///
/// \param a
///    A 64-bit vector of [4 x i16].
/// \param n
///    An immediate integer operand that determines which bits are extracted: \n
///    0: Bits [15:0] are copied to the destination. \n
///    1: Bits [31:16] are copied to the destination. \n
///    2: Bits [47:32] are copied to the destination. \n
///    3: Bits [63:48] are copied to the destination.
/// \returns A 16-bit integer containing the extracted 16 bits of packed data.



/// \brief Copies data from the 64-bit vector of [4 x i16] to the destination,
///    and inserts the lower 16-bits of an integer operand at the 16-bit offset
///    specified by the immediate operand \a n.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m64 _mm_insert_pi16(__m64 a, int d, int n);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPINSRW / PINSRW </c> instruction.
///
/// \param a
///    A 64-bit vector of [4 x i16].
/// \param d
///    An integer. The lower 16-bit value from this operand is written to the
///    destination at the offset specified by operand \a n.
/// \param n
///    An immediate integer operant that determines which the bits to be used
///    in the destination. \n
///    0: Bits [15:0] are copied to the destination. \n
///    1: Bits [31:16] are copied to the destination. \n
///    2: Bits [47:32] are copied to the destination. \n
///    3: Bits [63:48] are copied to the destination.  \n
///    The remaining bits in the destination are copied from the corresponding
///    bits in operand \a a.
/// \returns A 64-bit integer vector containing the copied packed data from the
///    operands.



/// \brief Compares each of the corresponding packed 16-bit integer values of
///    the 64-bit integer vectors, and writes the greater value to the
///    corresponding bits in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMAXSW </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the comparison results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_max_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxsw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Compares each of the corresponding packed 8-bit unsigned integer
///    values of the 64-bit integer vectors, and writes the greater value to the
///    corresponding bits in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMAXUB </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the comparison results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_max_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxub((__v8qi)__a, (__v8qi)__b);
}

/// \brief Compares each of the corresponding packed 16-bit integer values of
///    the 64-bit integer vectors, and writes the lesser value to the
///    corresponding bits in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMINSW </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the comparison results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_min_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminsw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Compares each of the corresponding packed 8-bit unsigned integer
///    values of the 64-bit integer vectors, and writes the lesser value to the
///    corresponding bits in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMINUB </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the comparison results.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_min_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminub((__v8qi)__a, (__v8qi)__b);
}

/// \brief Takes the most significant bit from each 8-bit element in a 64-bit
///    integer vector to create a 16-bit mask value. Zero-extends the value to
///    32-bit integer and writes it to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMOVMSKB </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing the values with bits to be extracted.
/// \returns The most significant bit from each 8-bit element in the operand,
///    written to bits [15:0].
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_movemask_pi8(__m64 __a)
{
  return __builtin_ia32_pmovmskb((__v8qi)__a);
}

/// \brief Multiplies packed 16-bit unsigned integer values and writes the
///    high-order 16 bits of each 32-bit product to the corresponding bits in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMULHUW </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the products of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_mulhi_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmulhuw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Shuffles the 4 16-bit integers from a 64-bit integer vector to the
///    destination, as specified by the immediate value operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m64 _mm_shuffle_pi16(__m64 a, __const int n);
/// \endcode
///
/// This intrinsic corresponds to the <c> PSHUFW </c> instruction.
///
/// \param a
///    A 64-bit integer vector containing the values to be shuffled.
/// \param n
///    An immediate value containing an 8-bit value specifying which elements to
///    copy from \a a. The destinations within the 64-bit destination are
///    assigned values as follows: \n
///    Bits [1:0] are used to assign values to bits [15:0] in the
///    destination. \n
///    Bits [3:2] are used to assign values to bits [31:16] in the
///    destination. \n
///    Bits [5:4] are used to assign values to bits [47:32] in the
///    destination. \n
///    Bits [7:6] are used to assign values to bits [63:48] in the
///    destination. \n
///    Bit value assignments: \n
///    00: assigned from bits [15:0] of \a a. \n
///    01: assigned from bits [31:16] of \a a. \n
///    10: assigned from bits [47:32] of \a a. \n
///    11: assigned from bits [63:48] of \a a.
/// \returns A 64-bit integer vector containing the shuffled values.



/// \brief Conditionally copies the values from each 8-bit element in the first
///    64-bit integer vector operand to the specified memory location, as
///    specified by the most significant bit in the corresponding element in the
///    second 64-bit integer vector operand.
///
///    To minimize caching, the data is flagged as non-temporal
///    (unlikely to be used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MASKMOVQ </c> instruction.
///
/// \param __d
///    A 64-bit integer vector containing the values with elements to be copied.
/// \param __n
///    A 64-bit integer vector operand. The most significant bit from each 8-bit
///    element determines whether the corresponding element in operand \a __d
///    is copied. If the most significant bit of a given element is 1, the
///    corresponding element in operand \a __d is copied.
/// \param __p
///    A pointer to a 64-bit memory location that will receive the conditionally
///    copied integer values. The address of the memory location does not have
///    to be aligned.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_maskmove_si64(__m64 __d, __m64 __n, char *__p)
{
  __builtin_ia32_maskmovq((__v8qi)__d, (__v8qi)__n, __p);
}

/// \brief Computes the rounded averages of the packed unsigned 8-bit integer
///    values and writes the averages to the corresponding bits in the
///    destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PAVGB </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the averages of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_avg_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgb((__v8qi)__a, (__v8qi)__b);
}

/// \brief Computes the rounded averages of the packed unsigned 16-bit integer
///    values and writes the averages to the corresponding bits in the
///    destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PAVGW </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector containing the averages of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_avg_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Subtracts the corresponding 8-bit unsigned integer values of the two
///    64-bit vector operands and computes the absolute value for each of the
///    difference. Then sum of the 8 absolute differences is written to the
///    bits [15:0] of the destination; the remaining bits [63:16] are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSADBW </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing one of the source operands.
/// \param __b
///    A 64-bit integer vector containing one of the source operands.
/// \returns A 64-bit integer vector whose lower 16 bits contain the sums of the
///    sets of absolute differences between both operands. The upper bits are
///    cleared.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_sad_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_psadbw((__v8qi)__a, (__v8qi)__b);
}





/// \brief Returns the contents of the MXCSR register as a 32-bit unsigned
///    integer value.
///
///    There are several groups of macros associated with this
///    intrinsic, including:
///    <ul>
///    <li>
///      For checking exception states: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO,
///      _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW,
///      _MM_EXCEPT_INEXACT. There is a convenience wrapper
///      _MM_GET_EXCEPTION_STATE().
///    </li>
///    <li>
///      For checking exception masks: _MM_MASK_UNDERFLOW, _MM_MASK_OVERFLOW,
///      _MM_MASK_INVALID, _MM_MASK_DENORM, _MM_MASK_DIV_ZERO, _MM_MASK_INEXACT.
///      There is a convenience wrapper _MM_GET_EXCEPTION_MASK().
///    </li>
///    <li>
///      For checking rounding modes: _MM_ROUND_NEAREST, _MM_ROUND_DOWN,
///      _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO. There is a convenience wrapper
///      _MM_GET_ROUNDING_MODE(x) where x is one of these macros.
///    </li>
///    <li>
///      For checking flush-to-zero mode: _MM_FLUSH_ZERO_ON, _MM_FLUSH_ZERO_OFF.
///      There is a convenience wrapper _MM_GET_FLUSH_ZERO_MODE().
///    </li>
///    <li>
///      For checking denormals-are-zero mode: _MM_DENORMALS_ZERO_ON,
///      _MM_DENORMALS_ZERO_OFF. There is a convenience wrapper
///      _MM_GET_DENORMALS_ZERO_MODE().
///    </li>
///    </ul>
///
///    For example, the expression below checks if an overflow exception has
///    occurred:
///      ( _mm_getcsr() & _MM_EXCEPT_OVERFLOW )
///
///    The following example gets the current rounding mode:
///      _MM_GET_ROUNDING_MODE()
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSTMXCSR / STMXCSR </c> instruction.
///
/// \returns A 32-bit unsigned integer containing the contents of the MXCSR
///    register.
unsigned int _mm_getcsr(void);

/// \brief Sets the MXCSR register with the 32-bit unsigned integer value.
///
///    There are several groups of macros associated with this intrinsic,
///    including:
///    <ul>
///    <li>
///      For setting exception states: _MM_EXCEPT_INVALID, _MM_EXCEPT_DIV_ZERO,
///      _MM_EXCEPT_DENORM, _MM_EXCEPT_OVERFLOW, _MM_EXCEPT_UNDERFLOW,
///      _MM_EXCEPT_INEXACT. There is a convenience wrapper
///      _MM_SET_EXCEPTION_STATE(x) where x is one of these macros.
///    </li>
///    <li>
///      For setting exception masks: _MM_MASK_UNDERFLOW, _MM_MASK_OVERFLOW,
///      _MM_MASK_INVALID, _MM_MASK_DENORM, _MM_MASK_DIV_ZERO, _MM_MASK_INEXACT.
///      There is a convenience wrapper _MM_SET_EXCEPTION_MASK(x) where x is one
///      of these macros.
///    </li>
///    <li>
///      For setting rounding modes: _MM_ROUND_NEAREST, _MM_ROUND_DOWN,
///      _MM_ROUND_UP, _MM_ROUND_TOWARD_ZERO. There is a convenience wrapper
///      _MM_SET_ROUNDING_MODE(x) where x is one of these macros.
///    </li>
///    <li>
///      For setting flush-to-zero mode: _MM_FLUSH_ZERO_ON, _MM_FLUSH_ZERO_OFF.
///      There is a convenience wrapper _MM_SET_FLUSH_ZERO_MODE(x) where x is
///      one of these macros.
///    </li>
///    <li>
///      For setting denormals-are-zero mode: _MM_DENORMALS_ZERO_ON,
///      _MM_DENORMALS_ZERO_OFF. There is a convenience wrapper
///      _MM_SET_DENORMALS_ZERO_MODE(x) where x is one of these macros.
///    </li>
///    </ul>
///
///    For example, the following expression causes subsequent floating-point
///    operations to round up:
///      _mm_setcsr(_mm_getcsr() | _MM_ROUND_UP)
///
///    The following example sets the DAZ and FTZ flags:
///      void setFlags() {
///        _MM_SET_FLUSH_ZERO_MODE(_MM_FLUSH_ZERO_ON)
///        _MM_SET_DENORMALS_ZERO_MODE(_MM_DENORMALS_ZERO_ON)
///      }
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VLDMXCSR / LDMXCSR </c> instruction.
///
/// \param __i
///    A 32-bit unsigned integer value to be written to the MXCSR register.
void _mm_setcsr(unsigned int __i);





/// \brief Selects 4 float values from the 128-bit operands of [4 x float], as
///    specified by the immediate value operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_shuffle_ps(__m128 a, __m128 b, __const int mask);
/// \endcode
///
/// This intrinsic corresponds to the <c> VSHUFPS / SHUFPS </c> instruction.
///
/// \param a
///    A 128-bit vector of [4 x float].
/// \param b
///    A 128-bit vector of [4 x float].
/// \param mask
///    An immediate value containing an 8-bit value specifying which elements to
///    copy from \a a and \a b. \n
///    Bits [3:0] specify the values copied from operand \a a. \n
///    Bits [7:4] specify the values copied from operand \a b. \n
///    The destinations within the 128-bit destination are assigned values as
///    follows: \n
///    Bits [1:0] are used to assign values to bits [31:0] in the
///    destination. \n
///    Bits [3:2] are used to assign values to bits [63:32] in the
///    destination. \n
///    Bits [5:4] are used to assign values to bits [95:64] in the
///    destination. \n
///    Bits [7:6] are used to assign values to bits [127:96] in the
///    destination. \n
///    Bit value assignments: \n
///    00: Bits [31:0] copied from the specified operand. \n
///    01: Bits [63:32] copied from the specified operand. \n
///    10: Bits [95:64] copied from the specified operand. \n
///    11: Bits [127:96] copied from the specified operand.
/// \returns A 128-bit vector of [4 x float] containing the shuffled values.







/// \brief Unpacks the high-order (index 2,3) values from two 128-bit vectors of
///    [4 x float] and interleaves them into a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKHPS / UNPCKHPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. \n
///    Bits [95:64] are written to bits [31:0] of the destination. \n
///    Bits [127:96] are written to bits [95:64] of the destination.
/// \param __b
///    A 128-bit vector of [4 x float].
///    Bits [95:64] are written to bits [63:32] of the destination. \n
///    Bits [127:96] are written to bits [127:96] of the destination.
/// \returns A 128-bit vector of [4 x float] containing the interleaved values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_unpackhi_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 2, 6, 3, 7);
}

/// \brief Unpacks the low-order (index 0,1) values from two 128-bit vectors of
///    [4 x float] and interleaves them into a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPS / UNPCKLPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. \n
///    Bits [31:0] are written to bits [31:0] of the destination.  \n
///    Bits [63:32] are written to bits [95:64] of the destination.
/// \param __b
///    A 128-bit vector of [4 x float]. \n
///    Bits [31:0] are written to bits [63:32] of the destination. \n
///    Bits [63:32] are written to bits [127:96] of the destination.
/// \returns A 128-bit vector of [4 x float] containing the interleaved values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_unpacklo_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 4, 1, 5);
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]. The lower
///    32 bits are set to the lower 32 bits of the second parameter. The upper
///    96 bits are set to the upper 96 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSS / MOVSS </c> instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float]. The upper 96 bits are
///    written to the upper 96 bits of the result.
/// \param __b
///    A 128-bit floating-point vector of [4 x float]. The lower 32 bits are
///    written to the lower 32 bits of the result.
/// \returns A 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_move_ss(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 4, 1, 2, 3);
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]. The lower
///    64 bits are set to the upper 64 bits of the second parameter. The upper
///    64 bits are set to the upper 64 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKHPD / UNPCKHPD </c> instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float]. The upper 64 bits are
///    written to the upper 64 bits of the result.
/// \param __b
///    A 128-bit floating-point vector of [4 x float]. The upper 64 bits are
///    written to the lower 64 bits of the result.
/// \returns A 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_movehl_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 6, 7, 2, 3);
}

/// \brief Constructs a 128-bit floating-point vector of [4 x float]. The lower
///    64 bits are set to the lower 64 bits of the first parameter. The upper
///    64 bits are set to the lower 64 bits of the second parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float]. The lower 64 bits are
///    written to the lower 64 bits of the result.
/// \param __b
///    A 128-bit floating-point vector of [4 x float]. The lower 64 bits are
///    written to the upper 64 bits of the result.
/// \returns A 128-bit floating-point vector of [4 x float].
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_movelh_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 1, 4, 5);
}

/// \brief Converts a 64-bit vector of [4 x i16] into a 128-bit vector of [4 x
///    float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16]. The elements of the destination are copied
///    from the corresponding elements in this operand.
/// \returns A 128-bit vector of [4 x float] containing the copied and converted
///    values from the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpi16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi16(__b, __a);
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}

/// \brief Converts a 64-bit vector of 16-bit unsigned integer values into a
///    128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.
///
/// \param __a
///    A 64-bit vector of 16-bit unsigned integer values. The elements of the
///    destination are copied from the corresponding elements in this operand.
/// \returns A 128-bit vector of [4 x float] containing the copied and converted
///    values from the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpu16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}

/// \brief Converts the lower four 8-bit values from a 64-bit vector of [8 x i8]
///    into a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.
///
/// \param __a
///    A 64-bit vector of [8 x i8]. The elements of the destination are copied
///    from the corresponding lower 4 elements in this operand.
/// \returns A 128-bit vector of [4 x float] containing the copied and converted
///    values from the operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpi8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi8(__b, __a);
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}

/// \brief Converts the lower four unsigned 8-bit integer values from a 64-bit
///    vector of [8 x u8] into a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.
///
/// \param __a
///    A 64-bit vector of unsigned 8-bit integer values. The elements of the
///    destination are copied from the corresponding lower 4 elements in this
///    operand.
/// \returns A 128-bit vector of [4 x float] containing the copied and converted
///    values from the source operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpu8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}

/// \brief Converts the two 32-bit __signed integer values from each 64-bit vector
///    operand of [2 x i32] into a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PS + COMPOSITE </c> instruction.
///
/// \param __a
///    A 64-bit vector of [2 x i32]. The lower elements of the destination are
///    copied from the elements in this operand.
/// \param __b
///    A 64-bit vector of [2 x i32]. The upper elements of the destination are
///    copied from the elements in this operand.
/// \returns A 128-bit vector of [4 x float] whose lower 64 bits contain the
///    copied and converted values from the first operand. The upper 64 bits
///    contain the copied and converted values from the second operand.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtpi32x2_ps(__m64 __a, __m64 __b)
{
  __m128 __c;

  __c = _mm_setzero_ps();
  __c = _mm_cvtpi32_ps(__c, __b);
  __c = _mm_movelh_ps(__c, __c);

  return _mm_cvtpi32_ps(__c, __a);
}

/// \brief Converts each single-precision floating-point element of a 128-bit
///    floating-point vector of [4 x float] into a 16-bit __signed integer, and
///    packs the results into a 64-bit integer vector of [4 x i16].
///
///    If the floating-point element is NaN or infinity, or if the
///    floating-point element is greater than 0x7FFFFFFF or less than -0x8000,
///    it is converted to 0x8000. Otherwise if the floating-point element is
///    greater than 0x7FFF, it is converted to 0x7FFF.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPS2PI + COMPOSITE </c> instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float].
/// \returns A 64-bit integer vector of [4 x i16] containing the converted
///    values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtps_pi16(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi32(__a);
  __a = _mm_movehl_ps(__a, __a);
  __c = _mm_cvtps_pi32(__a);

  return _mm_packs_pi32(__b, __c);
}

/// \brief Converts each single-precision floating-point element of a 128-bit
///    floating-point vector of [4 x float] into an 8-bit __signed integer, and
///    packs the results into the lower 32 bits of a 64-bit integer vector of
///    [8 x i8]. The upper 32 bits of the vector are set to 0.
///
///    If the floating-point element is NaN or infinity, or if the
///    floating-point element is greater than 0x7FFFFFFF or less than -0x80, it
///    is converted to 0x80. Otherwise if the floating-point element is greater
///    than 0x7F, it is converted to 0x7F.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPS2PI + COMPOSITE </c> instruction.
///
/// \param __a
///    128-bit floating-point vector of [4 x float].
/// \returns A 64-bit integer vector of [8 x i8]. The lower 32 bits contain the
///    converted values and the uppper 32 bits are set to zero.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_cvtps_pi8(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi16(__a);
  __c = _mm_setzero_si64();

  return _mm_packs_pi16(__b, __c);
}

/// \brief Extracts the sign bits from each single-precision floating-point
///    element of a 128-bit floating-point vector of [4 x float] and returns the
///    sign bits in bits [0:3] of the result. Bits [31:4] of the result are set
///    to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVMSKPS / MOVMSKPS </c> instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float].
/// \returns A 32-bit integer value. Bits [3:0] contain the sign bits from each
///    single-precision floating-point element of the parameter. Bits [31:4] are
///    set to zero.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse")))
_mm_movemask_ps(__m128 __a)
{
  return __builtin_ia32_movmskps((__v4sf)__a);
}











































# 2947 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 3 4



# 2964 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 3 4






# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 3 4



# 28 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 2 3 4

typedef double __m128d __attribute__((__vector_size__(16)));
typedef long long __m128i __attribute__((__vector_size__(16)));


typedef double __v2df __attribute__ ((__vector_size__ (16)));
typedef long long __v2di __attribute__ ((__vector_size__ (16)));
typedef short __v8hi __attribute__((__vector_size__(16)));
typedef char __v16qi __attribute__((__vector_size__(16)));


typedef unsigned long long __v2du __attribute__ ((__vector_size__ (16)));
typedef unsigned short __v8hu __attribute__((__vector_size__(16)));
typedef unsigned char __v16qu __attribute__((__vector_size__(16)));



typedef __signed char __v16qs __attribute__((__vector_size__(16)));


# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/f16cintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/f16cintrin.h" 3 4













/// \brief Converts a 16-bit half-precision float value into a 32-bit float
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.
///
/// \param __a
///    A 16-bit half-precision float value.
/// \returns The converted 32-bit float value.
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("f16c")))
_cvtsh_ss(unsigned short __a)
{
  __v8hi v = {(short)__a, 0, 0, 0, 0, 0, 0, 0};
  __v4sf r = __builtin_ia32_vcvtph2ps(v);
  return r[0];
}

/// \brief Converts a 32-bit single-precision float value to a 16-bit
///    half-precision float value.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// unsigned short _cvtss_sh(float a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.
///
/// \param a
///    A 32-bit single-precision float value to be converted to a 16-bit
///    half-precision float value.
/// \param imm
///    An immediate value controlling rounding using bits [2:0]: \n
///    000: Nearest \n
///    001: Down \n
///    010: Up \n
///    011: Truncate \n
///    1XX: Use MXCSR.RC for rounding
/// \returns The converted 16-bit half-precision float value.




/// \brief Converts a 128-bit vector containing 32-bit float values into a
///    128-bit vector containing 16-bit half-precision float values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_cvtps_ph(__m128 a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.
///
/// \param a
///    A 128-bit vector containing 32-bit float values.
/// \param imm
///    An immediate value controlling rounding using bits [2:0]: \n
///    000: Nearest \n
///    001: Down \n
///    010: Up \n
///    011: Truncate \n
///    1XX: Use MXCSR.RC for rounding
/// \returns A 128-bit vector containing converted 16-bit half-precision float
///    values. The lower 64 bits are used to store the converted 16-bit
///    half-precision floating-point values.



/// \brief Converts a 128-bit vector containing 16-bit half-precision float
///    values into a 128-bit vector containing 32-bit float values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.
///
/// \param __a
///    A 128-bit vector containing 16-bit half-precision float values. The lower
///    64 bits are used in the conversion.
/// \returns A 128-bit vector of [4 x float] containing converted float values.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("f16c")))
_mm_cvtph_ps(__m128i __a)
{
  return (__m128)__builtin_ia32_vcvtph2ps((__v8hi)__a);
}




# 48 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 2 3 4




/// \brief Adds lower double-precision values in both operands and returns the
///    sum in the lower 64 bits of the result. The upper 64 bits of the result
///    are copied from the upper double-precision value of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSD / ADDSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    sum of the lower 64 bits of both operands. The upper 64 bits are copied
///    from the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_sd(__m128d __a, __m128d __b)
{
  __a[0] += __b[0];
  return __a;
}

/// \brief Adds two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDPD / ADDPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] containing the sums of both
///    operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2df)__a + (__v2df)__b);
}

/// \brief Subtracts the lower double-precision value of the second operand
///    from the lower double-precision value of the first operand and returns
///    the difference in the lower 64 bits of the result. The upper 64 bits of
///    the result are copied from the upper double-precision value of the first
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBSD / SUBSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the minuend.
/// \param __b
///    A 128-bit vector of [2 x double] containing the subtrahend.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    difference of the lower 64 bits of both operands. The upper 64 bits are
///    copied from the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_sd(__m128d __a, __m128d __b)
{
  __a[0] -= __b[0];
  return __a;
}

/// \brief Subtracts two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBPD / SUBPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the minuend.
/// \param __b
///    A 128-bit vector of [2 x double] containing the subtrahend.
/// \returns A 128-bit vector of [2 x double] containing the differences between
///    both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2df)__a - (__v2df)__b);
}

/// \brief Multiplies lower double-precision values in both operands and returns
///    the product in the lower 64 bits of the result. The upper 64 bits of the
///    result are copied from the upper double-precision value of the first
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULSD / MULSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    product of the lower 64 bits of both operands. The upper 64 bits are
///    copied from the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mul_sd(__m128d __a, __m128d __b)
{
  __a[0] *= __b[0];
  return __a;
}

/// \brief Multiplies two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULPD / MULPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \returns A 128-bit vector of [2 x double] containing the products of both
///    operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mul_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2df)__a * (__v2df)__b);
}

/// \brief Divides the lower double-precision value of the first operand by the
///    lower double-precision value of the second operand and returns the
///    quotient in the lower 64 bits of the result. The upper 64 bits of the
///    result are copied from the upper double-precision value of the first
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVSD / DIVSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the dividend.
/// \param __b
///    A 128-bit vector of [2 x double] containing divisor.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    quotient of the lower 64 bits of both operands. The upper 64 bits are
///    copied from the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_div_sd(__m128d __a, __m128d __b)
{
  __a[0] /= __b[0];
  return __a;
}

/// \brief Performs an element-by-element division of two 128-bit vectors of
///    [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVPD / DIVPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the dividend.
/// \param __b
///    A 128-bit vector of [2 x double] containing the divisor.
/// \returns A 128-bit vector of [2 x double] containing the quotients of both
///    operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_div_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2df)__a / (__v2df)__b);
}

/// \brief Calculates the square root of the lower double-precision value of
///    the second operand and returns it in the lower 64 bits of the result.
///    The upper 64 bits of the result are copied from the upper
///    double-precision value of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTSD / SQRTSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    upper 64 bits of this operand are copied to the upper 64 bits of the
///    result.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    square root is calculated using the lower 64 bits of this operand.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    square root of the lower 64 bits of operand \a __b, and whose upper 64
///    bits are copied from the upper 64 bits of operand \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sqrt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_sqrtsd((__v2df)__b);
  return (__m128d) { __c[0], __a[1] };
}

/// \brief Calculates the square root of the each of two values stored in a
///    128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTPD / SQRTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector of [2 x double] containing the square roots of the
///    values in the operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sqrt_pd(__m128d __a)
{
  return __builtin_ia32_sqrtpd((__v2df)__a);
}

/// \brief Compares lower 64-bit double-precision values of both operands, and
///    returns the lesser of the pair of values in the lower 64-bits of the
///    result. The upper 64 bits of the result are copied from the upper
///    double-precision value of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINSD / MINSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    lower 64 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    lower 64 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    minimum value between both operands. The upper 64 bits are copied from
///    the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_min_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_minsd((__v2df)__a, (__v2df)__b);
}

/// \brief Performs element-by-element comparison of the two 128-bit vectors of
///    [2 x double] and returns the vector containing the lesser of each pair of
///    values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINPD / MINPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \returns A 128-bit vector of [2 x double] containing the minimum values
///    between both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_min_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_minpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares lower 64-bit double-precision values of both operands, and
///    returns the greater of the pair of values in the lower 64-bits of the
///    result. The upper 64 bits of the result are copied from the upper
///    double-precision value of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXSD / MAXSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    lower 64 bits of this operand are used in the comparison.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands. The
///    lower 64 bits of this operand are used in the comparison.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    maximum value between both operands. The upper 64 bits are copied from
///    the upper 64 bits of the first source operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_max_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_maxsd((__v2df)__a, (__v2df)__b);
}

/// \brief Performs element-by-element comparison of the two 128-bit vectors of
///    [2 x double] and returns the vector containing the greater of each pair
///    of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXPD / MAXPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the operands.
/// \returns A 128-bit vector of [2 x double] containing the maximum values
///    between both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_max_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_maxpd((__v2df)__a, (__v2df)__b);
}

/// \brief Performs a bitwise AND of two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPAND / PAND </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] containing the bitwise AND of the
///    values between both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_and_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2du)__a & (__v2du)__b);
}

/// \brief Performs a bitwise AND of two 128-bit vectors of [2 x double], using
///    the one's complement of the values contained in the first source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPANDN / PANDN </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the left source operand. The
///    one's complement of this value is used in the bitwise AND.
/// \param __b
///    A 128-bit vector of [2 x double] containing the right source operand.
/// \returns A 128-bit vector of [2 x double] containing the bitwise AND of the
///    values in the second operand and the one's complement of the first
///    operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_andnot_pd(__m128d __a, __m128d __b)
{
  return (__m128d)(~(__v2du)__a & (__v2du)__b);
}

/// \brief Performs a bitwise OR of two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPOR / POR </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] containing the bitwise OR of the
///    values between both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_or_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2du)__a | (__v2du)__b);
}

/// \brief Performs a bitwise XOR of two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPXOR / PXOR </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
/// \returns A 128-bit vector of [2 x double] containing the bitwise XOR of the
///    values between both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_xor_pd(__m128d __a, __m128d __b)
{
  return (__m128d)((__v2du)__a ^ (__v2du)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] for equality. Each comparison yields 0h
///    for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPEQPD / CMPEQPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpeq_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpeqpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are less than those in the second operand. Each comparison
///    yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTPD / CMPLTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmplt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are less than or equal to those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLEPD / CMPLEPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmple_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are greater than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTPD / CMPLTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpgt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__b, (__v2df)__a);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are greater than or equal to those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLEPD / CMPLEPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpge_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__b, (__v2df)__a);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are ordered with respect to those in the second operand.
///
///    A pair of double-precision values are "ordered" with respect to each
///    other if neither value is a NaN. Each comparison yields 0h for false,
///    FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPORDPD / CMPORDPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpord_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpordpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are unordered with respect to those in the second operand.
///
///    A pair of double-precision values are "unordered" with respect to each
///    other if one or both values are NaN. Each comparison yields 0h for false,
///    FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPUNORDPD / CMPUNORDPD </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpunord_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpunordpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are unequal to those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNEQPD / CMPNEQPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpneq_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpneqpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are not less than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTPD / CMPNLTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnlt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are not less than or equal to those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLEPD / CMPNLEPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnle_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are not greater than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTPD / CMPNLTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpngt_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__b, (__v2df)__a);
}

/// \brief Compares each of the corresponding double-precision values of the
///    128-bit vectors of [2 x double] to determine if the values in the first
///    operand are not greater than or equal to those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLEPD / CMPNLEPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector containing the comparison results.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnge_pd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__b, (__v2df)__a);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] for equality.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPEQSD / CMPEQSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpeq_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpeqsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than the corresponding value in
///    the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTSD / CMPLTSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmplt_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpltsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLESD / CMPLESD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmple_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmplesd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than the corresponding value
///    in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLTSD / CMPLTSD </c> instruction.
///
/// \param __a
///     A 128-bit vector of [2 x double]. The lower double-precision value is
///     compared to the lower double-precision value of \a __b.
/// \param __b
///     A 128-bit vector of [2 x double]. The lower double-precision value is
///     compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///     results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpgt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpltsd((__v2df)__b, (__v2df)__a);
  return (__m128d) { __c[0], __a[1] };
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPLESD / CMPLESD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpge_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmplesd((__v2df)__b, (__v2df)__a);
  return (__m128d) { __c[0], __a[1] };
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is "ordered" with respect to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true. A pair of
///    double-precision values are "ordered" with respect to each other if
///    neither value is a NaN.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPORDSD / CMPORDSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpord_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpordsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is "unordered" with respect to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true. A pair of
///    double-precision values are "unordered" with respect to each other if one
///    or both values are NaN.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPUNORDSD / CMPUNORDSD </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpunord_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpunordsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is unequal to the corresponding value in
///    the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNEQSD / CMPNEQSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpneq_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpneqsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is not less than the corresponding
///    value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTSD / CMPNLTSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnlt_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnltsd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is not less than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLESD / CMPNLESD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns  A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnle_sd(__m128d __a, __m128d __b)
{
  return (__m128d)__builtin_ia32_cmpnlesd((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is not greater than the corresponding
///    value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLTSD / CMPNLTSD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpngt_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpnltsd((__v2df)__b, (__v2df)__a);
  return (__m128d) { __c[0], __a[1] };
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is not greater than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0h for false, FFFFFFFFFFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCMPNLESD / CMPNLESD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns A 128-bit vector. The lower 64 bits contains the comparison
///    results. The upper 64 bits are copied from the upper 64 bits of \a __a.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpnge_sd(__m128d __a, __m128d __b)
{
  __m128d __c = __builtin_ia32_cmpnlesd((__v2df)__b, (__v2df)__a);
  return (__m128d) { __c[0], __a[1] };
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] for equality.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///    lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comieq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdeq((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than the corresponding value in
///    the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comilt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdlt((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///     A 128-bit vector of [2 x double]. The lower double-precision value is
///     compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comile_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdle((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than the corresponding value
///    in the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comigt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdgt((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///    lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comige_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdge((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is unequal to the corresponding value in
///    the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two
///    lower double-precision values is NaN, 1 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCOMISD / COMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 1 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_comineq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_comisdneq((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] for equality. The
///    comparison yields 0 for false, 1 for true.
///
///    If either of the two lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///    lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomieq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdeq((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than the corresponding value in
///    the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two lower
///    double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///    lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomilt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdlt((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is less than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two lower
///    double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///     A 128-bit vector of [2 x double]. The lower double-precision value is
///     compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomile_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdle((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than the corresponding value
///    in the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two lower
///    double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///     A 128-bit vector of [2 x double]. The lower double-precision value is
///     compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///     lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomigt_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdgt((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is greater than or equal to the
///    corresponding value in the second parameter.
///
///    The comparison yields 0 for false, 1 for true.  If either of the two
///    lower double-precision values is NaN, 0 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison results. If either of the two
///    lower double-precision values is NaN, 0 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomige_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdge((__v2df)__a, (__v2df)__b);
}

/// \brief Compares the lower double-precision floating-point values in each of
///    the two 128-bit floating-point vectors of [2 x double] to determine if
///    the value in the first parameter is unequal to the corresponding value in
///    the second parameter.
///
///    The comparison yields 0 for false, 1 for true. If either of the two lower
///    double-precision values is NaN, 1 is returned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUCOMISD / UCOMISD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __b.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision value is
///    compared to the lower double-precision value of \a __a.
/// \returns An integer containing the comparison result. If either of the two
///    lower double-precision values is NaN, 1 is returned.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_ucomineq_sd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_ucomisdneq((__v2df)__a, (__v2df)__b);
}

/// \brief Converts the two double-precision floating-point elements of a
///    128-bit vector of [2 x double] into two single-precision floating-point
///    values, returned in the lower 64 bits of a 128-bit vector of [4 x float].
///    The upper 64 bits of the result vector are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPD2PS / CVTPD2PS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector of [4 x float] whose lower 64 bits contain the
///    converted values. The upper 64 bits are set to zero.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtpd_ps(__m128d __a)
{
  return __builtin_ia32_cvtpd2ps((__v2df)__a);
}

/// \brief Converts the lower two single-precision floating-point elements of a
///    128-bit vector of [4 x float] into two double-precision floating-point
///    values, returned in a 128-bit vector of [2 x double]. The upper two
///    elements of the input vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPS2PD / CVTPS2PD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The lower two single-precision
///    floating-point elements are converted to double-precision values. The
///    upper two elements are unused.
/// \returns A 128-bit vector of [2 x double] containing the converted values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtps_pd(__m128 __a)
{
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1), __v2df);
}

/// \brief Converts the lower two integer elements of a 128-bit vector of
///    [4 x i32] into two double-precision floating-point values, returned in a
///    128-bit vector of [2 x double].
///
///    The upper two elements of the input vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTDQ2PD / CVTDQ2PD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector of [4 x i32]. The lower two integer elements are
///    converted to double-precision values.
///
///    The upper two elements are unused.
/// \returns A 128-bit vector of [2 x double] containing the converted values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtepi32_pd(__m128i __a)
{
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4si)__a, (__v4si)__a, 0, 1), __v2df);
}

/// \brief Converts the two double-precision floating-point elements of a
///    128-bit vector of [2 x double] into two __signed 32-bit integer values,
///    returned in the lower 64 bits of a 128-bit vector of [4 x i32]. The upper
///    64 bits of the result vector are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPD2DQ / CVTPD2DQ </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector of [4 x i32] whose lower 64 bits contain the
///    converted values. The upper 64 bits are set to zero.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtpd_epi32(__m128d __a)
{
  return __builtin_ia32_cvtpd2dq((__v2df)__a);
}

/// \brief Converts the low-order element of a 128-bit vector of [2 x double]
///    into a 32-bit __signed integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSD2SI / CVTSD2SI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the
///    conversion.
/// \returns A 32-bit __signed integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsd_si32(__m128d __a)
{
  return __builtin_ia32_cvtsd2si((__v2df)__a);
}

/// \brief Converts the lower double-precision floating-point element of a
///    128-bit vector of [2 x double], in the second parameter, into a
///    single-precision floating-point value, returned in the lower 32 bits of a
///    128-bit vector of [4 x float]. The upper 96 bits of the result vector are
///    copied from the upper 96 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSD2SS / CVTSD2SS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. The upper 96 bits of this parameter are
///    copied to the upper 96 bits of the result.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower double-precision
///    floating-point element is used in the conversion.
/// \returns A 128-bit vector of [4 x float]. The lower 32 bits contain the
///    converted value from the second parameter. The upper 96 bits are copied
///    from the upper 96 bits of the first parameter.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsd_ss(__m128 __a, __m128d __b)
{
  return (__m128)__builtin_ia32_cvtsd2ss((__v4sf)__a, (__v2df)__b);
}

/// \brief Converts a 32-bit __signed integer value, in the second parameter, into
///    a double-precision floating-point value, returned in the lower 64 bits of
///    a 128-bit vector of [2 x double]. The upper 64 bits of the result vector
///    are copied from the upper 64 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSI2SD / CVTSI2SD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The upper 64 bits of this parameter are
///    copied to the upper 64 bits of the result.
/// \param __b
///    A 32-bit __signed integer containing the value to be converted.
/// \returns A 128-bit vector of [2 x double]. The lower 64 bits contain the
///    converted value from the second parameter. The upper 64 bits are copied
///    from the upper 64 bits of the first parameter.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi32_sd(__m128d __a, int __b)
{
  __a[0] = __b;
  return __a;
}

/// \brief Converts the lower single-precision floating-point element of a
///    128-bit vector of [4 x float], in the second parameter, into a
///    double-precision floating-point value, returned in the lower 64 bits of
///    a 128-bit vector of [2 x double]. The upper 64 bits of the result vector
///    are copied from the upper 64 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSS2SD / CVTSS2SD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The upper 64 bits of this parameter are
///    copied to the upper 64 bits of the result.
/// \param __b
///    A 128-bit vector of [4 x float]. The lower single-precision
///    floating-point element is used in the conversion.
/// \returns A 128-bit vector of [2 x double]. The lower 64 bits contain the
///    converted value from the second parameter. The upper 64 bits are copied
///    from the upper 64 bits of the first parameter.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtss_sd(__m128d __a, __m128 __b)
{
  __a[0] = __b[0];
  return __a;
}

/// \brief Converts the two double-precision floating-point elements of a
///    128-bit vector of [2 x double] into two __signed 32-bit integer values,
///    returned in the lower 64 bits of a 128-bit vector of [4 x i32].
///
///    If the result of either conversion is inexact, the result is truncated
///    (rounded towards zero) regardless of the current MXCSR setting. The upper
///    64 bits of the result vector are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTPD2DQ / CVTTPD2DQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector of [4 x i32] whose lower 64 bits contain the
///    converted values. The upper 64 bits are set to zero.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvttpd_epi32(__m128d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq((__v2df)__a);
}

/// \brief Converts the low-order element of a [2 x double] vector into a 32-bit
///    __signed integer value, truncating the result when it is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTSD2SI / CVTTSD2SI </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the
///    conversion.
/// \returns A 32-bit __signed integer containing the converted value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvttsd_si32(__m128d __a)
{
  return __builtin_ia32_cvttsd2si((__v2df)__a);
}

/// \brief Converts the two double-precision floating-point elements of a
///    128-bit vector of [2 x double] into two __signed 32-bit integer values,
///    returned in a 64-bit vector of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPD2PI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 64-bit vector of [2 x i32] containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtpd_pi32(__m128d __a)
{
  return (__m64)__builtin_ia32_cvtpd2pi((__v2df)__a);
}

/// \brief Converts the two double-precision floating-point elements of a
///    128-bit vector of [2 x double] into two __signed 32-bit integer values,
///    returned in a 64-bit vector of [2 x i32].
///
///    If the result of either conversion is inexact, the result is truncated
///    (rounded towards zero) regardless of the current MXCSR setting.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTTPD2PI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 64-bit vector of [2 x i32] containing the converted values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvttpd_pi32(__m128d __a)
{
  return (__m64)__builtin_ia32_cvttpd2pi((__v2df)__a);
}

/// \brief Converts the two __signed 32-bit integer elements of a 64-bit vector of
///    [2 x i32] into two double-precision floating-point values, returned in a
///    128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CVTPI2PD </c> instruction.
///
/// \param __a
///    A 64-bit vector of [2 x i32].
/// \returns A 128-bit vector of [2 x double] containing the converted values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtpi32_pd(__m64 __a)
{
  return __builtin_ia32_cvtpi2pd((__v2si)__a);
}

/// \brief Returns the low-order element of a 128-bit vector of [2 x double] as
///    a double-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower 64 bits are returned.
/// \returns A double-precision floating-point value copied from the lower 64
///    bits of \a __a.
static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsd_f64(__m128d __a)
{
  return __a[0];
}

/// \brief Loads a 128-bit floating-point vector of [2 x double] from an aligned
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPD / MOVAPD </c> instruction.
///
/// \param __dp
///    A pointer to a 128-bit memory location. The address of the memory
///    location has to be 16-byte aligned.
/// \returns A 128-bit vector of [2 x double] containing the loaded values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_load_pd(double __const *__dp)
{
  return *(__m128d*)__dp;
}

/// \brief Loads a double-precision floating-point value from a specified memory
///    location and duplicates it to both vector elements of a 128-bit vector of
///    [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP / MOVDDUP </c> instruction.
///
/// \param __dp
///    A pointer to a memory location containing a double-precision value.
/// \returns A 128-bit vector of [2 x double] containing the loaded and
///    duplicated values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_load1_pd(double __const *__dp)
{
  struct __mm_load1_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_load1_pd_struct*)__dp)->__u;
  return (__m128d){ __u, __u };
}



/// \brief Loads two double-precision values, in reverse order, from an aligned
///    memory location into a 128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPD / MOVAPD </c> instruction +
/// needed shuffling instructions. In AVX mode, the shuffling may be combined
/// with the \c VMOVAPD, resulting in only a \c VPERMILPD instruction.
///
/// \param __dp
///    A 16-byte aligned pointer to an array of double-precision values to be
///    loaded in reverse order.
/// \returns A 128-bit vector of [2 x double] containing the reversed loaded
///    values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadr_pd(double __const *__dp)
{
  __m128d __u = *(__m128d*)__dp;
  return __builtin_shufflevector((__v2df)__u, (__v2df)__u, 1, 0);
}

/// \brief Loads a 128-bit floating-point vector of [2 x double] from an
///    unaligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPD / MOVUPD </c> instruction.
///
/// \param __dp
///    A pointer to a 128-bit memory location. The address of the memory
///    location does not have to be aligned.
/// \returns A 128-bit vector of [2 x double] containing the loaded values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadu_pd(double __const *__dp)
{
  struct __loadu_pd {
    __m128d __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_pd*)__dp)->__v;
}

/// \brief Loads a 64-bit integer value to the low element of a 128-bit integer
///    vector and clears the upper element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __a
///    A pointer to a 64-bit memory location. The address of the memory
///    location does not have to be aligned.
/// \returns A 128-bit vector of [2 x i64] containing the loaded value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadu_si64(void __const *__a)
{
  struct __loadu_si64 {
    long long __v;
  } __attribute__((__packed__, __may_alias__));
  long long __u = ((struct __loadu_si64*)__a)->__v;
  return (__m128i){__u, 0L};
}

/// \brief Loads a 64-bit double-precision value to the low element of a
///    128-bit integer vector and clears the upper element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSD / MOVSD </c> instruction.
///
/// \param __dp
///    A pointer to a memory location containing a double-precision value.
///    The address of the memory location does not have to be aligned.
/// \returns A 128-bit vector of [2 x double] containing the loaded value.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_load_sd(double __const *__dp)
{
  struct __mm_load_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_load_sd_struct*)__dp)->__u;
  return (__m128d){ __u, 0 };
}

/// \brief Loads a double-precision value into the high-order bits of a 128-bit
///    vector of [2 x double]. The low-order bits are copied from the low-order
///    bits of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. \n
///    Bits [63:0] are written to bits [63:0] of the result.
/// \param __dp
///    A pointer to a 64-bit memory location containing a double-precision
///    floating-point value that is loaded. The loaded value is written to bits
///    [127:64] of the result. The address of the memory location does not have
///    to be aligned.
/// \returns A 128-bit vector of [2 x double] containing the moved values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadh_pd(__m128d __a, double __const *__dp)
{
  struct __mm_loadh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_loadh_pd_struct*)__dp)->__u;
  return (__m128d){ __a[0], __u };
}

/// \brief Loads a double-precision value into the low-order bits of a 128-bit
///    vector of [2 x double]. The high-order bits are copied from the
///    high-order bits of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. \n
///    Bits [127:64] are written to bits [127:64] of the result.
/// \param __dp
///    A pointer to a 64-bit memory location containing a double-precision
///    floating-point value that is loaded. The loaded value is written to bits
///    [63:0] of the result. The address of the memory location does not have to
///    be aligned.
/// \returns A 128-bit vector of [2 x double] containing the moved values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadl_pd(__m128d __a, double __const *__dp)
{
  struct __mm_loadl_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((struct __mm_loadl_pd_struct*)__dp)->__u;
  return (__m128d){ __u, __a[1] };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double] with
///    unspecified content. This could be used as an argument to another
///    intrinsic function where the argument is required but the value is not
///    actually used.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 128-bit floating-point vector of [2 x double] with unspecified
///    content.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_undefined_pd(void)
{
  return (__m128d)__builtin_ia32_undef128();
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double]. The lower
///    64 bits of the vector are initialized with the specified double-precision
///    floating-point value. The upper 64 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize the lower 64
///    bits of the result.
/// \returns An initialized 128-bit floating-point vector of [2 x double]. The
///    lower 64 bits contain the value of the parameter. The upper 64 bits are
///    set to zero.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_sd(double __w)
{
  return (__m128d){ __w, 0 };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double], with each
///    of the two double-precision floating-point vector elements set to the
///    specified double-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP / MOVLHPS </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 128-bit floating-point vector of [2 x double].
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_pd(double __w)
{
  return (__m128d){ __w, __w };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double], with each
///    of the two double-precision floating-point vector elements set to the
///    specified double-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP / MOVLHPS </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 128-bit floating-point vector of [2 x double].
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_pd1(double __w)
{
  return _mm_set1_pd(__w);
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double]
///    initialized with the specified double-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize the upper 64
///    bits of the result.
/// \param __x
///    A double-precision floating-point value used to initialize the lower 64
///    bits of the result.
/// \returns An initialized 128-bit floating-point vector of [2 x double].
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_pd(double __w, double __x)
{
  return (__m128d){ __x, __w };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double],
///    initialized in reverse order with the specified double-precision
///    floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize the lower 64
///    bits of the result.
/// \param __x
///    A double-precision floating-point value used to initialize the upper 64
///    bits of the result.
/// \returns An initialized 128-bit floating-point vector of [2 x double].
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setr_pd(double __w, double __x)
{
  return (__m128d){ __w, __x };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double]
///    initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.
///
/// \returns An initialized 128-bit floating-point vector of [2 x double] with
///    all elements set to zero.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setzero_pd(void)
{
  return (__m128d){ 0, 0 };
}

/// \brief Constructs a 128-bit floating-point vector of [2 x double]. The lower
///    64 bits are set to the lower 64 bits of the second parameter. The upper
///    64 bits are set to the upper 64 bits of the first parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBLENDPD / BLENDPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The upper 64 bits are written to the
///    upper 64 bits of the result.
/// \param __b
///    A 128-bit vector of [2 x double]. The lower 64 bits are written to the
///    lower 64 bits of the result.
/// \returns A 128-bit vector of [2 x double] containing the moved values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_move_sd(__m128d __a, __m128d __b)
{
  return (__m128d){ __b[0], __a[1] };
}

/// \brief Stores the lower 64 bits of a 128-bit vector of [2 x double] to a
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSD / MOVSD </c> instruction.
///
/// \param __dp
///    A pointer to a 64-bit memory location.
/// \param __a
///    A 128-bit vector of [2 x double] containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_store_sd(double *__dp, __m128d __a)
{
  struct __mm_store_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sd_struct*)__dp)->__u = __a[0];
}

/// \brief Moves packed double-precision values from a 128-bit vector of
///    [2 x double] to a memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c>VMOVAPD / MOVAPS</c> instruction.
///
/// \param __dp
///    A pointer to an aligned memory location that can store two
///    double-precision values.
/// \param __a
///    A packed 128-bit vector of [2 x double] containing the values to be
///    moved.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_store_pd(double *__dp, __m128d __a)
{
  *(__m128d*)__dp = __a;
}

/// \brief Moves the lower 64 bits of a 128-bit vector of [2 x double] twice to
///    the upper and lower 64 bits of a memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the
///   <c> VMOVDDUP + VMOVAPD / MOVLHPS + MOVAPS </c> instruction.
///
/// \param __dp
///    A pointer to a memory location that can store two double-precision
///    values.
/// \param __a
///    A 128-bit vector of [2 x double] whose lower 64 bits are copied to each
///    of the values in \a __dp.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_store1_pd(double *__dp, __m128d __a)
{
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
  _mm_store_pd(__dp, __a);
}

/// \brief Moves the lower 64 bits of a 128-bit vector of [2 x double] twice to
///    the upper and lower 64 bits of a memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the
///   <c> VMOVDDUP + VMOVAPD / MOVLHPS + MOVAPS </c> instruction.
///
/// \param __dp
///    A pointer to a memory location that can store two double-precision
///    values.
/// \param __a
///    A 128-bit vector of [2 x double] whose lower 64 bits are copied to each
///    of the values in \a __dp.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_store_pd1(double *__dp, __m128d __a)
{
  return _mm_store1_pd(__dp, __a);
}

/// \brief Stores a 128-bit vector of [2 x double] into an unaligned memory
///    location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPD / MOVUPD </c> instruction.
///
/// \param __dp
///    A pointer to a 128-bit memory location. The address of the memory
///    location does not have to be aligned.
/// \param __a
///    A 128-bit vector of [2 x double] containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storeu_pd(double *__dp, __m128d __a)
{
  struct __storeu_pd {
    __m128d __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__dp)->__v = __a;
}

/// \brief Stores two double-precision values, in reverse order, from a 128-bit
///    vector of [2 x double] to a 16-byte aligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to a shuffling instruction followed by a
/// <c> VMOVAPD / MOVAPD </c> instruction.
///
/// \param __dp
///    A pointer to a 16-byte aligned memory location that can store two
///    double-precision values.
/// \param __a
///    A 128-bit vector of [2 x double] containing the values to be reversed and
///    stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storer_pd(double *__dp, __m128d __a)
{
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 1, 0);
  *(__m128d *)__dp = __a;
}

/// \brief Stores the upper 64 bits of a 128-bit vector of [2 x double] to a
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVHPD / MOVHPD </c> instruction.
///
/// \param __dp
///    A pointer to a 64-bit memory location.
/// \param __a
///    A 128-bit vector of [2 x double] containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storeh_pd(double *__dp, __m128d __a)
{
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[1];
}

/// \brief Stores the lower 64 bits of a 128-bit vector of [2 x double] to a
///    memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVLPD / MOVLPD </c> instruction.
///
/// \param __dp
///    A pointer to a 64-bit memory location.
/// \param __a
///    A 128-bit vector of [2 x double] containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storel_pd(double *__dp, __m128d __a)
{
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct*)__dp)->__u = __a[0];
}

/// \brief Adds the corresponding elements of two 128-bit vectors of [16 x i8],
///    saving the lower 8 bits of each sum in the corresponding element of a
///    128-bit result vector of [16 x i8].
///
///    The integer elements of both parameters can be either __signed or unsigned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDB / PADDB </c> instruction.
///
/// \param __a
///    A 128-bit vector of [16 x i8].
/// \param __b
///    A 128-bit vector of [16 x i8].
/// \returns A 128-bit vector of [16 x i8] containing the sums of both
///    parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qu)__a + (__v16qu)__b);
}

/// \brief Adds the corresponding elements of two 128-bit vectors of [8 x i16],
///    saving the lower 16 bits of each sum in the corresponding element of a
///    128-bit result vector of [8 x i16].
///
///    The integer elements of both parameters can be either __signed or unsigned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDW / PADDW </c> instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16].
/// \param __b
///    A 128-bit vector of [8 x i16].
/// \returns A 128-bit vector of [8 x i16] containing the sums of both
///    parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hu)__a + (__v8hu)__b);
}

/// \brief Adds the corresponding elements of two 128-bit vectors of [4 x i32],
///    saving the lower 32 bits of each sum in the corresponding element of a
///    128-bit result vector of [4 x i32].
///
///    The integer elements of both parameters can be either __signed or unsigned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDD / PADDD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32].
/// \param __b
///    A 128-bit vector of [4 x i32].
/// \returns A 128-bit vector of [4 x i32] containing the sums of both
///    parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a + (__v4su)__b);
}

/// \brief Adds two __signed or unsigned 64-bit integer values, returning the
///    lower 64 bits of the sum.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PADDQ </c> instruction.
///
/// \param __a
///    A 64-bit integer.
/// \param __b
///    A 64-bit integer.
/// \returns A 64-bit integer containing the sum of both parameters.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_si64(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_paddq((__v1di)__a, (__v1di)__b);
}

/// \brief Adds the corresponding elements of two 128-bit vectors of [2 x i64],
///    saving the lower 64 bits of each sum in the corresponding element of a
///    128-bit result vector of [2 x i64].
///
///    The integer elements of both parameters can be either __signed or unsigned.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDQ / PADDQ </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x i64].
/// \param __b
///    A 128-bit vector of [2 x i64].
/// \returns A 128-bit vector of [2 x i64] containing the sums of both
///    parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_add_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a + (__v2du)__b);
}

/// \brief Adds, with saturation, the corresponding elements of two 128-bit
///    __signed [16 x i8] vectors, saving each sum in the corresponding element of
///    a 128-bit result vector of [16 x i8]. Positive sums greater than 7Fh are
///    saturated to 7Fh. Negative sums less than 80h are saturated to 80h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDSB / PADDSB </c> instruction.
///
/// \param __a
///    A 128-bit __signed [16 x i8] vector.
/// \param __b
///    A 128-bit __signed [16 x i8] vector.
/// \returns A 128-bit __signed [16 x i8] vector containing the saturated sums of
///    both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_adds_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddsb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Adds, with saturation, the corresponding elements of two 128-bit
///    __signed [8 x i16] vectors, saving each sum in the corresponding element of
///    a 128-bit result vector of [8 x i16]. Positive sums greater than 7FFFh
///    are saturated to 7FFFh. Negative sums less than 8000h are saturated to
///    8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDSW / PADDSW </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [8 x i16] vector containing the saturated sums of
///    both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_adds_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Adds, with saturation, the corresponding elements of two 128-bit
///    unsigned [16 x i8] vectors, saving each sum in the corresponding element
///    of a 128-bit result vector of [16 x i8]. Positive sums greater than FFh
///    are saturated to FFh. Negative sums are saturated to 00h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDUSB / PADDUSB </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [16 x i8] vector.
/// \param __b
///    A 128-bit unsigned [16 x i8] vector.
/// \returns A 128-bit unsigned [16 x i8] vector containing the saturated sums
///    of both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_adds_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddusb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Adds, with saturation, the corresponding elements of two 128-bit
///    unsigned [8 x i16] vectors, saving each sum in the corresponding element
///    of a 128-bit result vector of [8 x i16]. Positive sums greater than FFFFh
///    are saturated to FFFFh. Negative sums are saturated to 0000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPADDUSB / PADDUSB </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [8 x i16] vector.
/// \param __b
///    A 128-bit unsigned [8 x i16] vector.
/// \returns A 128-bit unsigned [8 x i16] vector containing the saturated sums
///    of both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_adds_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_paddusw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Computes the rounded avarages of corresponding elements of two
///    128-bit unsigned [16 x i8] vectors, saving each result in the
///    corresponding element of a 128-bit result vector of [16 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPAVGB / PAVGB </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [16 x i8] vector.
/// \param __b
///    A 128-bit unsigned [16 x i8] vector.
/// \returns A 128-bit unsigned [16 x i8] vector containing the rounded
///    averages of both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_avg_epu8(__m128i __a, __m128i __b)
{
  typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));
  return (__m128i)__builtin_convertvector(
               ((__builtin_convertvector((__v16qu)__a, __v16hu) +
                 __builtin_convertvector((__v16qu)__b, __v16hu)) + 1)
                 >> 1, __v16qu);
}

/// \brief Computes the rounded avarages of corresponding elements of two
///    128-bit unsigned [8 x i16] vectors, saving each result in the
///    corresponding element of a 128-bit result vector of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPAVGW / PAVGW </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [8 x i16] vector.
/// \param __b
///    A 128-bit unsigned [8 x i16] vector.
/// \returns A 128-bit unsigned [8 x i16] vector containing the rounded
///    averages of both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_avg_epu16(__m128i __a, __m128i __b)
{
  typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));
  return (__m128i)__builtin_convertvector(
               ((__builtin_convertvector((__v8hu)__a, __v8su) +
                 __builtin_convertvector((__v8hu)__b, __v8su)) + 1)
                 >> 1, __v8hu);
}

/// \brief Multiplies the corresponding elements of two 128-bit __signed [8 x i16]
///    vectors, producing eight intermediate 32-bit __signed integer products, and
///    adds the consecutive pairs of 32-bit products to form a 128-bit __signed
///    [4 x i32] vector.
///
///    For example, bits [15:0] of both parameters are multiplied producing a
///    32-bit product, bits [31:16] of both parameters are multiplied producing
///    a 32-bit product, and the sum of those two products becomes bits [31:0]
///    of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMADDWD / PMADDWD </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [4 x i32] vector containing the sums of products
///    of both parameters.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_madd_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Compares corresponding elements of two 128-bit __signed [8 x i16]
///    vectors, saving the greater value from each comparison in the
///    corresponding element of a 128-bit result vector of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXSW / PMAXSW </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [8 x i16] vector containing the greater value of
///    each comparison.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_max_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaxsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Compares corresponding elements of two 128-bit unsigned [16 x i8]
///    vectors, saving the greater value from each comparison in the
///    corresponding element of a 128-bit result vector of [16 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXUB / PMAXUB </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [16 x i8] vector.
/// \param __b
///    A 128-bit unsigned [16 x i8] vector.
/// \returns A 128-bit unsigned [16 x i8] vector containing the greater value of
///    each comparison.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_max_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmaxub128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Compares corresponding elements of two 128-bit __signed [8 x i16]
///    vectors, saving the smaller value from each comparison in the
///    corresponding element of a 128-bit result vector of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINSW / PMINSW </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [8 x i16] vector containing the smaller value of
///    each comparison.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_min_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pminsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Compares corresponding elements of two 128-bit unsigned [16 x i8]
///    vectors, saving the smaller value from each comparison in the
///    corresponding element of a 128-bit result vector of [16 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINUB / PMINUB </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [16 x i8] vector.
/// \param __b
///    A 128-bit unsigned [16 x i8] vector.
/// \returns A 128-bit unsigned [16 x i8] vector containing the smaller value of
///    each comparison.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_min_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pminub128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Multiplies the corresponding elements of two __signed [8 x i16]
///    vectors, saving the upper 16 bits of each 32-bit product in the
///    corresponding element of a 128-bit __signed [8 x i16] result vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULHW / PMULHW </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [8 x i16] vector containing the upper 16 bits of
///    each of the eight 32-bit products.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mulhi_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Multiplies the corresponding elements of two unsigned [8 x i16]
///    vectors, saving the upper 16 bits of each 32-bit product in the
///    corresponding element of a 128-bit unsigned [8 x i16] result vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULHUW / PMULHUW </c> instruction.
///
/// \param __a
///    A 128-bit unsigned [8 x i16] vector.
/// \param __b
///    A 128-bit unsigned [8 x i16] vector.
/// \returns A 128-bit unsigned [8 x i16] vector containing the upper 16 bits
///    of each of the eight 32-bit products.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mulhi_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Multiplies the corresponding elements of two __signed [8 x i16]
///    vectors, saving the lower 16 bits of each 32-bit product in the
///    corresponding element of a 128-bit __signed [8 x i16] result vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULLW / PMULLW </c> instruction.
///
/// \param __a
///    A 128-bit __signed [8 x i16] vector.
/// \param __b
///    A 128-bit __signed [8 x i16] vector.
/// \returns A 128-bit __signed [8 x i16] vector containing the lower 16 bits of
///    each of the eight 32-bit products.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mullo_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hu)__a * (__v8hu)__b);
}

/// \brief Multiplies 32-bit unsigned integer values contained in the lower bits
///    of the two 64-bit integer vectors and returns the 64-bit unsigned
///    product.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PMULUDQ </c> instruction.
///
/// \param __a
///    A 64-bit integer containing one of the source operands.
/// \param __b
///    A 64-bit integer containing one of the source operands.
/// \returns A 64-bit integer vector containing the product of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mul_su32(__m64 __a, __m64 __b)
{
  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);
}

/// \brief Multiplies 32-bit unsigned integer values contained in the lower
///    bits of the corresponding elements of two [2 x i64] vectors, and returns
///    the 64-bit products in the corresponding elements of a [2 x i64] vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULUDQ / PMULUDQ </c> instruction.
///
/// \param __a
///    A [2 x i64] vector containing one of the source operands.
/// \param __b
///    A [2 x i64] vector containing one of the source operands.
/// \returns A [2 x i64] vector containing the product of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_mul_epu32(__m128i __a, __m128i __b)
{
  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);
}

/// \brief Computes the absolute differences of corresponding 8-bit integer
///    values in two 128-bit vectors. Sums the first 8 absolute differences, and
///    separately sums the second 8 absolute differences. Packs these two
///    unsigned 16-bit integer sums into the upper and lower elements of a
///    [2 x i64] vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSADBW / PSADBW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing one of the source operands.
/// \param __b
///    A 128-bit integer vector containing one of the source operands.
/// \returns A [2 x i64] vector containing the sums of the sets of absolute
///    differences between both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sad_epu8(__m128i __a, __m128i __b)
{
  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Subtracts the corresponding 8-bit integer values in the operands.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBB / PSUBB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qu)__a - (__v16qu)__b);
}

/// \brief Subtracts the corresponding 16-bit integer values in the operands.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBW / PSUBW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hu)__a - (__v8hu)__b);
}

/// \brief Subtracts the corresponding 32-bit integer values in the operands.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBD / PSUBD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a - (__v4su)__b);
}

/// \brief Subtracts __signed or unsigned 64-bit integer values and writes the
///    difference to the corresponding bits in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PSUBQ </c> instruction.
///
/// \param __a
///    A 64-bit integer vector containing the minuend.
/// \param __b
///    A 64-bit integer vector containing the subtrahend.
/// \returns A 64-bit integer vector containing the difference of the values in
///    the operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_si64(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_psubq((__v1di)__a, (__v1di)__b);
}

/// \brief Subtracts the corresponding elements of two [2 x i64] vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBQ / PSUBQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sub_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a - (__v2du)__b);
}

/// \brief Subtracts corresponding 8-bit __signed integer values in the input and
///    returns the differences in the corresponding bytes in the destination.
///    Differences greater than 7Fh are saturated to 7Fh, and differences less
///    than 80h are saturated to 80h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBSB / PSUBSB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_subs_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubsb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Subtracts corresponding 16-bit __signed integer values in the input and
///    returns the differences in the corresponding bytes in the destination.
///    Differences greater than 7FFFh are saturated to 7FFFh, and values less
///    than 8000h are saturated to 8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBSW / PSUBSW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the differences of the values
///    in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_subs_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Subtracts corresponding 8-bit unsigned integer values in the input
///    and returns the differences in the corresponding bytes in the
///    destination. Differences less than 00h are saturated to 00h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBUSB / PSUBUSB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the unsigned integer
///    differences of the values in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_subs_epu8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubusb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Subtracts corresponding 16-bit unsigned integer values in the input
///    and returns the differences in the corresponding bytes in the
///    destination. Differences less than 0000h are saturated to 0000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSUBUSW / PSUBUSW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the minuends.
/// \param __b
///    A 128-bit integer vector containing the subtrahends.
/// \returns A 128-bit integer vector containing the unsigned integer
///    differences of the values in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_subs_epu16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_psubusw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Performs a bitwise AND of two 128-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPAND / PAND </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing one of the source operands.
/// \param __b
///    A 128-bit integer vector containing one of the source operands.
/// \returns A 128-bit integer vector containing the bitwise AND of the values
///    in both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_and_si128(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a & (__v2du)__b);
}

/// \brief Performs a bitwise AND of two 128-bit integer vectors, using the
///    one's complement of the values contained in the first source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPANDN / PANDN </c> instruction.
///
/// \param __a
///    A 128-bit vector containing the left source operand. The one's complement
///    of this value is used in the bitwise AND.
/// \param __b
///    A 128-bit vector containing the right source operand.
/// \returns A 128-bit integer vector containing the bitwise AND of the one's
///    complement of the first operand and the values in the second operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_andnot_si128(__m128i __a, __m128i __b)
{
  return (__m128i)(~(__v2du)__a & (__v2du)__b);
}
/// \brief Performs a bitwise OR of two 128-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPOR / POR </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing one of the source operands.
/// \param __b
///    A 128-bit integer vector containing one of the source operands.
/// \returns A 128-bit integer vector containing the bitwise OR of the values
///    in both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_or_si128(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a | (__v2du)__b);
}

/// \brief Performs a bitwise exclusive OR of two 128-bit integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPXOR / PXOR </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing one of the source operands.
/// \param __b
///    A 128-bit integer vector containing one of the source operands.
/// \returns A 128-bit integer vector containing the bitwise exclusive OR of the
///    values in both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_xor_si128(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a ^ (__v2du)__b);
}

/// \brief Left-shifts the 128-bit integer vector operand by the specified
///    number of bytes. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_slli_si128(__m128i a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPSLLDQ / PSLLDQ </c> instruction.
///
/// \param a
///    A 128-bit integer vector containing the source operand.
/// \param imm
///    An immediate value specifying the number of bytes to left-shift operand
///    \a a.
/// \returns A 128-bit integer vector containing the left-shifted value.

# 2817 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 3 4




/// \brief Left-shifts each 16-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLW / PSLLW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to left-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_slli_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);
}

/// \brief Left-shifts each 16-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLW / PSLLW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to left-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sll_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);
}

/// \brief Left-shifts each 32-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLD / PSLLD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to left-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_slli_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);
}

/// \brief Left-shifts each 32-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLD / PSLLD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to left-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sll_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);
}

/// \brief Left-shifts each 64-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLQ / PSLLQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to left-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_slli_epi64(__m128i __a, int __count)
{
  return __builtin_ia32_psllqi128((__v2di)__a, __count);
}

/// \brief Left-shifts each 64-bit value in the 128-bit integer vector operand
///    by the specified number of bits. Low-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSLLQ / PSLLQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to left-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the left-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sll_epi64(__m128i __a, __m128i __count)
{
  return __builtin_ia32_psllq128((__v2di)__a, (__v2di)__count);
}

/// \brief Right-shifts each 16-bit value in the 128-bit integer vector operand
///    by the specified number of bits. High-order bits are filled with the sign
///    bit of the initial value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRAW / PSRAW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to right-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srai_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);
}

/// \brief Right-shifts each 16-bit value in the 128-bit integer vector operand
///    by the specified number of bits. High-order bits are filled with the sign
///    bit of the initial value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRAW / PSRAW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to right-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sra_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);
}

/// \brief Right-shifts each 32-bit value in the 128-bit integer vector operand
///    by the specified number of bits. High-order bits are filled with the sign
///    bit of the initial value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRAD / PSRAD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to right-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srai_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);
}

/// \brief Right-shifts each 32-bit value in the 128-bit integer vector operand
///    by the specified number of bits. High-order bits are filled with the sign
///    bit of the initial value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRAD / PSRAD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to right-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_sra_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);
}

/// \brief Right-shifts the 128-bit integer vector operand by the specified
///    number of bytes. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_srli_si128(__m128i a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPSRLDQ / PSRLDQ </c> instruction.
///
/// \param a
///    A 128-bit integer vector containing the source operand.
/// \param imm
///    An immediate value specifying the number of bytes to right-shift operand
///    \a a.
/// \returns A 128-bit integer vector containing the right-shifted value.

# 3052 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/emmintrin.h" 3 4




/// \brief Right-shifts each of 16-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLW / PSRLW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to right-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srli_epi16(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);
}

/// \brief Right-shifts each of 16-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLW / PSRLW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to right-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srl_epi16(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);
}

/// \brief Right-shifts each of 32-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLD / PSRLD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to right-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srli_epi32(__m128i __a, int __count)
{
  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);
}

/// \brief Right-shifts each of 32-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLD / PSRLD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to right-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srl_epi32(__m128i __a, __m128i __count)
{
  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);
}

/// \brief Right-shifts each of 64-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLQ / PSRLQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    An integer value specifying the number of bits to right-shift each value
///    in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srli_epi64(__m128i __a, int __count)
{
  return __builtin_ia32_psrlqi128((__v2di)__a, __count);
}

/// \brief Right-shifts each of 64-bit values in the 128-bit integer vector
///    operand by the specified number of bits. High-order bits are cleared.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSRLQ / PSRLQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the source operand.
/// \param __count
///    A 128-bit integer vector in which bits [63:0] specify the number of bits
///    to right-shift each value in operand \a __a.
/// \returns A 128-bit integer vector containing the right-shifted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_srl_epi64(__m128i __a, __m128i __count)
{
  return __builtin_ia32_psrlq128((__v2di)__a, (__v2di)__count);
}

/// \brief Compares each of the corresponding 8-bit values of the 128-bit
///    integer vectors for equality. Each comparison yields 0h for false, FFh
///    for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPEQB / PCMPEQB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpeq_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)((__v16qi)__a == (__v16qi)__b);
}

/// \brief Compares each of the corresponding 16-bit values of the 128-bit
///    integer vectors for equality. Each comparison yields 0h for false, FFFFh
///    for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPEQW / PCMPEQW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpeq_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a == (__v8hi)__b);
}

/// \brief Compares each of the corresponding 32-bit values of the 128-bit
///    integer vectors for equality. Each comparison yields 0h for false,
///    FFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPEQD / PCMPEQD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpeq_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a == (__v4si)__b);
}

/// \brief Compares each of the corresponding __signed 8-bit values of the 128-bit
///    integer vectors to determine if the values in the first operand are
///    greater than those in the second operand. Each comparison yields 0h for
///    false, FFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTB / PCMPGTB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpgt_epi8(__m128i __a, __m128i __b)
{
  

  return (__m128i)((__v16qs)__a > (__v16qs)__b);
}

/// \brief Compares each of the corresponding __signed 16-bit values of the
///    128-bit integer vectors to determine if the values in the first operand
///    are greater than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTW / PCMPGTW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpgt_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)((__v8hi)__a > (__v8hi)__b);
}

/// \brief Compares each of the corresponding __signed 32-bit values of the
///    128-bit integer vectors to determine if the values in the first operand
///    are greater than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTD / PCMPGTD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmpgt_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4si)__a > (__v4si)__b);
}

/// \brief Compares each of the corresponding __signed 8-bit values of the 128-bit
///    integer vectors to determine if the values in the first operand are less
///    than those in the second operand.
///
///    Each comparison yields 0h for false, FFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTB / PCMPGTB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmplt_epi8(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi8(__b, __a);
}

/// \brief Compares each of the corresponding __signed 16-bit values of the
///    128-bit integer vectors to determine if the values in the first operand
///    are less than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTW / PCMPGTW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmplt_epi16(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi16(__b, __a);
}

/// \brief Compares each of the corresponding __signed 32-bit values of the
///    128-bit integer vectors to determine if the values in the first operand
///    are less than those in the second operand.
///
///    Each comparison yields 0h for false, FFFFFFFFh for true.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTD / PCMPGTD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __b
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cmplt_epi32(__m128i __a, __m128i __b)
{
  return _mm_cmpgt_epi32(__b, __a);
}


/// \brief Converts a 64-bit __signed integer value from the second operand into a
///    double-precision value and returns it in the lower element of a [2 x
///    double] vector; the upper element of the returned vector is copied from
///    the upper element of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSI2SD / CVTSI2SD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The upper 64 bits of this operand are
///    copied to the upper 64 bits of the destination.
/// \param __b
///    A 64-bit __signed integer operand containing the value to be converted.
/// \returns A 128-bit vector of [2 x double] whose lower 64 bits contain the
///    converted value of the second operand. The upper 64 bits are copied from
///    the upper 64 bits of the first operand.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi64_sd(__m128d __a, long long __b)
{
  __a[0] = __b;
  return __a;
}

/// \brief Converts the first (lower) element of a vector of [2 x double] into a
///    64-bit __signed integer value, according to the current rounding mode.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTSD2SI / CVTSD2SI </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the
///    conversion.
/// \returns A 64-bit __signed integer containing the converted value.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsd_si64(__m128d __a)
{
  return __builtin_ia32_cvtsd2si64((__v2df)__a);
}

/// \brief Converts the first (lower) element of a vector of [2 x double] into a
///    64-bit __signed integer value, truncating the result when it is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTSD2SI / CVTTSD2SI </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. The lower 64 bits are used in the
///    conversion.
/// \returns A 64-bit __signed integer containing the converted value.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvttsd_si64(__m128d __a)
{
  return __builtin_ia32_cvttsd2si64((__v2df)__a);
}


/// \brief Converts a vector of [4 x i32] into a vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTDQ2PS / CVTDQ2PS </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 128-bit vector of [4 x float] containing the converted values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtepi32_ps(__m128i __a)
{
  return __builtin_ia32_cvtdq2ps((__v4si)__a);
}

/// \brief Converts a vector of [4 x float] into a vector of [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPS2DQ / CVTPS2DQ </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit integer vector of [4 x i32] containing the converted
///    values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtps_epi32(__m128 __a)
{
  return (__m128i)__builtin_ia32_cvtps2dq((__v4sf)__a);
}

/// \brief Converts a vector of [4 x float] into a vector of [4 x i32],
///    truncating the result when it is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTPS2DQ / CVTTPS2DQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 128-bit vector of [4 x i32] containing the converted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvttps_epi32(__m128 __a)
{
  return (__m128i)__builtin_ia32_cvttps2dq((__v4sf)__a);
}

/// \brief Returns a vector of [4 x i32] where the lowest element is the input
///    operand and the remaining elements are zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.
///
/// \param __a
///    A 32-bit __signed integer operand.
/// \returns A 128-bit vector of [4 x i32].
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi32_si128(int __a)
{
  return (__m128i)(__v4si){ __a, 0, 0, 0 };
}


/// \brief Returns a vector of [2 x i64] where the lower element is the input
///    operand and the upper element is zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __a
///    A 64-bit __signed integer operand containing the value to be converted.
/// \returns A 128-bit vector of [2 x i64] containing the converted value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi64_si128(long long __a)
{
  return (__m128i){ __a, 0 };
}


/// \brief Moves the least significant 32 bits of a vector of [4 x i32] to a
///    32-bit __signed integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVD / MOVD </c> instruction.
///
/// \param __a
///    A vector of [4 x i32]. The least significant 32 bits are moved to the
///    destination.
/// \returns A 32-bit __signed integer containing the moved value.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi128_si32(__m128i __a)
{
  __v4si __b = (__v4si)__a;
  return __b[0];
}


/// \brief Moves the least significant 64 bits of a vector of [2 x i64] to a
///    64-bit __signed integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __a
///    A vector of [2 x i64]. The least significant 64 bits are moved to the
///    destination.
/// \returns A 64-bit __signed integer containing the moved value.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_cvtsi128_si64(__m128i __a)
{
  return __a[0];
}


/// \brief Moves packed integer values from an aligned 128-bit memory location
///    to elements in a 128-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQA / MOVDQA </c> instruction.
///
/// \param __p
///    An aligned pointer to a memory location containing integer values.
/// \returns A 128-bit integer vector containing the moved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_load_si128(__m128i __const *__p)
{
  return *__p;
}

/// \brief Moves packed integer values from an unaligned 128-bit memory location
///    to elements in a 128-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQU / MOVDQU </c> instruction.
///
/// \param __p
///    A pointer to a memory location containing integer values.
/// \returns A 128-bit integer vector containing the moved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadu_si128(__m128i __const *__p)
{
  struct __loadu_si128 {
    __m128i __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_si128*)__p)->__v;
}

/// \brief Returns a vector of [2 x i64] where the lower element is taken from
///    the lower element of the operand, and the upper element is zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __p
///    A 128-bit vector of [2 x i64]. Bits [63:0] are written to bits [63:0] of
///    the destination.
/// \returns A 128-bit vector of [2 x i64]. The lower order bits contain the
///    moved value. The higher order bits are cleared.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_loadl_epi64(__m128i __const *__p)
{
  struct __mm_loadl_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  return (__m128i) { ((struct __mm_loadl_epi64_struct*)__p)->__u, 0};
}

/// \brief Generates a 128-bit vector of [4 x i32] with unspecified content.
///    This could be used as an argument to another intrinsic function where the
///    argument is required but the value is not actually used.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 128-bit vector of [4 x i32] with unspecified content.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_undefined_si128(void)
{
  return (__m128i)__builtin_ia32_undef128();
}

/// \brief Initializes both 64-bit values in a 128-bit vector of [2 x i64] with
///    the specified 64-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __q1
///    A 64-bit integer value used to initialize the upper 64 bits of the
///    destination vector of [2 x i64].
/// \param __q0
///    A 64-bit integer value used to initialize the lower 64 bits of the
///    destination vector of [2 x i64].
/// \returns An initialized 128-bit vector of [2 x i64] containing the values
///    provided in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_epi64x(long long __q1, long long __q0)
{
  return (__m128i){ __q0, __q1 };
}

/// \brief Initializes both 64-bit values in a 128-bit vector of [2 x i64] with
///    the specified 64-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __q1
///    A 64-bit integer value used to initialize the upper 64 bits of the
///    destination vector of [2 x i64].
/// \param __q0
///    A 64-bit integer value used to initialize the lower 64 bits of the
///    destination vector of [2 x i64].
/// \returns An initialized 128-bit vector of [2 x i64] containing the values
///    provided in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_epi64(__m64 __q1, __m64 __q0)
{
  return (__m128i){ (long long)__q0, (long long)__q1 };
}

/// \brief Initializes the 32-bit values in a 128-bit vector of [4 x i32] with
///    the specified 32-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __i3
///    A 32-bit integer value used to initialize bits [127:96] of the
///    destination vector.
/// \param __i2
///    A 32-bit integer value used to initialize bits [95:64] of the destination
///    vector.
/// \param __i1
///    A 32-bit integer value used to initialize bits [63:32] of the destination
///    vector.
/// \param __i0
///    A 32-bit integer value used to initialize bits [31:0] of the destination
///    vector.
/// \returns An initialized 128-bit vector of [4 x i32] containing the values
///    provided in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_epi32(int __i3, int __i2, int __i1, int __i0)
{
  return (__m128i)(__v4si){ __i0, __i1, __i2, __i3};
}

/// \brief Initializes the 16-bit values in a 128-bit vector of [8 x i16] with
///    the specified 16-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __w7
///    A 16-bit integer value used to initialize bits [127:112] of the
///    destination vector.
/// \param __w6
///    A 16-bit integer value used to initialize bits [111:96] of the
///    destination vector.
/// \param __w5
///    A 16-bit integer value used to initialize bits [95:80] of the destination
///    vector.
/// \param __w4
///    A 16-bit integer value used to initialize bits [79:64] of the destination
///    vector.
/// \param __w3
///    A 16-bit integer value used to initialize bits [63:48] of the destination
///    vector.
/// \param __w2
///    A 16-bit integer value used to initialize bits [47:32] of the destination
///    vector.
/// \param __w1
///    A 16-bit integer value used to initialize bits [31:16] of the destination
///    vector.
/// \param __w0
///    A 16-bit integer value used to initialize bits [15:0] of the destination
///    vector.
/// \returns An initialized 128-bit vector of [8 x i16] containing the values
///    provided in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_epi16(short __w7, short __w6, short __w5, short __w4, short __w3, short __w2, short __w1, short __w0)
{
  return (__m128i)(__v8hi){ __w0, __w1, __w2, __w3, __w4, __w5, __w6, __w7 };
}

/// \brief Initializes the 8-bit values in a 128-bit vector of [16 x i8] with
///    the specified 8-bit integer values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __b15
///    Initializes bits [127:120] of the destination vector.
/// \param __b14
///    Initializes bits [119:112] of the destination vector.
/// \param __b13
///    Initializes bits [111:104] of the destination vector.
/// \param __b12
///    Initializes bits [103:96] of the destination vector.
/// \param __b11
///    Initializes bits [95:88] of the destination vector.
/// \param __b10
///    Initializes bits [87:80] of the destination vector.
/// \param __b9
///    Initializes bits [79:72] of the destination vector.
/// \param __b8
///    Initializes bits [71:64] of the destination vector.
/// \param __b7
///    Initializes bits [63:56] of the destination vector.
/// \param __b6
///    Initializes bits [55:48] of the destination vector.
/// \param __b5
///    Initializes bits [47:40] of the destination vector.
/// \param __b4
///    Initializes bits [39:32] of the destination vector.
/// \param __b3
///    Initializes bits [31:24] of the destination vector.
/// \param __b2
///    Initializes bits [23:16] of the destination vector.
/// \param __b1
///    Initializes bits [15:8] of the destination vector.
/// \param __b0
///    Initializes bits [7:0] of the destination vector.
/// \returns An initialized 128-bit vector of [16 x i8] containing the values
///    provided in the operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set_epi8(char __b15, char __b14, char __b13, char __b12, char __b11, char __b10, char __b9, char __b8, char __b7, char __b6, char __b5, char __b4, char __b3, char __b2, char __b1, char __b0)
{
  return (__m128i)(__v16qi){ __b0, __b1, __b2, __b3, __b4, __b5, __b6, __b7, __b8, __b9, __b10, __b11, __b12, __b13, __b14, __b15 };
}

/// \brief Initializes both values in a 128-bit integer vector with the
///    specified 64-bit integer value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __q
///    Integer value used to initialize the elements of the destination integer
///    vector.
/// \returns An initialized 128-bit integer vector of [2 x i64] with both
///    elements containing the value provided in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_epi64x(long long __q)
{
  return (__m128i){ __q, __q };
}

/// \brief Initializes both values in a 128-bit vector of [2 x i64] with the
///    specified 64-bit value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __q
///    A 64-bit value used to initialize the elements of the destination integer
///    vector.
/// \returns An initialized 128-bit vector of [2 x i64] with all elements
///    containing the value provided in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_epi64(__m64 __q)
{
  return (__m128i){ (long long)__q, (long long)__q };
}

/// \brief Initializes all values in a 128-bit vector of [4 x i32] with the
///    specified 32-bit value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __i
///    A 32-bit value used to initialize the elements of the destination integer
///    vector.
/// \returns An initialized 128-bit vector of [4 x i32] with all elements
///    containing the value provided in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_epi32(int __i)
{
  return (__m128i)(__v4si){ __i, __i, __i, __i };
}

/// \brief Initializes all values in a 128-bit vector of [8 x i16] with the
///    specified 16-bit value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __w
///    A 16-bit value used to initialize the elements of the destination integer
///    vector.
/// \returns An initialized 128-bit vector of [8 x i16] with all elements
///    containing the value provided in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_epi16(short __w)
{
  return (__m128i)(__v8hi){ __w, __w, __w, __w, __w, __w, __w, __w };
}

/// \brief Initializes all values in a 128-bit vector of [16 x i8] with the
///    specified 8-bit value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __b
///    An 8-bit value used to initialize the elements of the destination integer
///    vector.
/// \returns An initialized 128-bit vector of [16 x i8] with all elements
///    containing the value provided in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_set1_epi8(char __b)
{
  return (__m128i)(__v16qi){ __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b };
}

/// \brief Constructs a 128-bit integer vector, initialized in reverse order
///     with the specified 64-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic does not correspond to a specific instruction.
///
/// \param __q0
///    A 64-bit integral value used to initialize the lower 64 bits of the
///    result.
/// \param __q1
///    A 64-bit integral value used to initialize the upper 64 bits of the
///    result.
/// \returns An initialized 128-bit integer vector.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setr_epi64(__m64 __q0, __m64 __q1)
{
  return (__m128i){ (long long)__q0, (long long)__q1 };
}

/// \brief Constructs a 128-bit integer vector, initialized in reverse order
///     with the specified 32-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __i0
///    A 32-bit integral value used to initialize bits [31:0] of the result.
/// \param __i1
///    A 32-bit integral value used to initialize bits [63:32] of the result.
/// \param __i2
///    A 32-bit integral value used to initialize bits [95:64] of the result.
/// \param __i3
///    A 32-bit integral value used to initialize bits [127:96] of the result.
/// \returns An initialized 128-bit integer vector.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setr_epi32(int __i0, int __i1, int __i2, int __i3)
{
  return (__m128i)(__v4si){ __i0, __i1, __i2, __i3};
}

/// \brief Constructs a 128-bit integer vector, initialized in reverse order
///     with the specified 16-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __w0
///    A 16-bit integral value used to initialize bits [15:0] of the result.
/// \param __w1
///    A 16-bit integral value used to initialize bits [31:16] of the result.
/// \param __w2
///    A 16-bit integral value used to initialize bits [47:32] of the result.
/// \param __w3
///    A 16-bit integral value used to initialize bits [63:48] of the result.
/// \param __w4
///    A 16-bit integral value used to initialize bits [79:64] of the result.
/// \param __w5
///    A 16-bit integral value used to initialize bits [95:80] of the result.
/// \param __w6
///    A 16-bit integral value used to initialize bits [111:96] of the result.
/// \param __w7
///    A 16-bit integral value used to initialize bits [127:112] of the result.
/// \returns An initialized 128-bit integer vector.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setr_epi16(short __w0, short __w1, short __w2, short __w3, short __w4, short __w5, short __w6, short __w7)
{
  return (__m128i)(__v8hi){ __w0, __w1, __w2, __w3, __w4, __w5, __w6, __w7 };
}

/// \brief Constructs a 128-bit integer vector, initialized in reverse order
///     with the specified 8-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __b0
///    An 8-bit integral value used to initialize bits [7:0] of the result.
/// \param __b1
///    An 8-bit integral value used to initialize bits [15:8] of the result.
/// \param __b2
///    An 8-bit integral value used to initialize bits [23:16] of the result.
/// \param __b3
///    An 8-bit integral value used to initialize bits [31:24] of the result.
/// \param __b4
///    An 8-bit integral value used to initialize bits [39:32] of the result.
/// \param __b5
///    An 8-bit integral value used to initialize bits [47:40] of the result.
/// \param __b6
///    An 8-bit integral value used to initialize bits [55:48] of the result.
/// \param __b7
///    An 8-bit integral value used to initialize bits [63:56] of the result.
/// \param __b8
///    An 8-bit integral value used to initialize bits [71:64] of the result.
/// \param __b9
///    An 8-bit integral value used to initialize bits [79:72] of the result.
/// \param __b10
///    An 8-bit integral value used to initialize bits [87:80] of the result.
/// \param __b11
///    An 8-bit integral value used to initialize bits [95:88] of the result.
/// \param __b12
///    An 8-bit integral value used to initialize bits [103:96] of the result.
/// \param __b13
///    An 8-bit integral value used to initialize bits [111:104] of the result.
/// \param __b14
///    An 8-bit integral value used to initialize bits [119:112] of the result.
/// \param __b15
///    An 8-bit integral value used to initialize bits [127:120] of the result.
/// \returns An initialized 128-bit integer vector.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setr_epi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5, char __b6, char __b7, char __b8, char __b9, char __b10, char __b11, char __b12, char __b13, char __b14, char __b15)
{
  return (__m128i)(__v16qi){ __b0, __b1, __b2, __b3, __b4, __b5, __b6, __b7, __b8, __b9, __b10, __b11, __b12, __b13, __b14, __b15 };
}

/// \brief Creates a 128-bit integer vector initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS / XORPS </c> instruction.
///
/// \returns An initialized 128-bit integer vector with all elements set to
///    zero.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_setzero_si128(void)
{
  return (__m128i){ 0LL, 0LL };
}

/// \brief Stores a 128-bit integer vector to a memory location aligned on a
///    128-bit boundary.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS / MOVAPS </c> instruction.
///
/// \param __p
///    A pointer to an aligned memory location that will receive the integer
///    values.
/// \param __b
///    A 128-bit integer vector containing the values to be moved.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_store_si128(__m128i *__p, __m128i __b)
{
  *__p = __b;
}

/// \brief Stores a 128-bit integer vector to an unaligned memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPS / MOVUPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the integer values.
/// \param __b
///    A 128-bit integer vector containing the values to be moved.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storeu_si128(__m128i *__p, __m128i __b)
{
  struct __storeu_si128 {
    __m128i __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si128*)__p)->__v = __b;
}

/// \brief Moves bytes selected by the mask from the first operand to the
///    specified unaligned memory location. When a mask bit is 1, the
///    corresponding byte is written, otherwise it is not written.
///
///    To minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon). Exception and trap behavior for elements not selected
///    for storage to memory are implementation dependent.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVDQU / MASKMOVDQU </c>
///   instruction.
///
/// \param __d
///    A 128-bit integer vector containing the values to be moved.
/// \param __n
///    A 128-bit integer vector containing the mask. The most significant bit of
///    each byte represents the mask bits.
/// \param __p
///    A pointer to an unaligned 128-bit memory location where the specified
///    values are moved.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_maskmoveu_si128(__m128i __d, __m128i __n, char *__p)
{
  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);
}

/// \brief Stores the lower 64 bits of a 128-bit integer vector of [2 x i64] to
///    a memory location.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVLPS / MOVLPS </c> instruction.
///
/// \param __p
///    A pointer to a 64-bit memory location that will receive the lower 64 bits
///    of the integer vector parameter.
/// \param __a
///    A 128-bit integer vector of [2 x i64]. The lower 64 bits contain the
///    value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_storel_epi64(__m128i *__p, __m128i __a)
{
  struct __mm_storel_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storel_epi64_struct*)__p)->__u = __a[0];
}

/// \brief Stores a 128-bit floating point vector of [2 x double] to a 128-bit
///    aligned memory location.
///
///    To minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.
///
/// \param __p
///    A pointer to the 128-bit aligned memory location used to store the value.
/// \param __a
///    A vector of [2 x double] containing the 64-bit values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_stream_pd(double *__p, __m128d __a)
{
  __builtin_nontemporal_store((__v2df)__a, (__v2df*)__p);
}

/// \brief Stores a 128-bit integer vector to a 128-bit aligned memory location.
///
///    To minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTPS / MOVNTPS </c> instruction.
///
/// \param __p
///    A pointer to the 128-bit aligned memory location used to store the value.
/// \param __a
///    A 128-bit integer vector containing the values to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_stream_si128(__m128i *__p, __m128i __a)
{
  __builtin_nontemporal_store((__v2di)__a, (__v2di*)__p);
}

/// \brief Stores a 32-bit integer value in the specified memory location.
///
///    To minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MOVNTI </c> instruction.
///
/// \param __p
///    A pointer to the 32-bit memory location used to store the value.
/// \param __a
///    A 32-bit integer containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_stream_si32(int *__p, int __a)
{
  __builtin_ia32_movnti(__p, __a);
}


/// \brief Stores a 64-bit integer value in the specified memory location.
///
///    To minimize caching, the data is flagged as non-temporal (unlikely to be
///    used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MOVNTIQ </c> instruction.
///
/// \param __p
///    A pointer to the 64-bit memory location used to store the value.
/// \param __a
///    A 64-bit integer containing the value to be stored.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_stream_si64(long long *__p, long long __a)
{
  __builtin_ia32_movnti64(__p, __a);
}






/// \brief The cache line containing \a __p is flushed and invalidated from all
///    caches in the coherency domain.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CLFLUSH </c> instruction.
///
/// \param __p
///    A pointer to the memory location used to identify the cache line to be
///    flushed.
void _mm_clflush(void __const * __p);

/// \brief Forces strong memory ordering (serialization) between load
///    instructions preceding this instruction and load instructions following
///    this instruction, ensuring the system completes all previous loads before
///    executing subsequent loads.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> LFENCE </c> instruction.
///
void _mm_lfence(void);

/// \brief Forces strong memory ordering (serialization) between load and store
///    instructions preceding this instruction and load and store instructions
///    following this instruction, ensuring that the system completes all
///    previous memory accesses before executing subsequent memory accesses.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MFENCE </c> instruction.
///
void _mm_mfence(void);





/// \brief Converts 16-bit __signed integers from both 128-bit integer vector
///    operands into 8-bit __signed integers, and packs the results into the
///    destination. Positive values greater than 0x7F are saturated to 0x7F.
///    Negative values less than 0x80 are saturated to 0x80.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPACKSSWB / PACKSSWB </c> instruction.
///
/// \param __a
///   A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as
///   a __signed integer and is converted to a 8-bit __signed integer with
///   saturation. Values greater than 0x7F are saturated to 0x7F. Values less
///   than 0x80 are saturated to 0x80. The converted [8 x i8] values are
///   written to the lower 64 bits of the result.
/// \param __b
///   A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as
///   a __signed integer and is converted to a 8-bit __signed integer with
///   saturation. Values greater than 0x7F are saturated to 0x7F. Values less
///   than 0x80 are saturated to 0x80. The converted [8 x i8] values are
///   written to the higher 64 bits of the result.
/// \returns A 128-bit vector of [16 x i8] containing the converted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_packs_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Converts 32-bit __signed integers from both 128-bit integer vector
///    operands into 16-bit __signed integers, and packs the results into the
///    destination. Positive values greater than 0x7FFF are saturated to 0x7FFF.
///    Negative values less than 0x8000 are saturated to 0x8000.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPACKSSDW / PACKSSDW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector of [4 x i32]. Each 32-bit element is treated as
///    a __signed integer and is converted to a 16-bit __signed integer with
///    saturation. Values greater than 0x7FFF are saturated to 0x7FFF. Values
///    less than 0x8000 are saturated to 0x8000. The converted [4 x i16] values
///    are written to the lower 64 bits of the result.
/// \param __b
///    A 128-bit integer vector of [4 x i32]. Each 32-bit element is treated as
///    a __signed integer and is converted to a 16-bit __signed integer with
///    saturation. Values greater than 0x7FFF are saturated to 0x7FFF. Values
///    less than 0x8000 are saturated to 0x8000. The converted [4 x i16] values
///    are written to the higher 64 bits of the result.
/// \returns A 128-bit vector of [8 x i16] containing the converted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_packs_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);
}

/// \brief Converts 16-bit __signed integers from both 128-bit integer vector
///    operands into 8-bit unsigned integers, and packs the results into the
///    destination. Values greater than 0xFF are saturated to 0xFF. Values less
///    than 0x00 are saturated to 0x00.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPACKUSWB / PACKUSWB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as
///    a __signed integer and is converted to an 8-bit unsigned integer with
///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less
///    than 0x00 are saturated to 0x00. The converted [8 x i8] values are
///    written to the lower 64 bits of the result.
/// \param __b
///    A 128-bit integer vector of [8 x i16]. Each 16-bit element is treated as
///    a __signed integer and is converted to an 8-bit unsigned integer with
///    saturation. Values greater than 0xFF are saturated to 0xFF. Values less
///    than 0x00 are saturated to 0x00. The converted [8 x i8] values are
///    written to the higher 64 bits of the result.
/// \returns A 128-bit vector of [16 x i8] containing the converted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_packus_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Extracts 16 bits from a 128-bit integer vector of [8 x i16], using
///    the immediate-value parameter as a selector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPEXTRW / PEXTRW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \param __imm
///    An immediate value. Bits [2:0] selects values from \a __a to be assigned
///    to bits[15:0] of the result. \n
///    000: assign values from bits [15:0] of \a __a. \n
///    001: assign values from bits [31:16] of \a __a. \n
///    010: assign values from bits [47:32] of \a __a. \n
///    011: assign values from bits [63:48] of \a __a. \n
///    100: assign values from bits [79:64] of \a __a. \n
///    101: assign values from bits [95:80] of \a __a. \n
///    110: assign values from bits [111:96] of \a __a. \n
///    111: assign values from bits [127:112] of \a __a.
/// \returns An integer, whose lower 16 bits are selected from the 128-bit
///    integer vector parameter and the remaining bits are assigned zeros.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_extract_epi16(__m128i __a, int __imm)
{
  __v8hi __b = (__v8hi)__a;
  return (unsigned short)__b[__imm & 7];
}

/// \brief Constructs a 128-bit integer vector by first making a copy of the
///    128-bit integer vector parameter, and then inserting the lower 16 bits
///    of an integer parameter into an offset specified by the immediate-value
///    parameter.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPINSRW / PINSRW </c> instruction.
///
/// \param __a
///    A 128-bit integer vector of [8 x i16]. This vector is copied to the
///    result and then one of the eight elements in the result is replaced by
///    the lower 16 bits of \a __b.
/// \param __b
///    An integer. The lower 16 bits of this parameter are written to the
///    result beginning at an offset specified by \a __imm.
/// \param __imm
///    An immediate value specifying the bit offset in the result at which the
///    lower 16 bits of \a __b are written.
/// \returns A 128-bit integer vector containing the constructed values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_insert_epi16(__m128i __a, int __b, int __imm)
{
  __v8hi __c = (__v8hi)__a;
  __c[__imm & 7] = __b;
  return (__m128i)__c;
}

/// \brief Copies the values of the most significant bits from each 8-bit
///    element in a 128-bit integer vector of [16 x i8] to create a 16-bit mask
///    value, zero-extends the value, and writes it to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVMSKB / PMOVMSKB </c> instruction.
///
/// \param __a
///    A 128-bit integer vector containing the values with bits to be extracted.
/// \returns The most significant bits from each 8-bit element in \a __a,
///    written to bits [15:0]. The other bits are assigned zeros.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_movemask_epi8(__m128i __a)
{
  return __builtin_ia32_pmovmskb128((__v16qi)__a);
}

/// \brief Constructs a 128-bit integer vector by shuffling four 32-bit
///    elements of a 128-bit integer vector parameter, using the immediate-value
///    parameter as a specifier.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_shuffle_epi32(__m128i a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPSHUFD / PSHUFD </c> instruction.
///
/// \param a
///    A 128-bit integer vector containing the values to be copied.
/// \param imm
///    An immediate value containing an 8-bit value specifying which elements to
///    copy from a. The destinations within the 128-bit destination are assigned
///    values as follows: \n
///    Bits [1:0] are used to assign values to bits [31:0] of the result. \n
///    Bits [3:2] are used to assign values to bits [63:32] of the result. \n
///    Bits [5:4] are used to assign values to bits [95:64] of the result. \n
///    Bits [7:6] are used to assign values to bits [127:96] of the result. \n
///    Bit value assignments: \n
///    00: assign values from bits [31:0] of \a a. \n
///    01: assign values from bits [63:32] of \a a. \n
///    10: assign values from bits [95:64] of \a a. \n
///    11: assign values from bits [127:96] of \a a.
/// \returns A 128-bit integer vector containing the shuffled values.






/// \brief Constructs a 128-bit integer vector by shuffling four lower 16-bit
///    elements of a 128-bit integer vector of [8 x i16], using the immediate
///    value parameter as a specifier.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_shufflelo_epi16(__m128i a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPSHUFLW / PSHUFLW </c> instruction.
///
/// \param a
///    A 128-bit integer vector of [8 x i16]. Bits [127:64] are copied to bits
///    [127:64] of the result.
/// \param imm
///    An 8-bit immediate value specifying which elements to copy from \a a. \n
///    Bits[1:0] are used to assign values to bits [15:0] of the result. \n
///    Bits[3:2] are used to assign values to bits [31:16] of the result. \n
///    Bits[5:4] are used to assign values to bits [47:32] of the result. \n
///    Bits[7:6] are used to assign values to bits [63:48] of the result. \n
///    Bit value assignments: \n
///    00: assign values from bits [15:0] of \a a. \n
///    01: assign values from bits [31:16] of \a a. \n
///    10: assign values from bits [47:32] of \a a. \n
///    11: assign values from bits [63:48] of \a a. \n
/// \returns A 128-bit integer vector containing the shuffled values.







/// \brief Constructs a 128-bit integer vector by shuffling four upper 16-bit
///    elements of a 128-bit integer vector of [8 x i16], using the immediate
///    value parameter as a specifier.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_shufflehi_epi16(__m128i a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPSHUFHW / PSHUFHW </c> instruction.
///
/// \param a
///    A 128-bit integer vector of [8 x i16]. Bits [63:0] are copied to bits
///    [63:0] of the result.
/// \param imm
///    An 8-bit immediate value specifying which elements to copy from \a a. \n
///    Bits[1:0] are used to assign values to bits [79:64] of the result. \n
///    Bits[3:2] are used to assign values to bits [95:80] of the result. \n
///    Bits[5:4] are used to assign values to bits [111:96] of the result. \n
///    Bits[7:6] are used to assign values to bits [127:112] of the result. \n
///    Bit value assignments: \n
///    00: assign values from bits [79:64] of \a a. \n
///    01: assign values from bits [95:80] of \a a. \n
///    10: assign values from bits [111:96] of \a a. \n
///    11: assign values from bits [127:112] of \a a. \n
/// \returns A 128-bit integer vector containing the shuffled values.









/// \brief Unpacks the high-order (index 8-15) values from two 128-bit vectors
///    of [16 x i8] and interleaves them into a 128-bit vector of [16 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKHBW / PUNPCKHBW </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [16 x i8].
///    Bits [71:64] are written to bits [7:0] of the result. \n
///    Bits [79:72] are written to bits [23:16] of the result. \n
///    Bits [87:80] are written to bits [39:32] of the result. \n
///    Bits [95:88] are written to bits [55:48] of the result. \n
///    Bits [103:96] are written to bits [71:64] of the result. \n
///    Bits [111:104] are written to bits [87:80] of the result. \n
///    Bits [119:112] are written to bits [103:96] of the result. \n
///    Bits [127:120] are written to bits [119:112] of the result.
/// \param __b
///    A 128-bit vector of [16 x i8]. \n
///    Bits [71:64] are written to bits [15:8] of the result. \n
///    Bits [79:72] are written to bits [31:24] of the result. \n
///    Bits [87:80] are written to bits [47:40] of the result. \n
///    Bits [95:88] are written to bits [63:56] of the result. \n
///    Bits [103:96] are written to bits [79:72] of the result. \n
///    Bits [111:104] are written to bits [95:88] of the result. \n
///    Bits [119:112] are written to bits [111:104] of the result. \n
///    Bits [127:120] are written to bits [127:120] of the result.
/// \returns A 128-bit vector of [16 x i8] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpackhi_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}

/// \brief Unpacks the high-order (index 4-7) values from two 128-bit vectors of
///    [8 x i16] and interleaves them into a 128-bit vector of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKHWD / PUNPCKHWD </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16].
///    Bits [79:64] are written to bits [15:0] of the result. \n
///    Bits [95:80] are written to bits [47:32] of the result. \n
///    Bits [111:96] are written to bits [79:64] of the result. \n
///    Bits [127:112] are written to bits [111:96] of the result.
/// \param __b
///    A 128-bit vector of [8 x i16].
///    Bits [79:64] are written to bits [31:16] of the result. \n
///    Bits [95:80] are written to bits [63:48] of the result. \n
///    Bits [111:96] are written to bits [95:80] of the result. \n
///    Bits [127:112] are written to bits [127:112] of the result.
/// \returns A 128-bit vector of [8 x i16] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpackhi_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8+4, 5, 8+5, 6, 8+6, 7, 8+7);
}

/// \brief Unpacks the high-order (index 2,3) values from two 128-bit vectors of
///    [4 x i32] and interleaves them into a 128-bit vector of [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKHDQ / PUNPCKHDQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32]. \n
///    Bits [95:64] are written to bits [31:0] of the destination. \n
///    Bits [127:96] are written to bits [95:64] of the destination.
/// \param __b
///    A 128-bit vector of [4 x i32]. \n
///    Bits [95:64] are written to bits [64:32] of the destination. \n
///    Bits [127:96] are written to bits [127:96] of the destination.
/// \returns A 128-bit vector of [4 x i32] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpackhi_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4+2, 3, 4+3);
}

/// \brief Unpacks the high-order 64-bit elements from two 128-bit vectors of
///    [2 x i64] and interleaves them into a 128-bit vector of [2 x i64].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKHQDQ / PUNPCKHQDQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x i64]. \n
///    Bits [127:64] are written to bits [63:0] of the destination.
/// \param __b
///    A 128-bit vector of [2 x i64]. \n
///    Bits [127:64] are written to bits [127:64] of the destination.
/// \returns A 128-bit vector of [2 x i64] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpackhi_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 1, 2+1);
}

/// \brief Unpacks the low-order (index 0-7) values from two 128-bit vectors of
///    [16 x i8] and interleaves them into a 128-bit vector of [16 x i8].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLBW / PUNPCKLBW </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [16 x i8]. \n
///    Bits [7:0] are written to bits [7:0] of the result. \n
///    Bits [15:8] are written to bits [23:16] of the result. \n
///    Bits [23:16] are written to bits [39:32] of the result. \n
///    Bits [31:24] are written to bits [55:48] of the result. \n
///    Bits [39:32] are written to bits [71:64] of the result. \n
///    Bits [47:40] are written to bits [87:80] of the result. \n
///    Bits [55:48] are written to bits [103:96] of the result. \n
///    Bits [63:56] are written to bits [119:112] of the result.
/// \param __b
///    A 128-bit vector of [16 x i8].
///    Bits [7:0] are written to bits [15:8] of the result. \n
///    Bits [15:8] are written to bits [31:24] of the result. \n
///    Bits [23:16] are written to bits [47:40] of the result. \n
///    Bits [31:24] are written to bits [63:56] of the result. \n
///    Bits [39:32] are written to bits [79:72] of the result. \n
///    Bits [47:40] are written to bits [95:88] of the result. \n
///    Bits [55:48] are written to bits [111:104] of the result. \n
///    Bits [63:56] are written to bits [127:120] of the result.
/// \returns A 128-bit vector of [16 x i8] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpacklo_epi8(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__a, (__v16qi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7);
}

/// \brief Unpacks the low-order (index 0-3) values from each of the two 128-bit
///    vectors of [8 x i16] and interleaves them into a 128-bit vector of
///    [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLWD / PUNPCKLWD </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16].
///    Bits [15:0] are written to bits [15:0] of the result. \n
///    Bits [31:16] are written to bits [47:32] of the result. \n
///    Bits [47:32] are written to bits [79:64] of the result. \n
///    Bits [63:48] are written to bits [111:96] of the result.
/// \param __b
///    A 128-bit vector of [8 x i16].
///    Bits [15:0] are written to bits [31:16] of the result. \n
///    Bits [31:16] are written to bits [63:48] of the result. \n
///    Bits [47:32] are written to bits [95:80] of the result. \n
///    Bits [63:48] are written to bits [127:112] of the result.
/// \returns A 128-bit vector of [8 x i16] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpacklo_epi16(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8+0, 1, 8+1, 2, 8+2, 3, 8+3);
}

/// \brief Unpacks the low-order (index 0,1) values from two 128-bit vectors of
///    [4 x i32] and interleaves them into a 128-bit vector of [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLDQ / PUNPCKLDQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32]. \n
///    Bits [31:0] are written to bits [31:0] of the destination. \n
///    Bits [63:32] are written to bits [95:64] of the destination.
/// \param __b
///    A 128-bit vector of [4 x i32]. \n
///    Bits [31:0] are written to bits [64:32] of the destination. \n
///    Bits [63:32] are written to bits [127:96] of the destination.
/// \returns A 128-bit vector of [4 x i32] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpacklo_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4+0, 1, 4+1);
}

/// \brief Unpacks the low-order 64-bit elements from two 128-bit vectors of
///    [2 x i64] and interleaves them into a 128-bit vector of [2 x i64].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLQDQ / PUNPCKLQDQ </c>
///   instruction.
///
/// \param __a
///    A 128-bit vector of [2 x i64]. \n
///    Bits [63:0] are written to bits [63:0] of the destination. \n
/// \param __b
///    A 128-bit vector of [2 x i64]. \n
///    Bits [63:0] are written to bits [127:64] of the destination. \n
/// \returns A 128-bit vector of [2 x i64] containing the interleaved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpacklo_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 0, 2+0);
}

/// \brief Returns the lower 64 bits of a 128-bit integer vector as a 64-bit
///    integer.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MOVDQ2Q </c> instruction.
///
/// \param __a
///    A 128-bit integer vector operand. The lower 64 bits are moved to the
///    destination.
/// \returns A 64-bit integer containing the lower 64 bits of the parameter.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_movepi64_pi64(__m128i __a)
{
  return (__m64)__a[0];
}

/// \brief Moves the 64-bit operand to a 128-bit integer vector, zeroing the
///    upper bits.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MOVD+VMOVQ </c> instruction.
///
/// \param __a
///    A 64-bit value.
/// \returns A 128-bit integer vector. The lower 64 bits contain the value from
///    the operand. The upper 64 bits are assigned zeros.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_movpi64_epi64(__m64 __a)
{
  return (__m128i){ (long long)__a, 0 };
}

/// \brief Moves the lower 64 bits of a 128-bit integer vector to a 128-bit
///    integer vector, zeroing the upper bits.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVQ / MOVQ </c> instruction.
///
/// \param __a
///    A 128-bit integer vector operand. The lower 64 bits are moved to the
///    destination.
/// \returns A 128-bit integer vector. The lower 64 bits contain the value from
///    the operand. The upper 64 bits are assigned zeros.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_move_epi64(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__m128i){ 0 }, 0, 2);
}

/// \brief Unpacks the high-order 64-bit elements from two 128-bit vectors of
///    [2 x double] and interleaves them into a 128-bit vector of [2 x
///    double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKHPD / UNPCKHPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. \n
///    Bits [127:64] are written to bits [63:0] of the destination.
/// \param __b
///    A 128-bit vector of [2 x double]. \n
///    Bits [127:64] are written to bits [127:64] of the destination.
/// \returns A 128-bit vector of [2 x double] containing the interleaved values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpackhi_pd(__m128d __a, __m128d __b)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 1, 2+1);
}

/// \brief Unpacks the low-order 64-bit elements from two 128-bit vectors
///    of [2 x double] and interleaves them into a 128-bit vector of [2 x
///    double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD / UNPCKLPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. \n
///    Bits [63:0] are written to bits [63:0] of the destination.
/// \param __b
///    A 128-bit vector of [2 x double]. \n
///    Bits [63:0] are written to bits [127:64] of the destination.
/// \returns A 128-bit vector of [2 x double] containing the interleaved values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_unpacklo_pd(__m128d __a, __m128d __b)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 0, 2+0);
}

/// \brief Extracts the sign bits of the double-precision values in the 128-bit
///    vector of [2 x double], zero-extends the value, and writes it to the
///    low-order bits of the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVMSKPD / MOVMSKPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the values with sign bits to
///    be extracted.
/// \returns The sign bits from each of the double-precision elements in \a __a,
///    written to bits [1:0]. The remaining bits are assigned values of zero.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_movemask_pd(__m128d __a)
{
  return __builtin_ia32_movmskpd((__v2df)__a);
}


/// \brief Constructs a 128-bit floating-point vector of [2 x double] from two
///    128-bit vector parameters of [2 x double], using the immediate-value
///     parameter as a specifier.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_shuffle_pd(__m128d a, __m128d b, __const int i);
/// \endcode
///
/// This intrinsic corresponds to the <c> VSHUFPD / SHUFPD </c> instruction.
///
/// \param a
///    A 128-bit vector of [2 x double].
/// \param b
///    A 128-bit vector of [2 x double].
/// \param i
///    An 8-bit immediate value. The least significant two bits specify which
///    elements to copy from \a a and \a b: \n
///    Bit[0] = 0: lower element of \a a copied to lower element of result. \n
///    Bit[0] = 1: upper element of \a a copied to lower element of result. \n
///    Bit[1] = 0: lower element of \a b copied to upper element of result. \n
///    Bit[1] = 1: upper element of \a b copied to upper element of result. \n
/// \returns A 128-bit vector of [2 x double] containing the shuffled values.





/// \brief Casts a 128-bit floating-point vector of [2 x double] into a 128-bit
///    floating-point vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [2 x double].
/// \returns A 128-bit floating-point vector of [4 x float] containing the same
///    bitwise pattern as the parameter.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castpd_ps(__m128d __a)
{
  return (__m128)__a;
}

/// \brief Casts a 128-bit floating-point vector of [2 x double] into a 128-bit
///    integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [2 x double].
/// \returns A 128-bit integer vector containing the same bitwise pattern as the
///    parameter.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castpd_si128(__m128d __a)
{
  return (__m128i)__a;
}

/// \brief Casts a 128-bit floating-point vector of [4 x float] into a 128-bit
///    floating-point vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float].
/// \returns A 128-bit floating-point vector of [2 x double] containing the same
///    bitwise pattern as the parameter.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castps_pd(__m128 __a)
{
  return (__m128d)__a;
}

/// \brief Casts a 128-bit floating-point vector of [4 x float] into a 128-bit
///    integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit floating-point vector of [4 x float].
/// \returns A 128-bit integer vector containing the same bitwise pattern as the
///    parameter.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castps_si128(__m128 __a)
{
  return (__m128i)__a;
}

/// \brief Casts a 128-bit integer vector into a 128-bit floating-point vector
///    of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 128-bit floating-point vector of [4 x float] containing the same
///    bitwise pattern as the parameter.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castsi128_ps(__m128i __a)
{
  return (__m128)__a;
}

/// \brief Casts a 128-bit integer vector into a 128-bit floating-point vector
///    of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 128-bit floating-point vector of [2 x double] containing the same
///    bitwise pattern as the parameter.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
_mm_castsi128_pd(__m128i __a)
{
  return (__m128d)__a;
}





/// \brief Indicates that a spin loop is being executed for the purposes of
///    optimizing power consumption during the loop.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> PAUSE </c> instruction.
///
void _mm_pause(void);

















# 2970 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xmmintrin.h" 2 3 4



# 33 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4








# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/pmmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/pmmintrin.h" 3 4











/// \brief Loads data from an unaligned memory location to elements in a 128-bit
///    vector.
///
///    If the address of the data is not 16-byte aligned, the instruction may
///    read two adjacent aligned blocks of memory to retrieve the requested
///    data.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VLDDQU </c> instruction.
///
/// \param __p
///    A pointer to a 128-bit integer vector containing integer values.
/// \returns A 128-bit vector containing the moved values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_lddqu_si128(__m128i __const *__p)
{
  return (__m128i)__builtin_ia32_lddqu((char __const *)__p);
}

/// \brief Adds the even-indexed values and subtracts the odd-indexed values of
///    two 128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSUBPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing the left source operand.
/// \param __b
///    A 128-bit vector of [4 x float] containing the right source operand.
/// \returns A 128-bit vector of [4 x float] containing the alternating sums and
///    differences of both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_addsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_addsubps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in two
///    128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHADDPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 128-bit vector of [4 x float] containing the horizontal sums of
///    both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_hadd_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_haddps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in two
///    128-bit vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHSUBPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The horizontal differences between the values are stored in the lower
///    bits of the destination.
/// \param __b
///    A 128-bit vector of [4 x float] containing one of the source operands.
///    The horizontal differences between the values are stored in the upper
///    bits of the destination.
/// \returns A 128-bit vector of [4 x float] containing the horizontal
///    differences of both operands.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_hsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_hsubps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Moves and duplicates odd-indexed values from a 128-bit vector
///    of [4 x float] to float values stored in a 128-bit vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSHDUP </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float]. \n
///    Bits [127:96] of the source are written to bits [127:96] and [95:64] of
///    the destination. \n
///    Bits [63:32] of the source are written to bits [63:32] and [31:0] of the
///    destination.
/// \returns A 128-bit vector of [4 x float] containing the moved and duplicated
///    values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_movehdup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 1, 1, 3, 3);
}

/// \brief Duplicates even-indexed values from a 128-bit vector of
///    [4 x float] to float values stored in a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSLDUP </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float] \n
///    Bits [95:64] of the source are written to bits [127:96] and [95:64] of
///    the destination. \n
///    Bits [31:0] of the source are written to bits [63:32] and [31:0] of the
///    destination.
/// \returns A 128-bit vector of [4 x float] containing the moved and duplicated
///    values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_moveldup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 2, 2);
}

/// \brief Adds the even-indexed values and subtracts the odd-indexed values of
///    two 128-bit vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSUBPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing the left source operand.
/// \param __b
///    A 128-bit vector of [2 x double] containing the right source operand.
/// \returns A 128-bit vector of [2 x double] containing the alternating sums
///    and differences of both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_addsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_addsubpd((__v2df)__a, (__v2df)__b);
}

/// \brief Horizontally adds the pairs of values contained in two 128-bit
///    vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHADDPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
///    The horizontal sum of the values is stored in the lower bits of the
///    destination.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
///    The horizontal sum of the values is stored in the upper bits of the
///    destination.
/// \returns A 128-bit vector of [2 x double] containing the horizontal sums of
///    both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_hadd_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_haddpd((__v2df)__a, (__v2df)__b);
}

/// \brief Horizontally subtracts the pairs of values contained in two 128-bit
///    vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHSUBPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double] containing one of the source operands.
///    The horizontal difference of the values is stored in the lower bits of
///    the destination.
/// \param __b
///    A 128-bit vector of [2 x double] containing one of the source operands.
///    The horizontal difference of the values is stored in the upper bits of
///    the destination.
/// \returns A 128-bit vector of [2 x double] containing the horizontal
///    differences of both operands.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_hsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_hsubpd((__v2df)__a, (__v2df)__b);
}

/// \brief Moves and duplicates one double-precision value to double-precision
///    values stored in a 128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_loaddup_pd(double __const * dp);
/// \endcode
///
/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.
///
/// \param dp
///    A pointer to a double-precision value to be moved and duplicated.
/// \returns A 128-bit vector of [2 x double] containing the moved and
///    duplicated values.


/// \brief Moves and duplicates the double-precision value in the lower bits of
///    a 128-bit vector of [2 x double] to double-precision values stored in a
///    128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double]. Bits [63:0] are written to bits
///    [127:64] and [63:0] of the destination.
/// \returns A 128-bit vector of [2 x double] containing the moved and
///    duplicated values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_movedup_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}

/// \brief Establishes a linear address memory range to be monitored and puts
///    the processor in the monitor event pending state. Data stored in the
///    monitored address range causes the processor to exit the pending state.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MONITOR </c> instruction.
///
/// \param __p
///    The memory range to be monitored. The size of the range is determined by
///    CPUID function 0000_0005h.
/// \param __extensions
///    Optional extensions for the monitoring state.
/// \param __hints
///    Optional hints for the monitoring state.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_monitor(void __const *__p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitor((void *)__p, __extensions, __hints);
}

/// \brief Used with the MONITOR instruction to wait while the processor is in
///    the monitor event pending state. Data stored in the monitored address
///    range causes the processor to exit the pending state.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> MWAIT </c> instruction.
///
/// \param __extensions
///    Optional extensions for the monitoring state, which may vary by
///    processor.
/// \param __hints
///    Optional hints for the monitoring state, which may vary by processor.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3")))
_mm_mwait(unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_mwait(__extensions, __hints);
}




# 41 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/tmmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/tmmintrin.h" 3 4










/// \brief Computes the absolute value of each of the packed 8-bit __signed
///    integers in the source operand and stores the 8-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PABSB instruction.
///
/// \param __a
///    A 64-bit vector of [8 x i8].
/// \returns A 64-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_pi8(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsb((__v8qi)__a);
}

/// \brief Computes the absolute value of each of the packed 8-bit __signed
///    integers in the source operand and stores the 8-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPABSB instruction.
///
/// \param __a
///    A 128-bit vector of [16 x i8].
/// \returns A 128-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_epi8(__m128i __a)
{
    return (__m128i)__builtin_ia32_pabsb128((__v16qi)__a);
}

/// \brief Computes the absolute value of each of the packed 16-bit __signed
///    integers in the source operand and stores the 16-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PABSW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16].
/// \returns A 64-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_pi16(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsw((__v4hi)__a);
}

/// \brief Computes the absolute value of each of the packed 16-bit __signed
///    integers in the source operand and stores the 16-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPABSW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16].
/// \returns A 128-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_epi16(__m128i __a)
{
    return (__m128i)__builtin_ia32_pabsw128((__v8hi)__a);
}

/// \brief Computes the absolute value of each of the packed 32-bit __signed
///    integers in the source operand and stores the 32-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PABSD instruction.
///
/// \param __a
///    A 64-bit vector of [2 x i32].
/// \returns A 64-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_pi32(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsd((__v2si)__a);
}

/// \brief Computes the absolute value of each of the packed 32-bit __signed
///    integers in the source operand and stores the 32-bit unsigned integer
///    results in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPABSD instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32].
/// \returns A 128-bit integer vector containing the absolute values of the
///    elements in the operand.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_abs_epi32(__m128i __a)
{
    return (__m128i)__builtin_ia32_pabsd128((__v4si)__a);
}

/// \brief Concatenates the two 128-bit integer vector operands, and
///    right-shifts the result by the number of bytes specified in the immediate
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_alignr_epi8(__m128i a, __m128i b, __const int n);
/// \endcode
///
/// This intrinsic corresponds to the \c PALIGNR instruction.
///
/// \param a
///    A 128-bit vector of [16 x i8] containing one of the source operands.
/// \param b
///    A 128-bit vector of [16 x i8] containing one of the source operands.
/// \param n
///    An immediate operand specifying how many bytes to right-shift the result.
/// \returns A 128-bit integer vector containing the concatenated right-shifted
///    value.




/// \brief Concatenates the two 64-bit integer vector operands, and right-shifts
///    the result by the number of bytes specified in the immediate operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m64 _mm_alignr_pi8(__m64 a, __m64 b, __const int n);
/// \endcode
///
/// This intrinsic corresponds to the \c PALIGNR instruction.
///
/// \param a
///    A 64-bit vector of [8 x i8] containing one of the source operands.
/// \param b
///    A 64-bit vector of [8 x i8] containing one of the source operands.
/// \param n
///    An immediate operand specifying how many bytes to right-shift the result.
/// \returns A 64-bit integer vector containing the concatenated right-shifted
///    value.



/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    128-bit vectors of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHADDW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 128-bit vector of [8 x i16] containing the horizontal sums of
///    both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadd_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    128-bit vectors of [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHADDD instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 128-bit vector of [4 x i32] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 128-bit vector of [4 x i32] containing the horizontal sums of
///    both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadd_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddd128((__v4si)__a, (__v4si)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    64-bit vectors of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHADDW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 64-bit vector of [4 x i16] containing the horizontal sums of both
///    operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadd_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    64-bit vectors of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHADDD instruction.
///
/// \param __a
///    A 64-bit vector of [2 x i32] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 64-bit vector of [2 x i32] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 64-bit vector of [2 x i32] containing the horizontal sums of both
///    operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadd_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddd((__v2si)__a, (__v2si)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    128-bit vectors of [8 x i16]. Positive sums greater than 7FFFh are
///    saturated to 7FFFh. Negative sums less than 8000h are saturated to 8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHADDSW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 128-bit vector of [8 x i16] containing the horizontal saturated
///    sums of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadds_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in 2 packed
///    64-bit vectors of [4 x i16]. Positive sums greater than 7FFFh are
///    saturated to 7FFFh. Negative sums less than 8000h are saturated to 8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHADDSW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the lower bits of the
///    destination.
/// \param __b
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal sums of the values are stored in the upper bits of the
///    destination.
/// \returns A 64-bit vector of [4 x i16] containing the horizontal saturated
///    sums of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hadds_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddsw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 128-bit vectors of [8 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHSUBW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 128-bit vector of [8 x i16] containing the horizontal differences
///    of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsub_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 128-bit vectors of [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHSUBD instruction.
///
/// \param __a
///    A 128-bit vector of [4 x i32] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 128-bit vector of [4 x i32] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 128-bit vector of [4 x i32] containing the horizontal differences
///    of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsub_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubd128((__v4si)__a, (__v4si)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 64-bit vectors of [4 x i16].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHSUBW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 64-bit vector of [4 x i16] containing the horizontal differences
///    of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsub_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 64-bit vectors of [2 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHSUBD instruction.
///
/// \param __a
///    A 64-bit vector of [2 x i32] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 64-bit vector of [2 x i32] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 64-bit vector of [2 x i32] containing the horizontal differences
///    of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsub_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubd((__v2si)__a, (__v2si)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 128-bit vectors of [8 x i16]. Positive differences greater than
///    7FFFh are saturated to 7FFFh. Negative differences less than 8000h are
///    saturated to 8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPHSUBSW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 128-bit vector of [8 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 128-bit vector of [8 x i16] containing the horizontal saturated
///    differences of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in 2
///    packed 64-bit vectors of [4 x i16]. Positive differences greater than
///    7FFFh are saturated to 7FFFh. Negative differences less than 8000h are
///    saturated to 8000h.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PHSUBSW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the lower bits of
///    the destination.
/// \param __b
///    A 64-bit vector of [4 x i16] containing one of the source operands. The
///    horizontal differences between the values are stored in the upper bits of
///    the destination.
/// \returns A 64-bit vector of [4 x i16] containing the horizontal saturated
///    differences of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_hsubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubsw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Multiplies corresponding pairs of packed 8-bit unsigned integer
///    values contained in the first source operand and packed 8-bit __signed
///    integer values contained in the second source operand, adds pairs of
///    contiguous products with __signed saturation, and writes the 16-bit sums to
///    the corresponding bits in the destination.
///
///    For example, bits [7:0] of both operands are multiplied, bits [15:8] of
///    both operands are multiplied, and the sum of both results is written to
///    bits [15:0] of the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPMADDUBSW instruction.
///
/// \param __a
///    A 128-bit integer vector containing the first source operand.
/// \param __b
///    A 128-bit integer vector containing the second source operand.
/// \returns A 128-bit integer vector containing the sums of products of both
///    operands: \n
///    \a R0 := (\a __a0 * \a __b0) + (\a __a1 * \a __b1) \n
///    \a R1 := (\a __a2 * \a __b2) + (\a __a3 * \a __b3) \n
///    \a R2 := (\a __a4 * \a __b4) + (\a __a5 * \a __b5) \n
///    \a R3 := (\a __a6 * \a __b6) + (\a __a7 * \a __b7) \n
///    \a R4 := (\a __a8 * \a __b8) + (\a __a9 * \a __b9) \n
///    \a R5 := (\a __a10 * \a __b10) + (\a __a11 * \a __b11) \n
///    \a R6 := (\a __a12 * \a __b12) + (\a __a13 * \a __b13) \n
///    \a R7 := (\a __a14 * \a __b14) + (\a __a15 * \a __b15)
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_maddubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmaddubsw128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Multiplies corresponding pairs of packed 8-bit unsigned integer
///    values contained in the first source operand and packed 8-bit __signed
///    integer values contained in the second source operand, adds pairs of
///    contiguous products with __signed saturation, and writes the 16-bit sums to
///    the corresponding bits in the destination.
///
///    For example, bits [7:0] of both operands are multiplied, bits [15:8] of
///    both operands are multiplied, and the sum of both results is written to
///    bits [15:0] of the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PMADDUBSW instruction.
///
/// \param __a
///    A 64-bit integer vector containing the first source operand.
/// \param __b
///    A 64-bit integer vector containing the second source operand.
/// \returns A 64-bit integer vector containing the sums of products of both
///    operands: \n
///    \a R0 := (\a __a0 * \a __b0) + (\a __a1 * \a __b1) \n
///    \a R1 := (\a __a2 * \a __b2) + (\a __a3 * \a __b3) \n
///    \a R2 := (\a __a4 * \a __b4) + (\a __a5 * \a __b5) \n
///    \a R3 := (\a __a6 * \a __b6) + (\a __a7 * \a __b7)
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_maddubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmaddubsw((__v8qi)__a, (__v8qi)__b);
}

/// \brief Multiplies packed 16-bit __signed integer values, truncates the 32-bit
///    products to the 18 most significant bits by right-shifting, rounds the
///    truncated value by adding 1, and writes bits [16:1] to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPMULHRSW instruction.
///
/// \param __a
///    A 128-bit vector of [8 x i16] containing one of the source operands.
/// \param __b
///    A 128-bit vector of [8 x i16] containing one of the source operands.
/// \returns A 128-bit vector of [8 x i16] containing the rounded and scaled
///    products of both operands.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_mulhrs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmulhrsw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief Multiplies packed 16-bit __signed integer values, truncates the 32-bit
///    products to the 18 most significant bits by right-shifting, rounds the
///    truncated value by adding 1, and writes bits [16:1] to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PMULHRSW instruction.
///
/// \param __a
///    A 64-bit vector of [4 x i16] containing one of the source operands.
/// \param __b
///    A 64-bit vector of [4 x i16] containing one of the source operands.
/// \returns A 64-bit vector of [4 x i16] containing the rounded and scaled
///    products of both operands.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_mulhrs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmulhrsw((__v4hi)__a, (__v4hi)__b);
}

/// \brief Copies the 8-bit integers from a 128-bit integer vector to the
///    destination or clears 8-bit values in the destination, as specified by
///    the second source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPSHUFB instruction.
///
/// \param __a
///    A 128-bit integer vector containing the values to be copied.
/// \param __b
///    A 128-bit integer vector containing control bytes corresponding to
///    positions in the destination:
///    Bit 7: \n
///    1: Clear the corresponding byte in the destination. \n
///    0: Copy the selected source byte to the corresponding byte in the
///    destination. \n
///    Bits [6:4] Reserved.  \n
///    Bits [3:0] select the source byte to be copied.
/// \returns A 128-bit integer vector containing the copied or cleared values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_shuffle_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pshufb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief Copies the 8-bit integers from a 64-bit integer vector to the
///    destination or clears 8-bit values in the destination, as specified by
///    the second source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PSHUFB instruction.
///
/// \param __a
///    A 64-bit integer vector containing the values to be copied.
/// \param __b
///    A 64-bit integer vector containing control bytes corresponding to
///    positions in the destination:
///    Bit 7: \n
///    1: Clear the corresponding byte in the destination. \n
///    0: Copy the selected source byte to the corresponding byte in the
///    destination. \n
///    Bits [3:0] select the source byte to be copied.
/// \returns A 64-bit integer vector containing the copied or cleared values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_shuffle_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pshufb((__v8qi)__a, (__v8qi)__b);
}

/// \brief For each 8-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the byte in the second source is negative, calculate the two's
///    complement of the corresponding byte in the first source, and write that
///    value to the destination. If the byte in the second source is positive,
///    copy the corresponding byte from the first source to the destination. If
///    the byte in the second source is zero, clear the corresponding byte in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPSIGNB instruction.
///
/// \param __a
///    A 128-bit integer vector containing the values to be copied.
/// \param __b
///    A 128-bit integer vector containing control bytes corresponding to
///    positions in the destination.
/// \returns A 128-bit integer vector containing the resultant values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignb128((__v16qi)__a, (__v16qi)__b);
}

/// \brief For each 16-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the word in the second source is negative, calculate the two's
///    complement of the corresponding word in the first source, and write that
///    value to the destination. If the word in the second source is positive,
///    copy the corresponding word from the first source to the destination. If
///    the word in the second source is zero, clear the corresponding word in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPSIGNW instruction.
///
/// \param __a
///    A 128-bit integer vector containing the values to be copied.
/// \param __b
///    A 128-bit integer vector containing control words corresponding to
///    positions in the destination.
/// \returns A 128-bit integer vector containing the resultant values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignw128((__v8hi)__a, (__v8hi)__b);
}

/// \brief For each 32-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the doubleword in the second source is negative, calculate the two's
///    complement of the corresponding word in the first source, and write that
///    value to the destination. If the doubleword in the second source is
///    positive, copy the corresponding word from the first source to the
///    destination. If the doubleword in the second source is zero, clear the
///    corresponding word in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c VPSIGND instruction.
///
/// \param __a
///    A 128-bit integer vector containing the values to be copied.
/// \param __b
///    A 128-bit integer vector containing control doublewords corresponding to
///    positions in the destination.
/// \returns A 128-bit integer vector containing the resultant values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignd128((__v4si)__a, (__v4si)__b);
}

/// \brief For each 8-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the byte in the second source is negative, calculate the two's
///    complement of the corresponding byte in the first source, and write that
///    value to the destination. If the byte in the second source is positive,
///    copy the corresponding byte from the first source to the destination. If
///    the byte in the second source is zero, clear the corresponding byte in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PSIGNB instruction.
///
/// \param __a
///    A 64-bit integer vector containing the values to be copied.
/// \param __b
///    A 64-bit integer vector containing control bytes corresponding to
///    positions in the destination.
/// \returns A 64-bit integer vector containing the resultant values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignb((__v8qi)__a, (__v8qi)__b);
}

/// \brief For each 16-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the word in the second source is negative, calculate the two's
///    complement of the corresponding word in the first source, and write that
///    value to the destination. If the word in the second source is positive,
///    copy the corresponding word from the first source to the destination. If
///    the word in the second source is zero, clear the corresponding word in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PSIGNW instruction.
///
/// \param __a
///    A 64-bit integer vector containing the values to be copied.
/// \param __b
///    A 64-bit integer vector containing control words corresponding to
///    positions in the destination.
/// \returns A 64-bit integer vector containing the resultant values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignw((__v4hi)__a, (__v4hi)__b);
}

/// \brief For each 32-bit integer in the first source operand, perform one of
///    the following actions as specified by the second source operand.
///
///    If the doubleword in the second source is negative, calculate the two's
///    complement of the corresponding doubleword in the first source, and
///    write that value to the destination. If the doubleword in the second
///    source is positive, copy the corresponding doubleword from the first
///    source to the destination. If the doubleword in the second source is
///    zero, clear the corresponding doubleword in the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c PSIGND instruction.
///
/// \param __a
///    A 64-bit integer vector containing the values to be copied.
/// \param __b
///    A 64-bit integer vector containing two control doublewords corresponding
///    to positions in the destination.
/// \returns A 64-bit integer vector containing the resultant values.
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("ssse3")))
_mm_sign_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignd((__v2si)__a, (__v2si)__b);
}




# 45 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/smmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/smmintrin.h" 3 4



























/// \brief Rounds up each element of the 128-bit vector of [4 x float] to an
///    integer and returns the rounded values in a 128-bit vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_ceil_ps(__m128 X);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float] values to be rounded up.
/// \returns A 128-bit vector of [4 x float] containing the rounded values.


/// \brief Rounds up each element of the 128-bit vector of [2 x double] to an
///    integer and returns the rounded values in a 128-bit vector of
///    [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_ceil_pd(__m128d X);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double] values to be rounded up.
/// \returns A 128-bit vector of [2 x double] containing the rounded values.


/// \brief Copies three upper elements of the first 128-bit vector operand to
///    the corresponding three upper elements of the 128-bit result vector of
///    [4 x float]. Rounds up the lowest element of the second 128-bit vector
///    operand to an integer and copies it to the lowest element of the 128-bit
///    result vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_ceil_ss(__m128 X, __m128 Y);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is
///    rounded up to the nearest integer and copied to the corresponding bits
///    of the result.
/// \returns A 128-bit vector of [4 x float] containing the copied and rounded
///    values.


/// \brief Copies the upper element of the first 128-bit vector operand to the
///    corresponding upper element of the 128-bit result vector of [2 x double].
///    Rounds up the lower element of the second 128-bit vector operand to an
///    integer and copies it to the lower element of the 128-bit result vector
///    of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_ceil_sd(__m128d X, __m128d Y);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is
///    rounded up to the nearest integer and copied to the corresponding bits
///    of the result.
/// \returns A 128-bit vector of [2 x double] containing the copied and rounded
///    values.


/// \brief Rounds down each element of the 128-bit vector of [4 x float] to an
///    an integer and returns the rounded values in a 128-bit vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_floor_ps(__m128 X);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float] values to be rounded down.
/// \returns A 128-bit vector of [4 x float] containing the rounded values.


/// \brief Rounds down each element of the 128-bit vector of [2 x double] to an
///    integer and returns the rounded values in a 128-bit vector of
///    [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_floor_pd(__m128d X);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double].
/// \returns A 128-bit vector of [2 x double] containing the rounded values.


/// \brief Copies three upper elements of the first 128-bit vector operand to
///    the corresponding three upper elements of the 128-bit result vector of
///    [4 x float]. Rounds down the lowest element of the second 128-bit vector
///    operand to an integer and copies it to the lowest element of the 128-bit
///    result vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_floor_ss(__m128 X, __m128 Y);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is
///    rounded down to the nearest integer and copied to the corresponding bits
///    of the result.
/// \returns A 128-bit vector of [4 x float] containing the copied and rounded
///    values.


/// \brief Copies the upper element of the first 128-bit vector operand to the
///    corresponding upper element of the 128-bit result vector of [2 x double].
///    Rounds down the lower element of the second 128-bit vector operand to an
///    integer and copies it to the lower element of the 128-bit result vector
///    of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_floor_sd(__m128d X, __m128d Y);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is
///    rounded down to the nearest integer and copied to the corresponding bits
///    of the result.
/// \returns A 128-bit vector of [2 x double] containing the copied and rounded
///    values.


/// \brief Rounds each element of the 128-bit vector of [4 x float] to an
///    integer value according to the rounding control specified by the second
///    argument and returns the rounded values in a 128-bit vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_round_ps(__m128 X, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS / ROUNDPS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float].
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used \n
///      1: The PE field is not updated \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M \n
///      1: Use the current MXCSR setting \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest \n
///      01: Downward (toward negative infinity) \n
///      10: Upward (toward positive infinity) \n
///      11: Truncated
/// \returns A 128-bit vector of [4 x float] containing the rounded values.



/// \brief Copies three upper elements of the first 128-bit vector operand to
///    the corresponding three upper elements of the 128-bit result vector of
///    [4 x float]. Rounds the lowest element of the second 128-bit vector
///    operand to an integer value according to the rounding control specified
///    by the third argument and copies it to the lowest element of the 128-bit
///    result vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_round_ss(__m128 X, __m128 Y, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSS / ROUNDSS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float]. The values stored in bits [127:32] are
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [4 x float]. The value stored in bits [31:0] is
///    rounded to the nearest integer using the specified rounding control and
///    copied to the corresponding bits of the result.
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used \n
///      1: The PE field is not updated \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M \n
///      1: Use the current MXCSR setting \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest \n
///      01: Downward (toward negative infinity) \n
///      10: Upward (toward positive infinity) \n
///      11: Truncated
/// \returns A 128-bit vector of [4 x float] containing the copied and rounded
///    values.




/// \brief Rounds each element of the 128-bit vector of [2 x double] to an
///    integer value according to the rounding control specified by the second
///    argument and returns the rounded values in a 128-bit vector of
///    [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_round_pd(__m128d X, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD / ROUNDPD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double].
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used \n
///      1: The PE field is not updated \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M \n
///      1: Use the current MXCSR setting \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest \n
///      01: Downward (toward negative infinity) \n
///      10: Upward (toward positive infinity) \n
///      11: Truncated
/// \returns A 128-bit vector of [2 x double] containing the rounded values.



/// \brief Copies the upper element of the first 128-bit vector operand to the
///    corresponding upper element of the 128-bit result vector of [2 x double].
///    Rounds the lower element of the second 128-bit vector operand to an
///    integer value according to the rounding control specified by the third
///    argument and copies it to the lower element of the 128-bit result vector
///    of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_round_sd(__m128d X, __m128d Y, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDSD / ROUNDSD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double]. The value stored in bits [127:64] is
///    copied to the corresponding bits of the result.
/// \param Y
///    A 128-bit vector of [2 x double]. The value stored in bits [63:0] is
///    rounded to the nearest integer using the specified rounding control and
///    copied to the corresponding bits of the result.
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used \n
///      1: The PE field is not updated \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M \n
///      1: Use the current MXCSR setting \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest \n
///      01: Downward (toward negative infinity) \n
///      10: Upward (toward positive infinity) \n
///      11: Truncated
/// \returns A 128-bit vector of [2 x double] containing the copied and rounded
///    values.





/// \brief Returns a 128-bit vector of [2 x double] where the values are
///    selected from either the first or second operand as specified by the
///    third operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_blend_pd(__m128d V1, __m128d V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VBLENDPD / BLENDPD </c> instruction.
///
/// \param V1
///    A 128-bit vector of [2 x double].
/// \param V2
///    A 128-bit vector of [2 x double].
/// \param M
///    An immediate integer operand, with mask bits [1:0] specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    index of a copied value. When a mask bit is 0, the corresponding 64-bit
///    element in operand \a V1 is copied to the same position in the result.
///    When a mask bit is 1, the corresponding 64-bit element in operand \a V2
///    is copied to the same position in the result.
/// \returns A 128-bit vector of [2 x double] containing the copied values.






/// \brief Returns a 128-bit vector of [4 x float] where the values are selected
///    from either the first or second operand as specified by the third
///    operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_blend_ps(__m128 V1, __m128 V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VBLENDPS / BLENDPS </c> instruction.
///
/// \param V1
///    A 128-bit vector of [4 x float].
/// \param V2
///    A 128-bit vector of [4 x float].
/// \param M
///    An immediate integer operand, with mask bits [3:0] specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    index of a copied value. When a mask bit is 0, the corresponding 32-bit
///    element in operand \a V1 is copied to the same position in the result.
///    When a mask bit is 1, the corresponding 32-bit element in operand \a V2
///    is copied to the same position in the result.
/// \returns A 128-bit vector of [4 x float] containing the copied values.







/// \brief Returns a 128-bit vector of [2 x double] where the values are
///    selected from either the first or second operand as specified by the
///    third operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBLENDVPD / BLENDVPD </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [2 x double].
/// \param __V2
///    A 128-bit vector of [2 x double].
/// \param __M
///    A 128-bit vector operand, with mask bits 127 and 63 specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    most significant bit of a copied value. When a mask bit is 0, the
///    corresponding 64-bit element in operand \a __V1 is copied to the same
///    position in the result. When a mask bit is 1, the corresponding 64-bit
///    element in operand \a __V2 is copied to the same position in the result.
/// \returns A 128-bit vector of [2 x double] containing the copied values.
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_blendv_pd (__m128d __V1, __m128d __V2, __m128d __M)
{
  return (__m128d) __builtin_ia32_blendvpd ((__v2df)__V1, (__v2df)__V2,
                                            (__v2df)__M);
}

/// \brief Returns a 128-bit vector of [4 x float] where the values are
///    selected from either the first or second operand as specified by the
///    third operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBLENDVPS / BLENDVPS </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x float].
/// \param __V2
///    A 128-bit vector of [4 x float].
/// \param __M
///    A 128-bit vector operand, with mask bits 127, 95, 63, and 31 specifying
///    how the values are to be copied. The position of the mask bit corresponds
///    to the most significant bit of a copied value. When a mask bit is 0, the
///    corresponding 32-bit element in operand \a __V1 is copied to the same
///    position in the result. When a mask bit is 1, the corresponding 32-bit
///    element in operand \a __V2 is copied to the same position in the result.
/// \returns A 128-bit vector of [4 x float] containing the copied values.
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_blendv_ps (__m128 __V1, __m128 __V2, __m128 __M)
{
  return (__m128) __builtin_ia32_blendvps ((__v4sf)__V1, (__v4sf)__V2,
                                           (__v4sf)__M);
}

/// \brief Returns a 128-bit vector of [16 x i8] where the values are selected
///    from either of the first or second operand as specified by the third
///    operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPBLENDVB / PBLENDVB </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [16 x i8].
/// \param __V2
///    A 128-bit vector of [16 x i8].
/// \param __M
///    A 128-bit vector operand, with mask bits 127, 119, 111 ... 7 specifying
///    how the values are to be copied. The position of the mask bit corresponds
///    to the most significant bit of a copied value. When a mask bit is 0, the
///    corresponding 8-bit element in operand \a __V1 is copied to the same
///    position in the result. When a mask bit is 1, the corresponding 8-bit
///    element in operand \a __V2 is copied to the same position in the result.
/// \returns A 128-bit vector of [16 x i8] containing the copied values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_blendv_epi8 (__m128i __V1, __m128i __V2, __m128i __M)
{
  return (__m128i) __builtin_ia32_pblendvb128 ((__v16qi)__V1, (__v16qi)__V2,
                                               (__v16qi)__M);
}

/// \brief Returns a 128-bit vector of [8 x i16] where the values are selected
///    from either of the first or second operand as specified by the third
///    operand, the control mask.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_blend_epi16(__m128i V1, __m128i V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPBLENDW / PBLENDW </c> instruction.
///
/// \param V1
///    A 128-bit vector of [8 x i16].
/// \param V2
///    A 128-bit vector of [8 x i16].
/// \param M
///    An immediate integer operand, with mask bits [7:0] specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    index of a copied value. When a mask bit is 0, the corresponding 16-bit
///    element in operand \a V1 is copied to the same position in the result.
///    When a mask bit is 1, the corresponding 16-bit element in operand \a V2
///    is copied to the same position in the result.
/// \returns A 128-bit vector of [8 x i16] containing the copied values.

# 545 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/smmintrin.h" 3 4


/// \brief Multiples corresponding elements of two 128-bit vectors of [4 x i32]
///    and returns the lower 32 bits of the each product in a 128-bit vector of
///    [4 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULLD / PMULLD </c> instruction.
///
/// \param __V1
///    A 128-bit integer vector.
/// \param __V2
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the products of both operands.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_mullo_epi32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) ((__v4su)__V1 * (__v4su)__V2);
}

/// \brief Multiplies corresponding even-indexed elements of two 128-bit
///    vectors of [4 x i32] and returns a 128-bit vector of [2 x i64]
///    containing the products.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMULDQ / PMULDQ </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x i32].
/// \param __V2
///    A 128-bit vector of [4 x i32].
/// \returns A 128-bit vector of [2 x i64] containing the products of both
///    operands.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_mul_epi32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pmuldq128 ((__v4si)__V1, (__v4si)__V2);
}


/// \brief Computes the dot product of the two 128-bit vectors of [4 x float]
///    and returns it in the elements of the 128-bit result vector of
///    [4 x float].
///
///    The immediate integer operand controls which input elements
///    will contribute to the dot product, and where the final results are
///    returned.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_dp_ps(__m128 X, __m128 Y, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VDPPS / DPPS </c> instruction.
///
/// \param X
///    A 128-bit vector of [4 x float].
/// \param Y
///    A 128-bit vector of [4 x float].
/// \param M
///    An immediate integer operand. Mask bits [7:4] determine which elements
///    of the input vectors are used, with bit [4] corresponding to the lowest
///    element and bit [7] corresponding to the highest element of each [4 x
///    float] vector. If a bit is set, the corresponding elements from the two
///    input vectors are used as an input for dot product; otherwise that input
///    is treated as zero. Bits [3:0] determine which elements of the result
///    will receive a copy of the final dot product, with bit [0] corresponding
///    to the lowest element and bit [3] corresponding to the highest element of
///    each [4 x float] subvector. If a bit is set, the dot product is returned
///    in the corresponding element; otherwise that element is set to zero.
/// \returns A 128-bit vector of [4 x float] containing the dot product.




/// \brief Computes the dot product of the two 128-bit vectors of [2 x double]
///    and returns it in the elements of the 128-bit result vector of
///    [2 x double].
///
///    The immediate integer operand controls which input
///    elements will contribute to the dot product, and where the final results
///    are returned.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_dp_pd(__m128d X, __m128d Y, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VDPPD / DPPD </c> instruction.
///
/// \param X
///    A 128-bit vector of [2 x double].
/// \param Y
///    A 128-bit vector of [2 x double].
/// \param M
///    An immediate integer operand. Mask bits [5:4] determine which elements
///    of the input vectors are used, with bit [4] corresponding to the lowest
///    element and bit [5] corresponding to the highest element of each of [2 x
///    double] vector. If a bit is set, the corresponding elements from the two
///    input vectors are used as an input for dot product; otherwise that input
///    is treated as zero. Bits [1:0] determine which elements of the result
///    will receive a copy of the final dot product, with bit [0] corresponding
///    to the lowest element and bit [1] corresponding to the highest element of
///    each [2 x double] vector. If a bit is set, the dot product is returned in
///    the corresponding element; otherwise that element is set to zero.





/// \brief Loads integer values from a 128-bit aligned memory location to a
///    128-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTDQA / MOVNTDQA </c> instruction.
///
/// \param __V
///    A pointer to a 128-bit aligned memory location that contains the integer
///    values.
/// \returns A 128-bit integer vector containing the data stored at the
///    specified memory location.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_stream_load_si128 (__m128i __const *__V)
{
  return (__m128i) __builtin_nontemporal_load ((__const __v2di *) __V);
}


/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [16 x i8] and returns a 128-bit vector of [16 x i8] containing the lesser
///    of the two values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINSB / PMINSB </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [16 x i8].
/// \param __V2
///    A 128-bit vector of [16 x i8]
/// \returns A 128-bit vector of [16 x i8] containing the lesser values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_min_epi8 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pminsb128 ((__v16qi) __V1, (__v16qi) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [16 x i8] and returns a 128-bit vector of [16 x i8] containing the
///    greater value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXSB / PMAXSB </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [16 x i8].
/// \param __V2
///    A 128-bit vector of [16 x i8].
/// \returns A 128-bit vector of [16 x i8] containing the greater values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_max_epi8 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pmaxsb128 ((__v16qi) __V1, (__v16qi) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [8 x u16] and returns a 128-bit vector of [8 x u16] containing the lesser
///    value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINUW / PMINUW </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [8 x u16].
/// \param __V2
///    A 128-bit vector of [8 x u16].
/// \returns A 128-bit vector of [8 x u16] containing the lesser values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_min_epu16 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pminuw128 ((__v8hi) __V1, (__v8hi) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [8 x u16] and returns a 128-bit vector of [8 x u16] containing the
///    greater value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXUW / PMAXUW </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [8 x u16].
/// \param __V2
///    A 128-bit vector of [8 x u16].
/// \returns A 128-bit vector of [8 x u16] containing the greater values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_max_epu16 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pmaxuw128 ((__v8hi) __V1, (__v8hi) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [4 x i32] and returns a 128-bit vector of [4 x i32] containing the lesser
///    value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINSD / PMINSD </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x i32].
/// \param __V2
///    A 128-bit vector of [4 x i32].
/// \returns A 128-bit vector of [4 x i32] containing the lesser values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_min_epi32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pminsd128 ((__v4si) __V1, (__v4si) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [4 x i32] and returns a 128-bit vector of [4 x i32] containing the
///    greater value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXSD / PMAXSD </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x i32].
/// \param __V2
///    A 128-bit vector of [4 x i32].
/// \returns A 128-bit vector of [4 x i32] containing the greater values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_max_epi32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pmaxsd128 ((__v4si) __V1, (__v4si) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [4 x u32] and returns a 128-bit vector of [4 x u32] containing the lesser
///    value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMINUD / PMINUD </c>  instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x u32].
/// \param __V2
///    A 128-bit vector of [4 x u32].
/// \returns A 128-bit vector of [4 x u32] containing the lesser values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_min_epu32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pminud128((__v4si) __V1, (__v4si) __V2);
}

/// \brief Compares the corresponding elements of two 128-bit vectors of
///    [4 x u32] and returns a 128-bit vector of [4 x u32] containing the
///    greater value of the two.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMAXUD / PMAXUD </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x u32].
/// \param __V2
///    A 128-bit vector of [4 x u32].
/// \returns A 128-bit vector of [4 x u32] containing the greater values.
static __inline__  __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_max_epu32 (__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_pmaxud128((__v4si) __V1, (__v4si) __V2);
}


/// \brief Takes the first argument \a X and inserts an element from the second
///    argument \a Y as selected by the third argument \a N. That result then
///    has elements zeroed out also as selected by the third argument \a N. The
///    resulting 128-bit vector of [4 x float] is then returned.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_insert_ps(__m128 X, __m128 Y, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VINSERTPS </c> instruction.
///
/// \param X
///    A 128-bit vector source operand of [4 x float]. With the exception of
///    those bits in the result copied from parameter \a Y and zeroed by bits
///    [3:0] of \a N, all bits from this parameter are copied to the result.
/// \param Y
///    A 128-bit vector source operand of [4 x float]. One single-precision
///    floating-point element from this source, as determined by the immediate
///    parameter, is copied to the result.
/// \param N
///    Specifies which bits from operand \a Y will be copied, which bits in the
///    result they will be be copied to, and which bits in the result will be
///    cleared. The following assignments are made: \n
///    Bits [7:6] specify the bits to copy from operand \a Y: \n
///      00: Selects bits [31:0] from operand \a Y. \n
///      01: Selects bits [63:32] from operand \a Y. \n
///      10: Selects bits [95:64] from operand \a Y. \n
///      11: Selects bits [127:96] from operand \a Y. \n
///    Bits [5:4] specify the bits in the result to which the selected bits
///    from operand \a Y are copied: \n
///      00: Copies the selected bits from \a Y to result bits [31:0]. \n
///      01: Copies the selected bits from \a Y to result bits [63:32]. \n
///      10: Copies the selected bits from \a Y to result bits [95:64]. \n
///      11: Copies the selected bits from \a Y to result bits [127:96]. \n
///    Bits[3:0]: If any of these bits are set, the corresponding result
///    element is cleared.
/// \returns A 128-bit vector of [4 x float] containing the copied
///    single-precision floating point elements from the operands.


/// \brief Extracts a 32-bit integer from a 128-bit vector of [4 x float] and
///    returns it, using the immediate value parameter \a N as a selector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_extract_ps(__m128 X, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VEXTRACTPS / EXTRACTPS </c>
/// instruction.
///
/// \param X
///    A 128-bit vector of [4 x float].
/// \param N
///    An immediate value. Bits [1:0] determines which bits from the argument
///    \a X are extracted and returned: \n
///    00: Bits [31:0] of parameter \a X are returned. \n
///    01: Bits [63:32] of parameter \a X are returned. \n
///    10: Bits [95:64] of parameter \a X are returned. \n
///    11: Bits [127:96] of parameter \a X are returned.
/// \returns A 32-bit integer containing the extracted 32 bits of float data.




















/// \brief Constructs a 128-bit vector of [16 x i8] by first making a copy of
///    the 128-bit integer vector parameter, and then inserting the lower 8 bits
///    of an integer parameter \a I into an offset specified by the immediate
///    value parameter \a N.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_insert_epi8(__m128i X, int I, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPINSRB / PINSRB </c> instruction.
///
/// \param X
///    A 128-bit integer vector of [16 x i8]. This vector is copied to the
///    result and then one of the sixteen elements in the result vector is
///    replaced by the lower 8 bits of \a I.
/// \param I
///    An integer. The lower 8 bits of this operand are written to the result
///    beginning at the offset specified by \a N.
/// \param N
///    An immediate value. Bits [3:0] specify the bit offset in the result at
///    which the lower 8 bits of \a I are written. \n
///    0000: Bits [7:0] of the result are used for insertion. \n
///    0001: Bits [15:8] of the result are used for insertion. \n
///    0010: Bits [23:16] of the result are used for insertion. \n
///    0011: Bits [31:24] of the result are used for insertion. \n
///    0100: Bits [39:32] of the result are used for insertion. \n
///    0101: Bits [47:40] of the result are used for insertion. \n
///    0110: Bits [55:48] of the result are used for insertion. \n
///    0111: Bits [63:56] of the result are used for insertion. \n
///    1000: Bits [71:64] of the result are used for insertion. \n
///    1001: Bits [79:72] of the result are used for insertion. \n
///    1010: Bits [87:80] of the result are used for insertion. \n
///    1011: Bits [95:88] of the result are used for insertion. \n
///    1100: Bits [103:96] of the result are used for insertion. \n
///    1101: Bits [111:104] of the result are used for insertion. \n
///    1110: Bits [119:112] of the result are used for insertion. \n
///    1111: Bits [127:120] of the result are used for insertion.
/// \returns A 128-bit integer vector containing the constructed values.





/// \brief Constructs a 128-bit vector of [4 x i32] by first making a copy of
///    the 128-bit integer vector parameter, and then inserting the 32-bit
///    integer parameter \a I at the offset specified by the immediate value
///    parameter \a N.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_insert_epi32(__m128i X, int I, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPINSRD / PINSRD </c> instruction.
///
/// \param X
///    A 128-bit integer vector of [4 x i32]. This vector is copied to the
///    result and then one of the four elements in the result vector is
///    replaced by \a I.
/// \param I
///    A 32-bit integer that is written to the result beginning at the offset
///    specified by \a N.
/// \param N
///    An immediate value. Bits [1:0] specify the bit offset in the result at
///    which the integer \a I is written. \n
///    00: Bits [31:0] of the result are used for insertion. \n
///    01: Bits [63:32] of the result are used for insertion. \n
///    10: Bits [95:64] of the result are used for insertion. \n
///    11: Bits [127:96] of the result are used for insertion.
/// \returns A 128-bit integer vector containing the constructed values.






/// \brief Constructs a 128-bit vector of [2 x i64] by first making a copy of
///    the 128-bit integer vector parameter, and then inserting the 64-bit
///    integer parameter \a I, using the immediate value parameter \a N as an
///    insertion location selector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_insert_epi64(__m128i X, long long I, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPINSRQ / PINSRQ </c> instruction.
///
/// \param X
///    A 128-bit integer vector of [2 x i64]. This vector is copied to the
///    result and then one of the two elements in the result vector is replaced
///    by \a I.
/// \param I
///    A 64-bit integer that is written to the result beginning at the offset
///    specified by \a N.
/// \param N
///    An immediate value. Bit [0] specifies the bit offset in the result at
///    which the integer \a I is written. \n
///    0: Bits [63:0] of the result are used for insertion. \n
///    1: Bits [127:64] of the result are used for insertion. \n
/// \returns A 128-bit integer vector containing the constructed values.









/// \brief Extracts an 8-bit element from the 128-bit integer vector of
///    [16 x i8], using the immediate value parameter \a N as a selector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_extract_epi8(__m128i X, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPEXTRB / PEXTRB </c> instruction.
///
/// \param X
///    A 128-bit integer vector.
/// \param N
///    An immediate value. Bits [3:0] specify which 8-bit vector element from
///    the argument \a X to extract and copy to the result. \n
///    0000: Bits [7:0] of parameter \a X are extracted. \n
///    0001: Bits [15:8] of the parameter \a X are extracted. \n
///    0010: Bits [23:16] of the parameter \a X are extracted. \n
///    0011: Bits [31:24] of the parameter \a X are extracted. \n
///    0100: Bits [39:32] of the parameter \a X are extracted. \n
///    0101: Bits [47:40] of the parameter \a X are extracted. \n
///    0110: Bits [55:48] of the parameter \a X are extracted. \n
///    0111: Bits [63:56] of the parameter \a X are extracted. \n
///    1000: Bits [71:64] of the parameter \a X are extracted. \n
///    1001: Bits [79:72] of the parameter \a X are extracted. \n
///    1010: Bits [87:80] of the parameter \a X are extracted. \n
///    1011: Bits [95:88] of the parameter \a X are extracted. \n
///    1100: Bits [103:96] of the parameter \a X are extracted. \n
///    1101: Bits [111:104] of the parameter \a X are extracted. \n
///    1110: Bits [119:112] of the parameter \a X are extracted. \n
///    1111: Bits [127:120] of the parameter \a X are extracted.
/// \returns  An unsigned integer, whose lower 8 bits are selected from the
///    128-bit integer vector parameter and the remaining bits are assigned
///    zeros.




/// \brief Extracts a 32-bit element from the 128-bit integer vector of
///    [4 x i32], using the immediate value parameter \a N as a selector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_extract_epi32(__m128i X, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPEXTRD / PEXTRD </c> instruction.
///
/// \param X
///    A 128-bit integer vector.
/// \param N
///    An immediate value. Bits [1:0] specify which 32-bit vector element from
///    the argument \a X to extract and copy to the result. \n
///    00: Bits [31:0] of the parameter \a X are extracted. \n
///    01: Bits [63:32] of the parameter \a X are extracted. \n
///    10: Bits [95:64] of the parameter \a X are extracted. \n
///    11: Bits [127:96] of the parameter \a X are exracted.
/// \returns  An integer, whose lower 32 bits are selected from the 128-bit
///    integer vector parameter and the remaining bits are assigned zeros.





/// \brief Extracts a 64-bit element from the 128-bit integer vector of
///    [2 x i64], using the immediate value parameter \a N as a selector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// long long _mm_extract_epi64(__m128i X, __const int N);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPEXTRQ / PEXTRQ </c> instruction.
///
/// \param X
///    A 128-bit integer vector.
/// \param N
///    An immediate value. Bit [0] specifies which 64-bit vector element from
///    the argument \a X to return. \n
///    0: Bits [63:0] are returned. \n
///    1: Bits [127:64] are returned. \n
/// \returns  A 64-bit integer.






/// \brief Tests whether the specified bits in a 128-bit integer vector are all
///    zeros.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param __M
///    A 128-bit integer vector containing the bits to be tested.
/// \param __V
///    A 128-bit integer vector selecting which bits to test in operand \a __M.
/// \returns TRUE if the specified bits are all zeros; FALSE otherwise.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_testz_si128(__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestz128((__v2di)__M, (__v2di)__V);
}

/// \brief Tests whether the specified bits in a 128-bit integer vector are all
///    ones.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param __M
///    A 128-bit integer vector containing the bits to be tested.
/// \param __V
///    A 128-bit integer vector selecting which bits to test in operand \a __M.
/// \returns TRUE if the specified bits are all ones; FALSE otherwise.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_testc_si128(__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestc128((__v2di)__M, (__v2di)__V);
}

/// \brief Tests whether the specified bits in a 128-bit integer vector are
///    neither all zeros nor all ones.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param __M
///    A 128-bit integer vector containing the bits to be tested.
/// \param __V
///    A 128-bit integer vector selecting which bits to test in operand \a __M.
/// \returns TRUE if the specified bits are neither all zeros nor all ones;
///    FALSE otherwise.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_testnzc_si128(__m128i __M, __m128i __V)
{
  return __builtin_ia32_ptestnzc128((__v2di)__M, (__v2di)__V);
}

/// \brief Tests whether the specified bits in a 128-bit integer vector are all
///    ones.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_test_all_ones(__m128i V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param V
///    A 128-bit integer vector containing the bits to be tested.
/// \returns TRUE if the bits specified in the operand are all set to 1; FALSE
///    otherwise.


/// \brief Tests whether the specified bits in a 128-bit integer vector are
///    neither all zeros nor all ones.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_test_mix_ones_zeros(__m128i M, __m128i V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param M
///    A 128-bit integer vector containing the bits to be tested.
/// \param V
///    A 128-bit integer vector selecting which bits to test in operand \a M.
/// \returns TRUE if the specified bits are neither all zeros nor all ones;
///    FALSE otherwise.


/// \brief Tests whether the specified bits in a 128-bit integer vector are all
///    zeros.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_test_all_zeros(__m128i M, __m128i V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPTEST / PTEST </c> instruction.
///
/// \param M
///    A 128-bit integer vector containing the bits to be tested.
/// \param V
///    A 128-bit integer vector selecting which bits to test in operand \a M.
/// \returns TRUE if the specified bits are all zeros; FALSE otherwise.



/// \brief Compares each of the corresponding 64-bit values of the 128-bit
///    integer vectors for equality.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPEQQ / PCMPEQQ </c> instruction.
///
/// \param __V1
///    A 128-bit integer vector.
/// \param __V2
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cmpeq_epi64(__m128i __V1, __m128i __V2)
{
  return (__m128i)((__v2di)__V1 == (__v2di)__V2);
}


/// \brief Sign-extends each of the lower eight 8-bit integer elements of a
///    128-bit vector of [16 x i8] to 16-bit values and returns them in a
///    128-bit vector of [8 x i16]. The upper eight elements of the input vector
///    are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXBW / PMOVSXBW </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower eight 8-bit elements are sign-
///    extended to 16-bit values.
/// \returns A 128-bit vector of [8 x i16] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi8_epi16(__m128i __V)
{
  

  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8hi);
}

/// \brief Sign-extends each of the lower four 8-bit integer elements of a
///    128-bit vector of [16 x i8] to 32-bit values and returns them in a
///    128-bit vector of [4 x i32]. The upper twelve elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXBD / PMOVSXBD </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower four 8-bit elements are sign-
///    extended to 32-bit values.
/// \returns A 128-bit vector of [4 x i32] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi8_epi32(__m128i __V)
{
  

  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4si);
}

/// \brief Sign-extends each of the lower two 8-bit integer elements of a
///    128-bit integer vector of [16 x i8] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper fourteen elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXBQ / PMOVSXBQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower two 8-bit elements are sign-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi8_epi64(__m128i __V)
{
  

  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1), __v2di);
}

/// \brief Sign-extends each of the lower four 16-bit integer elements of a
///    128-bit integer vector of [8 x i16] to 32-bit values and returns them in
///    a 128-bit vector of [4 x i32]. The upper four elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXWD / PMOVSXWD </c> instruction.
///
/// \param __V
///    A 128-bit vector of [8 x i16]. The lower four 16-bit elements are sign-
///    extended to 32-bit values.
/// \returns A 128-bit vector of [4 x i32] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi16_epi32(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4si);
}

/// \brief Sign-extends each of the lower two 16-bit integer elements of a
///    128-bit integer vector of [8 x i16] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper six elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXWQ / PMOVSXWQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [8 x i16]. The lower two 16-bit elements are sign-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi16_epi64(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1), __v2di);
}

/// \brief Sign-extends each of the lower two 32-bit integer elements of a
///    128-bit integer vector of [4 x i32] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper two elements of the input vector
///    are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVSXDQ / PMOVSXDQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [4 x i32]. The lower two 32-bit elements are sign-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the sign-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepi32_epi64(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v4si)__V, (__v4si)__V, 0, 1), __v2di);
}


/// \brief Zero-extends each of the lower eight 8-bit integer elements of a
///    128-bit vector of [16 x i8] to 16-bit values and returns them in a
///    128-bit vector of [8 x i16]. The upper eight elements of the input vector
///    are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXBW / PMOVZXBW </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower eight 8-bit elements are zero-
///    extended to 16-bit values.
/// \returns A 128-bit vector of [8 x i16] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu8_epi16(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8hi);
}

/// \brief Zero-extends each of the lower four 8-bit integer elements of a
///    128-bit vector of [16 x i8] to 32-bit values and returns them in a
///    128-bit vector of [4 x i32]. The upper twelve elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXBD / PMOVZXBD </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower four 8-bit elements are zero-
///    extended to 32-bit values.
/// \returns A 128-bit vector of [4 x i32] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu8_epi32(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4si);
}

/// \brief Zero-extends each of the lower two 8-bit integer elements of a
///    128-bit integer vector of [16 x i8] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper fourteen elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXBQ / PMOVZXBQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [16 x i8]. The lower two 8-bit elements are zero-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu8_epi64(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1), __v2di);
}

/// \brief Zero-extends each of the lower four 16-bit integer elements of a
///    128-bit integer vector of [8 x i16] to 32-bit values and returns them in
///    a 128-bit vector of [4 x i32]. The upper four elements of the input
///    vector are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXWD / PMOVZXWD </c> instruction.
///
/// \param __V
///    A 128-bit vector of [8 x i16]. The lower four 16-bit elements are zero-
///    extended to 32-bit values.
/// \returns A 128-bit vector of [4 x i32] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu16_epi32(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4si);
}

/// \brief Zero-extends each of the lower two 16-bit integer elements of a
///    128-bit integer vector of [8 x i16] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper six elements of the input vector
///    are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXWQ / PMOVZXWQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [8 x i16]. The lower two 16-bit elements are zero-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu16_epi64(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1), __v2di);
}

/// \brief Zero-extends each of the lower two 32-bit integer elements of a
///    128-bit integer vector of [4 x i32] to 64-bit values and returns them in
///    a 128-bit vector of [2 x i64]. The upper two elements of the input vector
///    are unused.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPMOVZXDQ / PMOVZXDQ </c> instruction.
///
/// \param __V
///    A 128-bit vector of [4 x i32]. The lower two 32-bit elements are zero-
///    extended to 64-bit values.
/// \returns A 128-bit vector of [2 x i64] containing the zero-extended values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_cvtepu32_epi64(__m128i __V)
{
  return (__m128i)__builtin_convertvector(__builtin_shufflevector((__v4su)__V, (__v4su)__V, 0, 1), __v2di);
}


/// \brief Converts 32-bit __signed integers from both 128-bit integer vector
///    operands into 16-bit unsigned integers, and returns the packed result.
///    Values greater than 0xFFFF are saturated to 0xFFFF. Values less than
///    0x0000 are saturated to 0x0000.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPACKUSDW / PACKUSDW </c> instruction.
///
/// \param __V1
///    A 128-bit vector of [4 x i32]. Each 32-bit element is treated as a
///    __signed integer and is converted to a 16-bit unsigned integer with
///    saturation. Values greater than 0xFFFF are saturated to 0xFFFF. Values
///    less than 0x0000 are saturated to 0x0000. The converted [4 x i16] values
///    are written to the lower 64 bits of the result.
/// \param __V2
///    A 128-bit vector of [4 x i32]. Each 32-bit element is treated as a
///    __signed integer and is converted to a 16-bit unsigned integer with
///    saturation. Values greater than 0xFFFF are saturated to 0xFFFF. Values
///    less than 0x0000 are saturated to 0x0000. The converted [4 x i16] values
///    are written to the higher 64 bits of the result.
/// \returns A 128-bit vector of [8 x i16] containing the converted values.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_packus_epi32(__m128i __V1, __m128i __V2)
{
  return (__m128i) __builtin_ia32_packusdw128((__v4si)__V1, (__v4si)__V2);
}


/// \brief Subtracts 8-bit unsigned integer values and computes the absolute
///    values of the differences to the corresponding bits in the destination.
///    Then sums of the absolute differences are returned according to the bit
///    fields in the immediate operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_mpsadbw_epu8(__m128i X, __m128i Y, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VMPSADBW / MPSADBW </c> instruction.
///
/// \param X
///    A 128-bit vector of [16 x i8].
/// \param Y
///    A 128-bit vector of [16 x i8].
/// \param M
///    An 8-bit immediate operand specifying how the absolute differences are to
///    be calculated, according to the following algorithm:
///    \code
///    // M2 represents bit 2 of the immediate operand
///    // M10 represents bits [1:0] of the immediate operand
///    i = M2 * 4
///    j = M10 * 4
///    for (k = 0; k < 8; k = k + 1) {
///      d0 = abs(X[i + k + 0] - Y[j + 0])
///      d1 = abs(X[i + k + 1] - Y[j + 1])
///      d2 = abs(X[i + k + 2] - Y[j + 2])
///      d3 = abs(X[i + k + 3] - Y[j + 3])
///      r[k] = d0 + d1 + d2 + d3
///    }
///    \endcode
/// \returns A 128-bit integer vector containing the sums of the sets of
///    absolute differences between both operands.




/// \brief Finds the minimum unsigned 16-bit element in the input 128-bit
///    vector of [8 x u16] and returns it and along with its index.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPHMINPOSUW / PHMINPOSUW </c>
/// instruction.
///
/// \param __V
///    A 128-bit vector of [8 x u16].
/// \returns A 128-bit value where bits [15:0] contain the minimum value found
///    in parameter \a __V, bits [18:16] contain the index of the minimum value
///    and the remaining bits are set to 0.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1")))
_mm_minpos_epu16(__m128i __V)
{
  return (__m128i) __builtin_ia32_phminposuw128((__v8hi)__V);
}




































/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns a 128-bit integer vector representing the result
///    mask of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_cmpistrm(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRM / PCMPISTRM </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words, the type of comparison to perform, and the format of the return
///    value. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
///    Bit [6]: Determines whether the result is zero-extended or expanded to 16
///             bytes. \n
///      0: The result is zero-extended to 16 bytes. \n
///      1: The result is expanded to 16 bytes (this expansion is performed by
///         repeating each bit 8 or 16 times).
/// \returns Returns a 128-bit integer vector representing the result mask of
///    the comparison.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns an integer representing the result index of the
///    comparison.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistri(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words, the type of comparison to perform, and the format of the return
///    value. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
///    Bit [6]: Determines whether the index of the lowest set bit or the
///             highest set bit is returned. \n
///      0: The index of the least significant set bit. \n
///      1: The index of the most significant set bit. \n
/// \returns Returns an integer representing the result index of the comparison.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns a 128-bit integer vector representing the result
///    mask of the comparison.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_cmpestrm(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRM / PCMPESTRM </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words, the type of comparison to perform, and the format of the return
///    value. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
///    Bit [6]: Determines whether the result is zero-extended or expanded to 16
///             bytes. \n
///      0: The result is zero-extended to 16 bytes. \n
///      1: The result is expanded to 16 bytes (this expansion is performed by
///         repeating each bit 8 or 16 times). \n
/// \returns Returns a 128-bit integer vector representing the result mask of
///    the comparison.





/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns an integer representing the result index of the
///    comparison.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestri(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words, the type of comparison to perform, and the format of the return
///    value. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
///    Bit [6]: Determines whether the index of the lowest set bit or the
///             highest set bit is returned. \n
///      0: The index of the least significant set bit. \n
///      1: The index of the most significant set bit. \n
/// \returns Returns an integer representing the result index of the comparison.






/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the bit mask is zero and the length of the
///    string in \a B is the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistra(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
/// \returns Returns 1 if the bit mask is zero and the length of the string in
///    \a B is the maximum; otherwise, returns 0.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the bit mask is non-zero, otherwise, returns
///    0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistrc(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B.
/// \returns Returns 1 if the bit mask is non-zero, otherwise, returns 0.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns bit 0 of the resulting bit mask.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistro(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
/// \returns Returns bit 0 of the resulting bit mask.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the length of the string in \a A is less than
///    the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistrs(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
/// \returns Returns 1 if the length of the string in \a A is less than the
///    maximum, otherwise, returns 0.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with implicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the length of the string in \a B is less than
///    the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpistrz(__m128i A, __m128i B, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPISTRI / PCMPISTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B.
/// \returns Returns 1 if the length of the string in \a B is less than the
///    maximum, otherwise, returns 0.




/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the bit mask is zero and the length of the
///    string in \a B is the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestra(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B.
/// \returns Returns 1 if the bit mask is zero and the length of the string in
///    \a B is the maximum, otherwise, returns 0.





/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the resulting mask is non-zero, otherwise,
///    returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestrc(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
/// \returns Returns 1 if the resulting mask is non-zero, otherwise, returns 0.





/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns bit 0 of the resulting bit mask.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestro(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B.
/// \returns Returns bit 0 of the resulting bit mask.





/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the length of the string in \a A is less than
///    the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestrs(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI / PCMPESTRI </c>
/// instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement in the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B. \n
/// \returns Returns 1 if the length of the string in \a A is less than the
///    maximum, otherwise, returns 0.





/// \brief Uses the immediate operand \a M to perform a comparison of string
///    data with explicitly defined lengths that is contained in source operands
///    \a A and \a B. Returns 1 if the length of the string in \a B is less than
///    the maximum, otherwise, returns 0.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// int _mm_cmpestrz(__m128i A, int LA, __m128i B, int LB, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCMPESTRI </c> instruction.
///
/// \param A
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LA
///    An integer that specifies the length of the string in \a A.
/// \param B
///    A 128-bit integer vector containing one of the source operands to be
///    compared.
/// \param LB
///    An integer that specifies the length of the string in \a B.
/// \param M
///    An 8-bit immediate operand specifying whether the characters are bytes or
///    words and the type of comparison to perform. \n
///    Bits [1:0]: Determine source data format. \n
///      00: 16 unsigned bytes  \n
///      01: 8 unsigned words \n
///      10: 16 __signed bytes \n
///      11: 8 __signed words \n
///    Bits [3:2]: Determine comparison type and aggregation method. \n
///      00: Subset: Each character in \a B is compared for equality with all
///          the characters in \a A. \n
///      01: Ranges: Each character in \a B is compared to \a A. The comparison
///          basis is greater than or equal for even-indexed elements in \a A,
///          and less than or equal for odd-indexed elements in \a A. \n
///      10: Match: Compare each pair of corresponding characters in \a A and
///          \a B for equality. \n
///      11: Substring: Search \a B for substring matches of \a A. \n
///    Bits [5:4]: Determine whether to perform a one's complement on the bit
///                mask of the comparison results. \n
///      00: No effect. \n
///      01: Negate the bit mask. \n
///      10: No effect. \n
///      11: Negate the bit mask only for bits with an index less than or equal
///          to the size of \a A or \a B.
/// \returns Returns 1 if the length of the string in \a B is less than the
///    maximum, otherwise, returns 0.






/// \brief Compares each of the corresponding 64-bit values of the 128-bit
///    integer vectors to determine if the values in the first operand are
///    greater than those in the second operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPCMPGTQ / PCMPGTQ </c> instruction.
///
/// \param __V1
///    A 128-bit integer vector.
/// \param __V2
///    A 128-bit integer vector.
/// \returns A 128-bit integer vector containing the comparison results.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.2")))
_mm_cmpgt_epi64(__m128i __V1, __m128i __V2)
{
  return (__m128i)((__v2di)__V1 > (__v2di)__V2);
}


/// \brief Adds the unsigned integer operand to the CRC-32C checksum of the
///    unsigned char operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CRC32B </c> instruction.
///
/// \param __C
///    An unsigned integer operand to add to the CRC-32C checksum of operand
///    \a  __D.
/// \param __D
///    An unsigned 8-bit integer operand used to compute the CRC-32C checksum.
/// \returns The result of adding operand \a __C to the CRC-32C checksum of
///    operand \a __D.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sse4.2")))
_mm_crc32_u8(unsigned int __C, unsigned char __D)
{
  return __builtin_ia32_crc32qi(__C, __D);
}

/// \brief Adds the unsigned integer operand to the CRC-32C checksum of the
///    unsigned short operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CRC32W </c> instruction.
///
/// \param __C
///    An unsigned integer operand to add to the CRC-32C checksum of operand
///    \a __D.
/// \param __D
///    An unsigned 16-bit integer operand used to compute the CRC-32C checksum.
/// \returns The result of adding operand \a __C to the CRC-32C checksum of
///    operand \a __D.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sse4.2")))
_mm_crc32_u16(unsigned int __C, unsigned short __D)
{
  return __builtin_ia32_crc32hi(__C, __D);
}

/// \brief Adds the first unsigned integer operand to the CRC-32C checksum of
///    the second unsigned integer operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CRC32L </c> instruction.
///
/// \param __C
///    An unsigned integer operand to add to the CRC-32C checksum of operand
///    \a __D.
/// \param __D
///    An unsigned 32-bit integer operand used to compute the CRC-32C checksum.
/// \returns The result of adding operand \a __C to the CRC-32C checksum of
///    operand \a __D.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sse4.2")))
_mm_crc32_u32(unsigned int __C, unsigned int __D)
{
  return __builtin_ia32_crc32si(__C, __D);
}


/// \brief Adds the unsigned integer operand to the CRC-32C checksum of the
///    unsigned 64-bit integer operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> CRC32Q </c> instruction.
///
/// \param __C
///    An unsigned integer operand to add to the CRC-32C checksum of operand
///    \a __D.
/// \param __D
///    An unsigned 64-bit integer operand used to compute the CRC-32C checksum.
/// \returns The result of adding operand \a __C to the CRC-32C checksum of
///    operand \a __D.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("sse4.2")))
_mm_crc32_u64(unsigned long long __C, unsigned long long __D)
{
  return __builtin_ia32_crc32di(__C, __D);
}






# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/popcntintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/popcntintrin.h" 3 4








/// \brief Counts the number of bits in the source operand having a value of 1.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> POPCNT </c> instruction.
///
/// \param __A
///    An unsigned 32-bit integer operand.
/// \returns A 32-bit integer containing the number of bits with value 1 in the
///    source operand.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u32(unsigned int __A)
{
  return __builtin_popcount(__A);
}

/// \brief Counts the number of bits in the source operand having a value of 1.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> POPCNT </c> instruction.
///
/// \param __A
///    A __signed 32-bit integer operand.
/// \returns A 32-bit integer containing the number of bits with value 1 in the
///    source operand.
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_popcnt32(int __A)
{
  return __builtin_popcount(__A);
}


/// \brief Counts the number of bits in the source operand having a value of 1.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> POPCNT </c> instruction.
///
/// \param __A
///    An unsigned 64-bit integer operand.
/// \returns A 64-bit integer containing the number of bits with value 1 in the
///    source operand.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u64(unsigned long long __A)
{
  return __builtin_popcountll(__A);
}

/// \brief Counts the number of bits in the source operand having a value of 1.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> POPCNT </c> instruction.
///
/// \param __A
///    A __signed 64-bit integer operand.
/// \returns A 64-bit integer containing the number of bits with value 1 in the
///    source operand.
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_popcnt64(long long __A)
{
  return __builtin_popcountll(__A);
}





# 2463 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/smmintrin.h" 2 3 4



# 50 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/wmmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/wmmintrin.h" 3 4








# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/__wmmintrin_aes.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/__wmmintrin_aes.h" 3 4









/// \brief Performs a single round of AES encryption using the Equivalent
///    Inverse Cipher, transforming the state value from the first source
///    operand using a 128-bit round key value contained in the second source
///    operand, and writes the result to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VAESENC </c> instruction.
///
/// \param __V
///    A 128-bit integer vector containing the state value.
/// \param __R
///    A 128-bit integer vector containing the round key value.
/// \returns A 128-bit integer vector containing the encrypted value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes")))
_mm_aesenc_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenc128((__v2di)__V, (__v2di)__R);
}

/// \brief Performs the final round of AES encryption using the Equivalent
///    Inverse Cipher, transforming the state value from the first source
///    operand using a 128-bit round key value contained in the second source
///    operand, and writes the result to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VAESENCLAST </c> instruction.
///
/// \param __V
///    A 128-bit integer vector containing the state value.
/// \param __R
///    A 128-bit integer vector containing the round key value.
/// \returns A 128-bit integer vector containing the encrypted value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes")))
_mm_aesenclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenclast128((__v2di)__V, (__v2di)__R);
}

/// \brief Performs a single round of AES decryption using the Equivalent
///    Inverse Cipher, transforming the state value from the first source
///    operand using a 128-bit round key value contained in the second source
///    operand, and writes the result to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VAESDEC </c> instruction.
///
/// \param __V
///    A 128-bit integer vector containing the state value.
/// \param __R
///    A 128-bit integer vector containing the round key value.
/// \returns A 128-bit integer vector containing the decrypted value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes")))
_mm_aesdec_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdec128((__v2di)__V, (__v2di)__R);
}

/// \brief Performs the final round of AES decryption using the Equivalent
///    Inverse Cipher, transforming the state value from the first source
///    operand using a 128-bit round key value contained in the second source
///    operand, and writes the result to the destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VAESDECLAST </c> instruction.
///
/// \param __V
///    A 128-bit integer vector containing the state value.
/// \param __R
///    A 128-bit integer vector containing the round key value.
/// \returns A 128-bit integer vector containing the decrypted value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes")))
_mm_aesdeclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdeclast128((__v2di)__V, (__v2di)__R);
}

/// \brief Applies the AES InvMixColumns() transformation to an expanded key
///    contained in the source operand, and writes the result to the
///    destination.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VAESIMC </c> instruction.
///
/// \param __V
///    A 128-bit integer vector containing the expanded key.
/// \returns A 128-bit integer vector containing the transformed value.
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes")))
_mm_aesimc_si128(__m128i __V)
{
  return (__m128i)__builtin_ia32_aesimc128((__v2di)__V);
}

/// \brief Generates a round key for AES encyption, operating on 128-bit data
///    specified in the first source operand and using an 8-bit round constant
///    specified by the second source operand, and writes the result to the
///    destination.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_aeskeygenassist_si128(__m128i C, __const int R);
/// \endcode
///
/// This intrinsic corresponds to the <c> AESKEYGENASSIST </c> instruction.
///
/// \param C
///    A 128-bit integer vector that is used to generate the AES encryption key.
/// \param R
///    An 8-bit round constant used to generate the AES encryption key.
/// \returns A 128-bit round key for AES encryption.






# 30 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/wmmintrin.h" 2 3 4


# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/__wmmintrin_pclmul.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/__wmmintrin_pclmul.h" 3 4




/// \brief Multiplies two 64-bit integer values, which are selected from source
///    operands using the immediate-value operand. The multiplication is a
///    carry-less multiplication, and the 128-bit integer product is stored in
///    the destination.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm_clmulepi64_si128(__m128i __X, __m128i __Y, __const int __I);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPCLMULQDQ </c> instruction.
///
/// \param __X
///    A 128-bit vector of [2 x i64] containing one of the source operands.
/// \param __Y
///    A 128-bit vector of [2 x i64] containing one of the source operands.
/// \param __I
///    An immediate value specifying which 64-bit values to select from the
///    operands. Bit 0 is used to select a value from operand \a __X, and bit
///    4 is used to select a value from operand \a __Y: \n
///    Bit[0]=0 indicates that bits[63:0] of operand \a __X are used. \n
///    Bit[0]=1 indicates that bits[127:64] of operand \a __X are used. \n
///    Bit[4]=0 indicates that bits[63:0] of operand \a __Y are used. \n
///    Bit[4]=1 indicates that bits[127:64] of operand \a __Y are used.
/// \returns The 128-bit integer vector containing the result of the carry-less
///    multiplication of the selected 64-bit values.





# 32 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/wmmintrin.h" 2 3 4


# 55 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/clflushoptintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/clflushoptintrin.h" 3 4












static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clflushopt")))
_mm_clflushopt(void __const * __m) {
  __builtin_ia32_clflushopt(__m);
}




# 59 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/clwbintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/clwbintrin.h" 3 4












/// \brief Writes back to memory the cache line (if modified) that contains the
/// linear address specified in \a __p from any level of the cache hierarchy in
/// the cache coherence domain
///
/// \headerfile <immintrin.h>
///
/// This intrinsic corresponds to the <c> CLWB </c> instruction.
///
/// \param __p
///    A pointer to the memory location used to identify the cache line to be
///    written back.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clwb")))
_mm_clwb(void __const *__p) {
  __builtin_ia32_clwb(__p);
}




# 63 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4









typedef double __v4df __attribute__ ((__vector_size__ (32)));
typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef long long __v4di __attribute__ ((__vector_size__ (32)));
typedef int __v8si __attribute__ ((__vector_size__ (32)));
typedef short __v16hi __attribute__ ((__vector_size__ (32)));
typedef char __v32qi __attribute__ ((__vector_size__ (32)));


typedef unsigned long long __v4du __attribute__ ((__vector_size__ (32)));
typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));
typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));
typedef unsigned char __v32qu __attribute__ ((__vector_size__ (32)));



typedef __signed char __v32qs __attribute__((__vector_size__(32)));

typedef float __m256 __attribute__ ((__vector_size__ (32)));
typedef double __m256d __attribute__((__vector_size__(32)));
typedef long long __m256i __attribute__((__vector_size__(32)));





/// \brief Adds two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \returns A 256-bit vector of [4 x double] containing the sums of both
///    operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_add_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a+(__v4df)__b);
}

/// \brief Adds two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \returns A 256-bit vector of [8 x float] containing the sums of both
///    operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_add_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a+(__v8sf)__b);
}

/// \brief Subtracts two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing the minuend.
/// \param __b
///    A 256-bit vector of [4 x double] containing the subtrahend.
/// \returns A 256-bit vector of [4 x double] containing the differences between
///    both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_sub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a-(__v4df)__b);
}

/// \brief Subtracts two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSUBPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing the minuend.
/// \param __b
///    A 256-bit vector of [8 x float] containing the subtrahend.
/// \returns A 256-bit vector of [8 x float] containing the differences between
///    both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_sub_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a-(__v8sf)__b);
}

/// \brief Adds the even-indexed values and subtracts the odd-indexed values of
///    two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSUBPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing the left source operand.
/// \param __b
///    A 256-bit vector of [4 x double] containing the right source operand.
/// \returns A 256-bit vector of [4 x double] containing the alternating sums
///    and differences between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_addsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_addsubpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Adds the even-indexed values and subtracts the odd-indexed values of
///    two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VADDSUBPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing the left source operand.
/// \param __b
///    A 256-bit vector of [8 x float] containing the right source operand.
/// \returns A 256-bit vector of [8 x float] containing the alternating sums and
///    differences between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_addsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_addsubps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Divides two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing the dividend.
/// \param __b
///    A 256-bit vector of [4 x double] containing the divisor.
/// \returns A 256-bit vector of [4 x double] containing the quotients of both
///    operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_div_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a/(__v4df)__b);
}

/// \brief Divides two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VDIVPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing the dividend.
/// \param __b
///    A 256-bit vector of [8 x float] containing the divisor.
/// \returns A 256-bit vector of [8 x float] containing the quotients of both
///    operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_div_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a/(__v8sf)__b);
}

/// \brief Compares two 256-bit vectors of [4 x double] and returns the greater
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \returns A 256-bit vector of [4 x double] containing the maximum values
///    between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_max_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_maxpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Compares two 256-bit vectors of [8 x float] and returns the greater
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMAXPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \returns A 256-bit vector of [8 x float] containing the maximum values
///    between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_max_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_maxps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Compares two 256-bit vectors of [4 x double] and returns the lesser
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \returns A 256-bit vector of [4 x double] containing the minimum values
///    between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_min_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_minpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Compares two 256-bit vectors of [8 x float] and returns the lesser
///    of each pair of values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMINPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \returns A 256-bit vector of [8 x float] containing the minimum values
///    between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_min_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_minps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Multiplies two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the operands.
/// \returns A 256-bit vector of [4 x double] containing the products of both
///    operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_mul_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a * (__v4df)__b);
}

/// \brief Multiplies two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMULPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the operands.
/// \returns A 256-bit vector of [8 x float] containing the products of both
///    operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_mul_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a * (__v8sf)__b);
}

/// \brief Calculates the square roots of the values in a 256-bit vector of
///    [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 256-bit vector of [4 x double] containing the square roots of the
///    values in the operand.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_sqrt_pd(__m256d __a)
{
  return (__m256d)__builtin_ia32_sqrtpd256((__v4df)__a);
}

/// \brief Calculates the square roots of the values in a 256-bit vector of
///    [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VSQRTPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the square roots of the
///    values in the operand.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_sqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_sqrtps256((__v8sf)__a);
}

/// \brief Calculates the reciprocal square roots of the values in a 256-bit
///    vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRSQRTPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the reciprocal square
///    roots of the values in the operand.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_rsqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rsqrtps256((__v8sf)__a);
}

/// \brief Calculates the reciprocals of the values in a 256-bit vector of
///    [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VRCPPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the reciprocals of the
///    values in the operand.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_rcp_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rcpps256((__v8sf)__a);
}

/// \brief Rounds the values in a 256-bit vector of [4 x double] as specified
///    by the byte operand. The source values are rounded to integer values and
///    returned as 64-bit double-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_round_pd(__m256d V, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.
///
/// \param V
///    A 256-bit vector of [4 x double].
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used. \n
///      1: The PE field is not updated. \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M. \n
///      1: Use the current MXCSR setting. \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest. \n
///      01: Downward (toward negative infinity). \n
///      10: Upward (toward positive infinity). \n
///      11: Truncated.
/// \returns A 256-bit vector of [4 x double] containing the rounded values.



/// \brief Rounds the values stored in a 256-bit vector of [8 x float] as
///    specified by the byte operand. The source values are rounded to integer
///    values and returned as floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_round_ps(__m256 V, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.
///
/// \param V
///    A 256-bit vector of [8 x float].
/// \param M
///    An integer value that specifies the rounding operation. \n
///    Bits [7:4] are reserved. \n
///    Bit [3] is a precision exception value: \n
///      0: A normal PE exception is used. \n
///      1: The PE field is not updated. \n
///    Bit [2] is the rounding control source: \n
///      0: Use bits [1:0] of \a M. \n
///      1: Use the current MXCSR setting. \n
///    Bits [1:0] contain the rounding control definition: \n
///      00: Nearest. \n
///      01: Downward (toward negative infinity). \n
///      10: Upward (toward positive infinity). \n
///      11: Truncated.
/// \returns A 256-bit vector of [8 x float] containing the rounded values.



/// \brief Rounds up the values stored in a 256-bit vector of [4 x double]. The
///    source values are rounded up to integer values and returned as 64-bit
///    double-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_ceil_pd(__m256d V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.
///
/// \param V
///    A 256-bit vector of [4 x double].
/// \returns A 256-bit vector of [4 x double] containing the rounded up values.


/// \brief Rounds down the values stored in a 256-bit vector of [4 x double].
///    The source values are rounded down to integer values and returned as
///    64-bit double-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_floor_pd(__m256d V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPD </c> instruction.
///
/// \param V
///    A 256-bit vector of [4 x double].
/// \returns A 256-bit vector of [4 x double] containing the rounded down
///    values.


/// \brief Rounds up the values stored in a 256-bit vector of [8 x float]. The
///    source values are rounded up to integer values and returned as
///    floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_ceil_ps(__m256 V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.
///
/// \param V
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the rounded up values.


/// \brief Rounds down the values stored in a 256-bit vector of [8 x float]. The
///    source values are rounded down to integer values and returned as
///    floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_floor_ps(__m256 V);
/// \endcode
///
/// This intrinsic corresponds to the <c> VROUNDPS </c> instruction.
///
/// \param V
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the rounded down values.



/// \brief Performs a bitwise AND of two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \returns A 256-bit vector of [4 x double] containing the bitwise AND of the
///    values between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_and_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a & (__v4du)__b);
}

/// \brief Performs a bitwise AND of two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \returns A 256-bit vector of [8 x float] containing the bitwise AND of the
///    values between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_and_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a & (__v8su)__b);
}

/// \brief Performs a bitwise AND of two 256-bit vectors of [4 x double], using
///    the one's complement of the values contained in the first source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDNPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing the left source operand. The
///    one's complement of this value is used in the bitwise AND.
/// \param __b
///    A 256-bit vector of [4 x double] containing the right source operand.
/// \returns A 256-bit vector of [4 x double] containing the bitwise AND of the
///    values of the second operand and the one's complement of the first
///    operand.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_andnot_pd(__m256d __a, __m256d __b)
{
  return (__m256d)(~(__v4du)__a & (__v4du)__b);
}

/// \brief Performs a bitwise AND of two 256-bit vectors of [8 x float], using
///    the one's complement of the values contained in the first source operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VANDNPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing the left source operand. The
///    one's complement of this value is used in the bitwise AND.
/// \param __b
///    A 256-bit vector of [8 x float] containing the right source operand.
/// \returns A 256-bit vector of [8 x float] containing the bitwise AND of the
///    values of the second operand and the one's complement of the first
///    operand.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_andnot_ps(__m256 __a, __m256 __b)
{
  return (__m256)(~(__v8su)__a & (__v8su)__b);
}

/// \brief Performs a bitwise OR of two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VORPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \returns A 256-bit vector of [4 x double] containing the bitwise OR of the
///    values between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_or_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a | (__v4du)__b);
}

/// \brief Performs a bitwise OR of two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VORPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \returns A 256-bit vector of [8 x float] containing the bitwise OR of the
///    values between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_or_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a | (__v8su)__b);
}

/// \brief Performs a bitwise XOR of two 256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
/// \returns A 256-bit vector of [4 x double] containing the bitwise XOR of the
///    values between both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_xor_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a ^ (__v4du)__b);
}

/// \brief Performs a bitwise XOR of two 256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
/// \returns A 256-bit vector of [8 x float] containing the bitwise XOR of the
///    values between both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_xor_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a ^ (__v8su)__b);
}


/// \brief Horizontally adds the adjacent pairs of values contained in two
///    256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHADDPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
///    The horizontal sums of the values are returned in the even-indexed
///    elements of a vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
///    The horizontal sums of the values are returned in the odd-indexed
///    elements of a vector of [4 x double].
/// \returns A 256-bit vector of [4 x double] containing the horizontal sums of
///    both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_hadd_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_haddpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Horizontally adds the adjacent pairs of values contained in two
///    256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHADDPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
///    The horizontal sums of the values are returned in the elements with
///    index 0, 1, 4, 5 of a vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
///    The horizontal sums of the values are returned in the elements with
///    index 2, 3, 6, 7 of a vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the horizontal sums of
///    both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_hadd_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_haddps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in two
///    256-bit vectors of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHSUBPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing one of the source operands.
///    The horizontal differences between the values are returned in the
///    even-indexed elements of a vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double] containing one of the source operands.
///    The horizontal differences between the values are returned in the
///    odd-indexed elements of a vector of [4 x double].
/// \returns A 256-bit vector of [4 x double] containing the horizontal
///    differences of both operands.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_hsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_hsubpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Horizontally subtracts the adjacent pairs of values contained in two
///    256-bit vectors of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VHSUBPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing one of the source operands.
///    The horizontal differences between the values are returned in the
///    elements with index 0, 1, 4, 5 of a vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float] containing one of the source operands.
///    The horizontal differences between the values are returned in the
///    elements with index 2, 3, 6, 7 of a vector of [8 x float].
/// \returns A 256-bit vector of [8 x float] containing the horizontal
///    differences of both operands.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_hsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_hsubps256((__v8sf)__a, (__v8sf)__b);
}


/// \brief Copies the values in a 128-bit vector of [2 x double] as specified
///    by the 128-bit integer vector operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __c
///    A 128-bit integer vector operand specifying how the values are to be
///    copied. \n
///    Bit [1]: \n
///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned
///         vector. \n
///      1: Bits [127:64] of the source are copied to bits [63:0] of the
///         returned vector. \n
///    Bit [65]: \n
///      0: Bits [63:0] of the source are copied to bits [127:64] of the
///         returned vector. \n
///      1: Bits [127:64] of the source are copied to bits [127:64] of the
///         returned vector.
/// \returns A 128-bit vector of [2 x double] containing the copied values.
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_permutevar_pd(__m128d __a, __m128i __c)
{
  return (__m128d)__builtin_ia32_vpermilvarpd((__v2df)__a, (__v2di)__c);
}

/// \brief Copies the values in a 256-bit vector of [4 x double] as specified
///    by the 256-bit integer vector operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \param __c
///    A 256-bit integer vector operand specifying how the values are to be
///    copied. \n
///    Bit [1]: \n
///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned
///         vector. \n
///      1: Bits [127:64] of the source are copied to bits [63:0] of the
///         returned vector. \n
///    Bit [65]: \n
///      0: Bits [63:0] of the source are copied to bits [127:64] of the
///         returned vector. \n
///      1: Bits [127:64] of the source are copied to bits [127:64] of the
///         returned vector. \n
///    Bit [129]: \n
///      0: Bits [191:128] of the source are copied to bits [191:128] of the
///         returned vector. \n
///      1: Bits [255:192] of the source are copied to bits [191:128] of the
///         returned vector. \n
///    Bit [193]: \n
///      0: Bits [191:128] of the source are copied to bits [255:192] of the
///         returned vector. \n
///      1: Bits [255:192] of the source are copied to bits [255:192] of the
///    returned vector.
/// \returns A 256-bit vector of [4 x double] containing the copied values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_permutevar_pd(__m256d __a, __m256i __c)
{
  return (__m256d)__builtin_ia32_vpermilvarpd256((__v4df)__a, (__v4di)__c);
}

/// \brief Copies the values stored in a 128-bit vector of [4 x float] as
///    specified by the 128-bit integer vector operand.
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __c
///    A 128-bit integer vector operand specifying how the values are to be
///    copied. \n
///    Bits [1:0]: \n
///      00: Bits [31:0] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [31:0] of the
///          returned vector. \n
///    Bits [33:32]: \n
///      00: Bits [31:0] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [63:32] of the
///          returned vector. \n
///    Bits [65:64]: \n
///      00: Bits [31:0] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [95:64] of the
///          returned vector. \n
///    Bits [97:96]: \n
///      00: Bits [31:0] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [127:96] of the
///          returned vector.
/// \returns A 128-bit vector of [4 x float] containing the copied values.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_permutevar_ps(__m128 __a, __m128i __c)
{
  return (__m128)__builtin_ia32_vpermilvarps((__v4sf)__a, (__v4si)__c);
}

/// \brief Copies the values stored in a 256-bit vector of [8 x float] as
///    specified by the 256-bit integer vector operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \param __c
///    A 256-bit integer vector operand specifying how the values are to be
///    copied. \n
///    Bits [1:0]: \n
///      00: Bits [31:0] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [31:0] of the
///          returned vector. \n
///    Bits [33:32]: \n
///      00: Bits [31:0] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [63:32] of the
///          returned vector. \n
///    Bits [65:64]: \n
///      00: Bits [31:0] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [95:64] of the
///          returned vector. \n
///    Bits [97:96]: \n
///      00: Bits [31:0] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [127:96] of the
///          returned vector. \n
///    Bits [129:128]: \n
///      00: Bits [159:128] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [159:128] of the
///          returned vector. \n
///    Bits [161:160]: \n
///      00: Bits [159:128] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [191:160] of the
///          returned vector. \n
///    Bits [193:192]: \n
///      00: Bits [159:128] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [223:192] of the
///          returned vector. \n
///    Bits [225:224]: \n
///      00: Bits [159:128] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [255:224] of the
///          returned vector.
/// \returns A 256-bit vector of [8 x float] containing the copied values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_permutevar_ps(__m256 __a, __m256i __c)
{
  return (__m256)__builtin_ia32_vpermilvarps256((__v8sf)__a, (__v8si)__c);
}

/// \brief Copies the values in a 128-bit vector of [2 x double] as specified
///    by the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_permute_pd(__m128d A, __const int C);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.
///
/// \param A
///    A 128-bit vector of [2 x double].
/// \param C
///    An immediate integer operand specifying how the values are to be
///    copied. \n
///    Bit [0]: \n
///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned
///         vector. \n
///      1: Bits [127:64] of the source are copied to bits [63:0] of the
///         returned vector. \n
///    Bit [1]: \n
///      0: Bits [63:0] of the source are copied to bits [127:64] of the
///         returned vector. \n
///      1: Bits [127:64] of the source are copied to bits [127:64] of the
///         returned vector.
/// \returns A 128-bit vector of [2 x double] containing the copied values.





/// \brief Copies the values in a 256-bit vector of [4 x double] as specified by
///    the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_permute_pd(__m256d A, __const int C);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERMILPD </c> instruction.
///
/// \param A
///    A 256-bit vector of [4 x double].
/// \param C
///    An immediate integer operand specifying how the values are to be
///    copied. \n
///    Bit [0]: \n
///      0: Bits [63:0] of the source are copied to bits [63:0] of the returned
///         vector. \n
///      1: Bits [127:64] of the source are copied to bits [63:0] of the
///         returned vector. \n
///    Bit [1]: \n
///      0: Bits [63:0] of the source are copied to bits [127:64] of the
///         returned vector. \n
///      1: Bits [127:64] of the source are copied to bits [127:64] of the
///         returned vector. \n
///    Bit [2]: \n
///      0: Bits [191:128] of the source are copied to bits [191:128] of the
///         returned vector. \n
///      1: Bits [255:192] of the source are copied to bits [191:128] of the
///         returned vector. \n
///    Bit [3]: \n
///      0: Bits [191:128] of the source are copied to bits [255:192] of the
///         returned vector. \n
///      1: Bits [255:192] of the source are copied to bits [255:192] of the
///         returned vector.
/// \returns A 256-bit vector of [4 x double] containing the copied values.








/// \brief Copies the values in a 128-bit vector of [4 x float] as specified by
///    the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_permute_ps(__m128 A, __const int C);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.
///
/// \param A
///    A 128-bit vector of [4 x float].
/// \param C
///    An immediate integer operand specifying how the values are to be
///    copied. \n
///    Bits [1:0]: \n
///      00: Bits [31:0] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [31:0] of the
///          returned vector. \n
///    Bits [3:2]: \n
///      00: Bits [31:0] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [63:32] of the
///          returned vector. \n
///    Bits [5:4]: \n
///      00: Bits [31:0] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [95:64] of the
///          returned vector. \n
///    Bits [7:6]: \n
///      00: Bits [31:0] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [127:96] of the
///          returned vector.
/// \returns A 128-bit vector of [4 x float] containing the copied values.






/// \brief Copies the values in a 256-bit vector of [8 x float] as specified by
///    the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_permute_ps(__m256 A, __const int C);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERMILPS </c> instruction.
///
/// \param A
///    A 256-bit vector of [8 x float].
/// \param C
///    An immediate integer operand specifying how the values are to be \n
///    copied. \n
///    Bits [1:0]: \n
///      00: Bits [31:0] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [31:0] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [31:0] of the
///          returned vector. \n
///    Bits [3:2]: \n
///      00: Bits [31:0] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [63:32] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [63:32] of the
///          returned vector. \n
///    Bits [5:4]: \n
///      00: Bits [31:0] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [95:64] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [95:64] of the
///          returned vector. \n
///    Bits [7:6]: \n
///      00: Bits [31:qq0] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      01: Bits [63:32] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      10: Bits [95:64] of the source are copied to bits [127:96] of the
///          returned vector. \n
///      11: Bits [127:96] of the source are copied to bits [127:96] of the
///          returned vector. \n
///    Bits [1:0]: \n
///      00: Bits [159:128] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [159:128] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [159:128] of the
///          returned vector. \n
///    Bits [3:2]: \n
///      00: Bits [159:128] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [191:160] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [191:160] of the
///          returned vector. \n
///    Bits [5:4]: \n
///      00: Bits [159:128] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [223:192] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [223:192] of the
///          returned vector. \n
///    Bits [7:6]: \n
///      00: Bits [159:128] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      01: Bits [191:160] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      10: Bits [223:192] of the source are copied to bits [255:224] of the
///          returned vector. \n
///      11: Bits [255:224] of the source are copied to bits [255:224] of the
///          returned vector.
/// \returns A 256-bit vector of [8 x float] containing the copied values.

# 1209 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4

/// \brief Permutes 128-bit data values stored in two 256-bit vectors of
///    [4 x double], as specified by the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_permute2f128_pd(__m256d V1, __m256d V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.
///
/// \param V1
///    A 256-bit vector of [4 x double].
/// \param V2
///    A 256-bit vector of [4 x double.
/// \param M
///    An immediate integer operand specifying how the values are to be
///    permuted. \n
///    Bits [1:0]: \n
///      00: Bits [127:0] of operand \a V1 are copied to bits [127:0] of the
///          destination. \n
///      01: Bits [255:128] of operand \a V1 are copied to bits [127:0] of the
///          destination. \n
///      10: Bits [127:0] of operand \a V2 are copied to bits [127:0] of the
///          destination. \n
///      11: Bits [255:128] of operand \a V2 are copied to bits [127:0] of the
///          destination. \n
///    Bits [5:4]: \n
///      00: Bits [127:0] of operand \a V1 are copied to bits [255:128] of the
///          destination. \n
///      01: Bits [255:128] of operand \a V1 are copied to bits [255:128] of the
///          destination. \n
///      10: Bits [127:0] of operand \a V2 are copied to bits [255:128] of the
///          destination. \n
///      11: Bits [255:128] of operand \a V2 are copied to bits [255:128] of the
///          destination.
/// \returns A 256-bit vector of [4 x double] containing the copied values.




/// \brief Permutes 128-bit data values stored in two 256-bit vectors of
///    [8 x float], as specified by the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_permute2f128_ps(__m256 V1, __m256 V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.
///
/// \param V1
///    A 256-bit vector of [8 x float].
/// \param V2
///    A 256-bit vector of [8 x float].
/// \param M
///    An immediate integer operand specifying how the values are to be
///    permuted. \n
///    Bits [1:0]: \n
///    00: Bits [127:0] of operand \a V1 are copied to bits [127:0] of the
///    destination. \n
///    01: Bits [255:128] of operand \a V1 are copied to bits [127:0] of the
///    destination. \n
///    10: Bits [127:0] of operand \a V2 are copied to bits [127:0] of the
///    destination. \n
///    11: Bits [255:128] of operand \a V2 are copied to bits [127:0] of the
///    destination. \n
///    Bits [5:4]: \n
///    00: Bits [127:0] of operand \a V1 are copied to bits [255:128] of the
///    destination. \n
///    01: Bits [255:128] of operand \a V1 are copied to bits [255:128] of the
///    destination. \n
///    10: Bits [127:0] of operand \a V2 are copied to bits [255:128] of the
///    destination. \n
///    11: Bits [255:128] of operand \a V2 are copied to bits [255:128] of the
///    destination.
/// \returns A 256-bit vector of [8 x float] containing the copied values.




/// \brief Permutes 128-bit data values stored in two 256-bit integer vectors,
///    as specified by the immediate integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256i _mm256_permute2f128_si256(__m256i V1, __m256i V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VPERM2F128 </c> instruction.
///
/// \param V1
///    A 256-bit integer vector.
/// \param V2
///    A 256-bit integer vector.
/// \param M
///    An immediate integer operand specifying how the values are to be copied.
///    Bits [1:0]: \n
///    00: Bits [127:0] of operand \a V1 are copied to bits [127:0] of the
///    destination. \n
///    01: Bits [255:128] of operand \a V1 are copied to bits [127:0] of the
///    destination. \n
///    10: Bits [127:0] of operand \a V2 are copied to bits [127:0] of the
///    destination. \n
///    11: Bits [255:128] of operand \a V2 are copied to bits [127:0] of the
///    destination. \n
///    Bits [5:4]: \n
///    00: Bits [127:0] of operand \a V1 are copied to bits [255:128] of the
///    destination. \n
///    01: Bits [255:128] of operand \a V1 are copied to bits [255:128] of the
///    destination. \n
///    10: Bits [127:0] of operand \a V2 are copied to bits [255:128] of the
///    destination. \n
///    11: Bits [255:128] of operand \a V2 are copied to bits [255:128] of the
///    destination.
/// \returns A 256-bit integer vector containing the copied values.





/// \brief Merges 64-bit double-precision data values stored in either of the
///    two 256-bit vectors of [4 x double], as specified by the immediate
///    integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_blend_pd(__m256d V1, __m256d V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VBLENDPD </c> instruction.
///
/// \param V1
///    A 256-bit vector of [4 x double].
/// \param V2
///    A 256-bit vector of [4 x double].
/// \param M
///    An immediate integer operand, with mask bits [3:0] specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    index of a copied value. When a mask bit is 0, the corresponding 64-bit
///    element in operand \a V1 is copied to the same position in the
///    destination. When a mask bit is 1, the corresponding 64-bit element in
///    operand \a V2 is copied to the same position in the destination.
/// \returns A 256-bit vector of [4 x double] containing the copied values.








/// \brief Merges 32-bit single-precision data values stored in either of the
///    two 256-bit vectors of [8 x float], as specified by the immediate
///    integer operand.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_blend_ps(__m256 V1, __m256 V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VBLENDPS </c> instruction.
///
/// \param V1
///    A 256-bit vector of [8 x float].
/// \param V2
///    A 256-bit vector of [8 x float].
/// \param M
///    An immediate integer operand, with mask bits [7:0] specifying how the
///    values are to be copied. The position of the mask bit corresponds to the
///    index of a copied value. When a mask bit is 0, the corresponding 32-bit
///    element in operand \a V1 is copied to the same position in the
///    destination. When a mask bit is 1, the corresponding 32-bit element in
///    operand \a V2 is copied to the same position in the destination.
/// \returns A 256-bit vector of [8 x float] containing the copied values.

# 1400 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4

/// \brief Merges 64-bit double-precision data values stored in either of the
///    two 256-bit vectors of [4 x double], as specified by the 256-bit vector
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBLENDVPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double].
/// \param __c
///    A 256-bit vector operand, with mask bits 255, 191, 127, and 63 specifying
///    how the values are to be copied. The position of the mask bit corresponds
///    to the most significant bit of a copied value. When a mask bit is 0, the
///    corresponding 64-bit element in operand \a __a is copied to the same
///    position in the destination. When a mask bit is 1, the corresponding
///    64-bit element in operand \a __b is copied to the same position in the
///    destination.
/// \returns A 256-bit vector of [4 x double] containing the copied values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_blendv_pd(__m256d __a, __m256d __b, __m256d __c)
{
  return (__m256d)__builtin_ia32_blendvpd256(
    (__v4df)__a, (__v4df)__b, (__v4df)__c);
}

/// \brief Merges 32-bit single-precision data values stored in either of the
///    two 256-bit vectors of [8 x float], as specified by the 256-bit vector
///    operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBLENDVPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float].
/// \param __c
///    A 256-bit vector operand, with mask bits 255, 223, 191, 159, 127, 95, 63,
///    and 31 specifying how the values are to be copied. The position of the
///    mask bit corresponds to the most significant bit of a copied value. When
///    a mask bit is 0, the corresponding 32-bit element in operand \a __a is
///    copied to the same position in the destination. When a mask bit is 1, the
///    corresponding 32-bit element in operand \a __b is copied to the same
///    position in the destination.
/// \returns A 256-bit vector of [8 x float] containing the copied values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_blendv_ps(__m256 __a, __m256 __b, __m256 __c)
{
  return (__m256)__builtin_ia32_blendvps256(
    (__v8sf)__a, (__v8sf)__b, (__v8sf)__c);
}


/// \brief Computes two dot products in parallel, using the lower and upper
///    halves of two [8 x float] vectors as input to the two computations, and
///    returning the two dot products in the lower and upper halves of the
///    [8 x float] result.
///
///    The immediate integer operand controls which input elements will
///    contribute to the dot product, and where the final results are returned.
///    In general, for each dot product, the four corresponding elements of the
///    input vectors are multiplied; the first two and second two products are
///    summed, then the two sums are added to form the final result.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_dp_ps(__m256 V1, __m256 V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VDPPS </c> instruction.
///
/// \param V1
///    A vector of [8 x float] values, treated as two [4 x float] vectors.
/// \param V2
///    A vector of [8 x float] values, treated as two [4 x float] vectors.
/// \param M
///    An immediate integer argument. Bits [7:4] determine which elements of
///    the input vectors are used, with bit [4] corresponding to the lowest
///    element and bit [7] corresponding to the highest element of each [4 x
///    float] subvector. If a bit is set, the corresponding elements from the
///    two input vectors are used as an input for dot product; otherwise that
///    input is treated as zero. Bits [3:0] determine which elements of the
///    result will receive a copy of the final dot product, with bit [0]
///    corresponding to the lowest element and bit [3] corresponding to the
///    highest element of each [4 x float] subvector. If a bit is set, the dot
///    product is returned in the corresponding element; otherwise that element
///    is set to zero. The bitmask is applied in the same way to each of the
///    two parallel dot product computations.
/// \returns A 256-bit vector of [8 x float] containing the two dot products.





/// \brief Selects 8 float values from the 256-bit operands of [8 x float], as
///    specified by the immediate value operand.
///
///    The four selected elements in each operand are copied to the destination
///    according to the bits specified in the immediate operand. The selected
///    elements from the first 256-bit operand are copied to bits [63:0] and
///    bits [191:128] of the destination, and the selected elements from the
///    second 256-bit operand are copied to bits [127:64] and bits [255:192] of
///    the destination. For example, if bits [7:0] of the immediate operand
///    contain a value of 0xFF, the 256-bit destination vector would contain the
///    following values: b[7], b[7], a[7], a[7], b[3], b[3], a[3], a[3].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_shuffle_ps(__m256 a, __m256 b, __const int mask);
/// \endcode
///
/// This intrinsic corresponds to the <c> VSHUFPS </c> instruction.
///
/// \param a
///    A 256-bit vector of [8 x float]. The four selected elements in this
///    operand are copied to bits [63:0] and bits [191:128] in the destination,
///    according to the bits specified in the immediate operand.
/// \param b
///    A 256-bit vector of [8 x float]. The four selected elements in this
///    operand are copied to bits [127:64] and bits [255:192] in the
///    destination, according to the bits specified in the immediate operand.
/// \param mask
///    An immediate value containing an 8-bit value specifying which elements to
///    copy from \a a and \a b \n.
///    Bits [3:0] specify the values copied from operand \a a. \n
///    Bits [7:4] specify the values copied from operand \a b. \n
///    The destinations within the 256-bit destination are assigned values as
///    follows, according to the bit value assignments described below: \n
///    Bits [1:0] are used to assign values to bits [31:0] and [159:128] in the
///    destination. \n
///    Bits [3:2] are used to assign values to bits [63:32] and [191:160] in the
///    destination. \n
///    Bits [5:4] are used to assign values to bits [95:64] and [223:192] in the
///    destination. \n
///    Bits [7:6] are used to assign values to bits [127:96] and [255:224] in
///    the destination. \n
///    Bit value assignments: \n
///    00: Bits [31:0] and [159:128] are copied from the selected operand. \n
///    01: Bits [63:32] and [191:160] are copied from the selected operand. \n
///    10: Bits [95:64] and [223:192] are copied from the selected operand. \n
///    11: Bits [127:96] and [255:224] are copied from the selected operand.
/// \returns A 256-bit vector of [8 x float] containing the shuffled values.

# 1560 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4

/// \brief Selects four double-precision values from the 256-bit operands of
///    [4 x double], as specified by the immediate value operand.
///
///    The selected elements from the first 256-bit operand are copied to bits
///    [63:0] and bits [191:128] in the destination, and the selected elements
///    from the second 256-bit operand are copied to bits [127:64] and bits
///    [255:192] in the destination. For example, if bits [3:0] of the immediate
///    operand contain a value of 0xF, the 256-bit destination vector would
///    contain the following values: b[3], a[3], b[1], a[1].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_shuffle_pd(__m256d a, __m256d b, __const int mask);
/// \endcode
///
/// This intrinsic corresponds to the <c> VSHUFPD </c> instruction.
///
/// \param a
///    A 256-bit vector of [4 x double].
/// \param b
///    A 256-bit vector of [4 x double].
/// \param mask
///    An immediate value containing 8-bit values specifying which elements to
///    copy from \a a and \a b: \n
///    Bit [0]=0: Bits [63:0] are copied from \a a to bits [63:0] of the
///    destination. \n
///    Bit [0]=1: Bits [127:64] are copied from \a a to bits [63:0] of the
///    destination. \n
///    Bit [1]=0: Bits [63:0] are copied from \a b to bits [127:64] of the
///    destination. \n
///    Bit [1]=1: Bits [127:64] are copied from \a b to bits [127:64] of the
///    destination. \n
///    Bit [2]=0: Bits [191:128] are copied from \a a to bits [191:128] of the
///    destination. \n
///    Bit [2]=1: Bits [255:192] are copied from \a a to bits [191:128] of the
///    destination. \n
///    Bit [3]=0: Bits [191:128] are copied from \a b to bits [255:192] of the
///    destination. \n
///    Bit [3]=1: Bits [255:192] are copied from \a b to bits [255:192] of the
///    destination.
/// \returns A 256-bit vector of [4 x double] containing the shuffled values.










# 1644 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4

/// \brief Compares each of the corresponding double-precision values of two
///    128-bit vectors of [2 x double], using the operation specified by the
///    immediate integer operand.
///
///    Returns a [2 x double] vector consisting of two doubles corresponding to
///    the two comparison results: zero if the comparison is false, and all 1's
///    if the comparison is true.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_cmp_pd(__m128d a, __m128d b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPPD </c> instruction.
///
/// \param a
///    A 128-bit vector of [2 x double].
/// \param b
///    A 128-bit vector of [2 x double].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 128-bit vector of [2 x double] containing the comparison results.




/// \brief Compares each of the corresponding values of two 128-bit vectors of
///    [4 x float], using the operation specified by the immediate integer
///    operand.
///
///    Returns a [4 x float] vector consisting of four floats corresponding to
///    the four comparison results: zero if the comparison is false, and all 1's
///    if the comparison is true.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_cmp_ps(__m128 a, __m128 b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPPS </c> instruction.
///
/// \param a
///    A 128-bit vector of [4 x float].
/// \param b
///    A 128-bit vector of [4 x float].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 128-bit vector of [4 x float] containing the comparison results.




/// \brief Compares each of the corresponding double-precision values of two
///    256-bit vectors of [4 x double], using the operation specified by the
///    immediate integer operand.
///
///    Returns a [4 x double] vector consisting of four doubles corresponding to
///    the four comparison results: zero if the comparison is false, and all 1's
///    if the comparison is true.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_cmp_pd(__m256d a, __m256d b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPPD </c> instruction.
///
/// \param a
///    A 256-bit vector of [4 x double].
/// \param b
///    A 256-bit vector of [4 x double].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 256-bit vector of [4 x double] containing the comparison results.




/// \brief Compares each of the corresponding values of two 256-bit vectors of
///    [8 x float], using the operation specified by the immediate integer
///    operand.
///
///    Returns a [8 x float] vector consisting of eight floats corresponding to
///    the eight comparison results: zero if the comparison is false, and all
///    1's if the comparison is true.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_cmp_ps(__m256 a, __m256 b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPPS </c> instruction.
///
/// \param a
///    A 256-bit vector of [8 x float].
/// \param b
///    A 256-bit vector of [8 x float].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 256-bit vector of [8 x float] containing the comparison results.




/// \brief Compares each of the corresponding scalar double-precision values of
///    two 128-bit vectors of [2 x double], using the operation specified by the
///    immediate integer operand.
///
///    If the result is true, all 64 bits of the destination vector are set;
///    otherwise they are cleared.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm_cmp_sd(__m128d a, __m128d b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPSD </c> instruction.
///
/// \param a
///    A 128-bit vector of [2 x double].
/// \param b
///    A 128-bit vector of [2 x double].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 128-bit vector of [2 x double] containing the comparison results.




/// \brief Compares each of the corresponding scalar values of two 128-bit
///    vectors of [4 x float], using the operation specified by the immediate
///    integer operand.
///
///    If the result is true, all 32 bits of the destination vector are set;
///    otherwise they are cleared.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm_cmp_ss(__m128 a, __m128 b, __const int c);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCMPSS </c> instruction.
///
/// \param a
///    A 128-bit vector of [4 x float].
/// \param b
///    A 128-bit vector of [4 x float].
/// \param c
///    An immediate integer operand, with bits [4:0] specifying which comparison
///    operation to use: \n
///    0x00 : Equal (ordered, non-signaling)
///    0x01 : Less-than (ordered, signaling)
///    0x02 : Less-than-or-equal (ordered, signaling)
///    0x03 : Unordered (non-signaling)
///    0x04 : Not-equal (unordered, non-signaling)
///    0x05 : Not-less-than (unordered, signaling)
///    0x06 : Not-less-than-or-equal (unordered, signaling)
///    0x07 : Ordered (non-signaling)
///    0x08 : Equal (unordered, non-signaling)
///    0x09 : Not-greater-than-or-equal (unordered, signaling)
///    0x0a : Not-greater-than (unordered, signaling)
///    0x0b : False (ordered, non-signaling)
///    0x0c : Not-equal (ordered, non-signaling)
///    0x0d : Greater-than-or-equal (ordered, signaling)
///    0x0e : Greater-than (ordered, signaling)
///    0x0f : True (unordered, non-signaling)
///    0x10 : Equal (ordered, signaling)
///    0x11 : Less-than (ordered, non-signaling)
///    0x12 : Less-than-or-equal (ordered, non-signaling)
///    0x13 : Unordered (signaling)
///    0x14 : Not-equal (unordered, signaling)
///    0x15 : Not-less-than (unordered, non-signaling)
///    0x16 : Not-less-than-or-equal (unordered, non-signaling)
///    0x17 : Ordered (signaling)
///    0x18 : Equal (unordered, signaling)
///    0x19 : Not-greater-than-or-equal (unordered, non-signaling)
///    0x1a : Not-greater-than (unordered, non-signaling)
///    0x1b : False (ordered, signaling)
///    0x1c : Not-equal (ordered, signaling)
///    0x1d : Greater-than-or-equal (ordered, non-signaling)
///    0x1e : Greater-than (ordered, non-signaling)
///    0x1f : True (unordered, signaling)
/// \returns A 128-bit vector of [4 x float] containing the comparison results.




/// \brief Takes a [8 x i32] vector and returns the vector element value
///    indexed by the immediate constant operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A 256-bit vector of [8 x i32].
/// \param __imm
///    An immediate integer operand with bits [2:0] determining which vector
///    element is extracted and returned.
/// \returns A 32-bit integer containing the extracted 32 bits of extended
///    packed data.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_extract_epi32(__m256i __a, __const int __imm)
{
  __v8si __b = (__v8si)__a;
  return __b[__imm & 7];
}

/// \brief Takes a [16 x i16] vector and returns the vector element value
///    indexed by the immediate constant operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A 256-bit integer vector of [16 x i16].
/// \param __imm
///    An immediate integer operand with bits [3:0] determining which vector
///    element is extracted and returned.
/// \returns A 32-bit integer containing the extracted 16 bits of zero extended
///    packed data.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_extract_epi16(__m256i __a, __const int __imm)
{
  __v16hi __b = (__v16hi)__a;
  return (unsigned short)__b[__imm & 15];
}

/// \brief Takes a [32 x i8] vector and returns the vector element value
///    indexed by the immediate constant operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A 256-bit integer vector of [32 x i8].
/// \param __imm
///    An immediate integer operand with bits [4:0] determining which vector
///    element is extracted and returned.
/// \returns A 32-bit integer containing the extracted 8 bits of zero extended
///    packed data.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_extract_epi8(__m256i __a, __const int __imm)
{
  __v32qi __b = (__v32qi)__a;
  return (unsigned char)__b[__imm & 31];
}


/// \brief Takes a [4 x i64] vector and returns the vector element value
///    indexed by the immediate constant operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A 256-bit integer vector of [4 x i64].
/// \param __imm
///    An immediate integer operand with bits [1:0] determining which vector
///    element is extracted and returned.
/// \returns A 64-bit integer containing the extracted 64 bits of extended
///    packed data.
static __inline long long  __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_extract_epi64(__m256i __a, __const int __imm)
{
  __v4di __b = (__v4di)__a;
  return __b[__imm & 3];
}


/// \brief Takes a [8 x i32] vector and replaces the vector element value
///    indexed by the immediate constant operand by a new value. Returns the
///    modified vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A vector of [8 x i32] to be used by the insert operation.
/// \param __b
///    An integer value. The replacement value for the insert operation.
/// \param __imm
///    An immediate integer specifying the index of the vector element to be
///    replaced.
/// \returns A copy of vector \a __a, after replacing its element indexed by
///    \a __imm with \a __b.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_insert_epi32(__m256i __a, int __b, int __const __imm)
{
  __v8si __c = (__v8si)__a;
  __c[__imm & 7] = __b;
  return (__m256i)__c;
}


/// \brief Takes a [16 x i16] vector and replaces the vector element value
///    indexed by the immediate constant operand with a new value. Returns the
///    modified vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A vector of [16 x i16] to be used by the insert operation.
/// \param __b
///    An i16 integer value. The replacement value for the insert operation.
/// \param __imm
///    An immediate integer specifying the index of the vector element to be
///    replaced.
/// \returns A copy of vector \a __a, after replacing its element indexed by
///    \a __imm with \a __b.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_insert_epi16(__m256i __a, int __b, int __const __imm)
{
  __v16hi __c = (__v16hi)__a;
  __c[__imm & 15] = __b;
  return (__m256i)__c;
}

/// \brief Takes a [32 x i8] vector and replaces the vector element value
///    indexed by the immediate constant operand with a new value. Returns the
///    modified vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A vector of [32 x i8] to be used by the insert operation.
/// \param __b
///    An i8 integer value. The replacement value for the insert operation.
/// \param __imm
///    An immediate integer specifying the index of the vector element to be
///    replaced.
/// \returns A copy of vector \a __a, after replacing its element indexed by
///    \a __imm with \a __b.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_insert_epi8(__m256i __a, int __b, int __const __imm)
{
  __v32qi __c = (__v32qi)__a;
  __c[__imm & 31] = __b;
  return (__m256i)__c;
}


/// \brief Takes a [4 x i64] vector and replaces the vector element value
///    indexed by the immediate constant operand with a new value. Returns the
///    modified vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128+COMPOSITE </c>
///   instruction.
///
/// \param __a
///    A vector of [4 x i64] to be used by the insert operation.
/// \param __b
///    A 64-bit integer value. The replacement value for the insert operation.
/// \param __imm
///    An immediate integer specifying the index of the vector element to be
///    replaced.
/// \returns A copy of vector \a __a, after replacing its element indexed by
///     \a __imm with \a __b.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_insert_epi64(__m256i __a, long long __b, int __const __imm)
{
  __v4di __c = (__v4di)__a;
  __c[__imm & 3] = __b;
  return (__m256i)__c;
}



/// \brief Converts a vector of [4 x i32] into a vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTDQ2PD </c> instruction.
///
/// \param __a
///    A 128-bit integer vector of [4 x i32].
/// \returns A 256-bit vector of [4 x double] containing the converted values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtepi32_pd(__m128i __a)
{
  return (__m256d)__builtin_convertvector((__v4si)__a, __v4df);
}

/// \brief Converts a vector of [8 x i32] into a vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTDQ2PS </c> instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \returns A 256-bit vector of [8 x float] containing the converted values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtepi32_ps(__m256i __a)
{
  return (__m256)__builtin_ia32_cvtdq2ps256((__v8si) __a);
}

/// \brief Converts a 256-bit vector of [4 x double] into a 128-bit vector of
///    [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPD2PS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 128-bit vector of [4 x float] containing the converted values.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtpd_ps(__m256d __a)
{
  return (__m128)__builtin_ia32_cvtpd2ps256((__v4df) __a);
}

/// \brief Converts a vector of [8 x float] into a vector of [8 x i32].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPS2DQ </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit integer vector containing the converted values.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvtps2dq256((__v8sf) __a);
}

/// \brief Converts a 128-bit vector of [4 x float] into a 256-bit vector of [4
///    x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPS2PD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 256-bit vector of [4 x double] containing the converted values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtps_pd(__m128 __a)
{
  return (__m256d)__builtin_convertvector((__v4sf)__a, __v4df);
}

/// \brief Converts a 256-bit vector of [4 x double] into a 128-bit vector of [4
///    x i32], truncating the result by rounding towards zero when it is
///    inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTPD2DQ </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 128-bit integer vector containing the converted values.
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvttpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq256((__v4df) __a);
}

/// \brief Converts a 256-bit vector of [4 x double] into a 128-bit vector of [4
///    x i32]. When a conversion is inexact, the value returned is rounded
///    according to the rounding control bits in the MXCSR register.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPD2DQ </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 128-bit integer vector containing the converted values.
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvtpd2dq256((__v4df) __a);
}

/// \brief Converts a vector of [8 x float] into a vector of [8 x i32],
///    truncating the result by rounding towards zero when it is inexact.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTTPS2DQ </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 256-bit integer vector containing the converted values.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvttps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvttps2dq256((__v8sf) __a);
}

/// \brief Returns the first element of the input vector of [4 x double].
///
/// \headerfile <avxintrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 64 bit double containing the first element of the input vector.
static __inline double __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtsd_f64(__m256d __a)
{
 return __a[0];
}

/// \brief Returns the first element of the input vector of [8 x i32].
///
/// \headerfile <avxintrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __a
///    A 256-bit vector of [8 x i32].
/// \returns A 32 bit integer containing the first element of the input vector.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtsi256_si32(__m256i __a)
{
 __v8si __b = (__v8si)__a;
 return __b[0];
}

/// \brief Returns the first element of the input vector of [8 x float].
///
/// \headerfile <avxintrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///    instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 32 bit float containing the first element of the input vector.
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_cvtss_f32(__m256 __a)
{
 return __a[0];
}


/// \brief Moves and duplicates high-order (odd-indexed) values from a 256-bit
///    vector of [8 x float] to float values in a 256-bit vector of
///    [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSHDUP </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float]. \n
///    Bits [255:224] of \a __a are written to bits [255:224] and [223:192] of
///    the return value. \n
///    Bits [191:160] of \a __a are written to bits [191:160] and [159:128] of
///    the return value. \n
///    Bits [127:96] of \a __a are written to bits [127:96] and [95:64] of the
///    return value. \n
///    Bits [63:32] of \a __a are written to bits [63:32] and [31:0] of the
///    return value.
/// \returns A 256-bit vector of [8 x float] containing the moved and duplicated
///    values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_movehdup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 1, 1, 3, 3, 5, 5, 7, 7);
}

/// \brief Moves and duplicates low-order (even-indexed) values from a 256-bit
///    vector of [8 x float] to float values in a 256-bit vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVSLDUP </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float]. \n
///    Bits [223:192] of \a __a are written to bits [255:224] and [223:192] of
///    the return value. \n
///    Bits [159:128] of \a __a are written to bits [191:160] and [159:128] of
///    the return value. \n
///    Bits [95:64] of \a __a are written to bits [127:96] and [95:64] of the
///    return value. \n
///    Bits [31:0] of \a __a are written to bits [63:32] and [31:0] of the
///    return value.
/// \returns A 256-bit vector of [8 x float] containing the moved and duplicated
///    values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_moveldup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 0, 2, 2, 4, 4, 6, 6);
}

/// \brief Moves and duplicates double-precision floating point values from a
///    256-bit vector of [4 x double] to double-precision values in a 256-bit
///    vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double]. \n
///    Bits [63:0] of \a __a are written to bits [127:64] and [63:0] of the
///    return value. \n
///    Bits [191:128] of \a __a are written to bits [255:192] and [191:128] of
///    the return value.
/// \returns A 256-bit vector of [4 x double] containing the moved and
///    duplicated values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_movedup_pd(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 0, 2, 2);
}


/// \brief Unpacks the odd-indexed vector elements from two 256-bit vectors of
///    [4 x double] and interleaves them into a 256-bit vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKHPD </c> instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [4 x double]. \n
///    Bits [127:64] are written to bits [63:0] of the return value. \n
///    Bits [255:192] are written to bits [191:128] of the return value. \n
/// \param __b
///    A 256-bit floating-point vector of [4 x double]. \n
///    Bits [127:64] are written to bits [127:64] of the return value. \n
///    Bits [255:192] are written to bits [255:192] of the return value. \n
/// \returns A 256-bit vector of [4 x double] containing the interleaved values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_unpackhi_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 1, 5, 1+2, 5+2);
}

/// \brief Unpacks the even-indexed vector elements from two 256-bit vectors of
///    [4 x double] and interleaves them into a 256-bit vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD </c> instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [4 x double]. \n
///    Bits [63:0] are written to bits [63:0] of the return value. \n
///    Bits [191:128] are written to bits [191:128] of the return value.
/// \param __b
///    A 256-bit floating-point vector of [4 x double]. \n
///    Bits [63:0] are written to bits [127:64] of the return value. \n
///    Bits [191:128] are written to bits [255:192] of the return value. \n
/// \returns A 256-bit vector of [4 x double] containing the interleaved values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_unpacklo_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 0, 4, 0+2, 4+2);
}

/// \brief Unpacks the 32-bit vector elements 2, 3, 6 and 7 from each of the
///    two 256-bit vectors of [8 x float] and interleaves them into a 256-bit
///    vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKHPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float]. \n
///    Bits [95:64] are written to bits [31:0] of the return value. \n
///    Bits [127:96] are written to bits [95:64] of the return value. \n
///    Bits [223:192] are written to bits [159:128] of the return value. \n
///    Bits [255:224] are written to bits [223:192] of the return value.
/// \param __b
///    A 256-bit vector of [8 x float]. \n
///    Bits [95:64] are written to bits [63:32] of the return value. \n
///    Bits [127:96] are written to bits [127:96] of the return value. \n
///    Bits [223:192] are written to bits [191:160] of the return value. \n
///    Bits [255:224] are written to bits [255:224] of the return value.
/// \returns A 256-bit vector of [8 x float] containing the interleaved values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_unpackhi_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 2, 10, 2+1, 10+1, 6, 14, 6+1, 14+1);
}

/// \brief Unpacks the 32-bit vector elements 0, 1, 4 and 5 from each of the
///    two 256-bit vectors of [8 x float] and interleaves them into a 256-bit
///    vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float]. \n
///    Bits [31:0] are written to bits [31:0] of the return value. \n
///    Bits [63:32] are written to bits [95:64] of the return value. \n
///    Bits [159:128] are written to bits [159:128] of the return value. \n
///    Bits [191:160] are written to bits [223:192] of the return value.
/// \param __b
///    A 256-bit vector of [8 x float]. \n
///    Bits [31:0] are written to bits [63:32] of the return value. \n
///    Bits [63:32] are written to bits [127:96] of the return value. \n
///    Bits [159:128] are written to bits [191:160] of the return value. \n
///    Bits [191:160] are written to bits [255:224] of the return value.
/// \returns A 256-bit vector of [8 x float] containing the interleaved values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_unpacklo_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 0, 8, 0+1, 8+1, 4, 12, 4+1, 12+1);
}


/// \brief Given two 128-bit floating-point vectors of [2 x double], perform an
///    element-by-element comparison of the double-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the ZF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns the ZF flag in the EFLAGS register.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testz_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestzpd((__v2df)__a, (__v2df)__b);
}

/// \brief Given two 128-bit floating-point vectors of [2 x double], perform an
///    element-by-element comparison of the double-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the CF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns the CF flag in the EFLAGS register.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestcpd((__v2df)__a, (__v2df)__b);
}

/// \brief Given two 128-bit floating-point vectors of [2 x double], perform an
///    element-by-element comparison of the double-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,
///    otherwise it returns 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \param __b
///    A 128-bit vector of [2 x double].
/// \returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testnzc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestnzcpd((__v2df)__a, (__v2df)__b);
}

/// \brief Given two 128-bit floating-point vectors of [4 x float], perform an
///    element-by-element comparison of the single-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the ZF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns the ZF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testz_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestzps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Given two 128-bit floating-point vectors of [4 x float], perform an
///    element-by-element comparison of the single-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the CF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns the CF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestcps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Given two 128-bit floating-point vectors of [4 x float], perform an
///    element-by-element comparison of the single-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,
///    otherwise it returns 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \param __b
///    A 128-bit vector of [4 x float].
/// \returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_testnzc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestnzcps((__v4sf)__a, (__v4sf)__b);
}

/// \brief Given two 256-bit floating-point vectors of [4 x double], perform an
///    element-by-element comparison of the double-precision elements in the
///    first source vector and the corresponding elements in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the ZF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double].
/// \returns the ZF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testz_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestzpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Given two 256-bit floating-point vectors of [4 x double], perform an
///    element-by-element comparison of the double-precision elements in the
///    first source vector and the corresponding elements in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the CF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double].
/// \returns the CF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestcpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Given two 256-bit floating-point vectors of [4 x double], perform an
///    element-by-element comparison of the double-precision elements in the
///    first source vector and the corresponding elements in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of double-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of double-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,
///    otherwise it returns 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \param __b
///    A 256-bit vector of [4 x double].
/// \returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testnzc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestnzcpd256((__v4df)__a, (__v4df)__b);
}

/// \brief Given two 256-bit floating-point vectors of [8 x float], perform an
///    element-by-element comparison of the single-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the ZF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float].
/// \returns the ZF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testz_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestzps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Given two 256-bit floating-point vectors of [8 x float], perform an
///    element-by-element comparison of the single-precision element in the
///    first source vector and the corresponding element in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the CF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float].
/// \returns the CF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestcps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Given two 256-bit floating-point vectors of [8 x float], perform an
///    element-by-element comparison of the single-precision elements in the
///    first source vector and the corresponding elements in the second source
///    vector.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of single-precision elements where the
///    sign-bits of both elements are 1, the ZF flag is set to 0. Otherwise the
///    ZF flag is set to 1. \n
///    If there is at least one pair of single-precision elements where the
///    sign-bit of the first element is 0 and the sign-bit of the second element
///    is 1, the CF flag is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,
///    otherwise it returns 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VTESTPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \param __b
///    A 256-bit vector of [8 x float].
/// \returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testnzc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestnzcps256((__v8sf)__a, (__v8sf)__b);
}

/// \brief Given two 256-bit integer vectors, perform a bit-by-bit comparison
///    of the two source vectors.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of bits where both bits are 1, the ZF flag
///    is set to 0. Otherwise the ZF flag is set to 1. \n
///    If there is at least one pair of bits where the bit from the first source
///    vector is 0 and the bit from the second source vector is 1, the CF flag
///    is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the ZF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST </c> instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \param __b
///    A 256-bit integer vector.
/// \returns the ZF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testz_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestz256((__v4di)__a, (__v4di)__b);
}

/// \brief Given two 256-bit integer vectors, perform a bit-by-bit comparison
///    of the two source vectors.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of bits where both bits are 1, the ZF flag
///    is set to 0. Otherwise the ZF flag is set to 1. \n
///    If there is at least one pair of bits where the bit from the first source
///    vector is 0 and the bit from the second source vector is 1, the CF flag
///    is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns the value of the CF flag.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST </c> instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \param __b
///    A 256-bit integer vector.
/// \returns the CF flag.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestc256((__v4di)__a, (__v4di)__b);
}

/// \brief Given two 256-bit integer vectors, perform a bit-by-bit comparison
///    of the two source vectors.
///
///    The EFLAGS register is updated as follows: \n
///    If there is at least one pair of bits where both bits are 1, the ZF flag
///    is set to 0. Otherwise the ZF flag is set to 1. \n
///    If there is at least one pair of bits where the bit from the first source
///    vector is 0 and the bit from the second source vector is 1, the CF flag
///    is set to 0. Otherwise the CF flag is set to 1. \n
///    This intrinsic returns 1 if both the ZF and CF flags are set to 0,
///    otherwise it returns 0.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPTEST </c> instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \param __b
///    A 256-bit integer vector.
/// \returns 1 if both the ZF and CF flags are set to 0, otherwise returns 0.
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_testnzc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestnzc256((__v4di)__a, (__v4di)__b);
}


/// \brief Extracts the sign bits of double-precision floating point elements
///    in a 256-bit vector of [4 x double] and writes them to the lower order
///    bits of the return value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVMSKPD </c> instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double] containing the double-precision
///    floating point values with sign bits to be extracted.
/// \returns The sign bits from the operand, written to bits [3:0].
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_movemask_pd(__m256d __a)
{
  return __builtin_ia32_movmskpd256((__v4df)__a);
}

/// \brief Extracts the sign bits of double-precision floating point elements
///    in a 256-bit vector of [8 x float] and writes them to the lower order
///    bits of the return value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVMSKPS </c> instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float] containing the double-precision floating
///    point values with sign bits to be extracted.
/// \returns The sign bits from the operand, written to bits [7:0].
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_movemask_ps(__m256 __a)
{
  return __builtin_ia32_movmskps256((__v8sf)__a);
}


/// \brief Zeroes the contents of all XMM or YMM registers.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VZEROALL </c> instruction.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroall(void)
{
  __builtin_ia32_vzeroall();
}

/// \brief Zeroes the upper 128 bits (bits 255:128) of all YMM registers.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VZEROUPPER </c> instruction.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroupper(void)
{
  __builtin_ia32_vzeroupper();
}


/// \brief Loads a scalar single-precision floating point value from the
///    specified address pointed to by \a __a and broadcasts it to the elements
///    of a [4 x float] vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBROADCASTSS </c> instruction.
///
/// \param __a
///    The single-precision floating point value to be broadcast.
/// \returns A 128-bit vector of [4 x float] whose 32-bit elements are set
///    equal to the broadcast value.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_broadcast_ss(float __const *__a)
{
  float __f = *__a;
  return (__m128)(__v4sf){ __f, __f, __f, __f };
}

/// \brief Loads a scalar double-precision floating point value from the
///    specified address pointed to by \a __a and broadcasts it to the elements
///    of a [4 x double] vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBROADCASTSD </c> instruction.
///
/// \param __a
///    The double-precision floating point value to be broadcast.
/// \returns A 256-bit vector of [4 x double] whose 64-bit elements are set
///    equal to the broadcast value.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_broadcast_sd(double __const *__a)
{
  double __d = *__a;
  return (__m256d)(__v4df){ __d, __d, __d, __d };
}

/// \brief Loads a scalar single-precision floating point value from the
///    specified address pointed to by \a __a and broadcasts it to the elements
///    of a [8 x float] vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBROADCASTSS </c> instruction.
///
/// \param __a
///    The single-precision floating point value to be broadcast.
/// \returns A 256-bit vector of [8 x float] whose 32-bit elements are set
///    equal to the broadcast value.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_broadcast_ss(float __const *__a)
{
  float __f = *__a;
  return (__m256)(__v8sf){ __f, __f, __f, __f, __f, __f, __f, __f };
}

/// \brief Loads the data from a 128-bit vector of [2 x double] from the
///    specified address pointed to by \a __a and broadcasts it to 128-bit
///    elements in a 256-bit vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBROADCASTF128 </c> instruction.
///
/// \param __a
///    The 128-bit vector of [2 x double] to be broadcast.
/// \returns A 256-bit vector of [4 x double] whose 128-bit elements are set
///    equal to the broadcast value.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_broadcast_pd(__m128d __const *__a)
{
  return (__m256d)__builtin_ia32_vbroadcastf128_pd256((__v2df __const *)__a);
}

/// \brief Loads the data from a 128-bit vector of [4 x float] from the
///    specified address pointed to by \a __a and broadcasts it to 128-bit
///    elements in a 256-bit vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VBROADCASTF128 </c> instruction.
///
/// \param __a
///    The 128-bit vector of [4 x float] to be broadcast.
/// \returns A 256-bit vector of [8 x float] whose 128-bit elements are set
///    equal to the broadcast value.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_broadcast_ps(__m128 __const *__a)
{
  return (__m256)__builtin_ia32_vbroadcastf128_ps256((__v4sf __const *)__a);
}


/// \brief Loads 4 double-precision floating point values from a 32-byte aligned
///    memory location pointed to by \a __p into a vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPD </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a memory location containing
///    double-precision floating point values.
/// \returns A 256-bit vector of [4 x double] containing the moved values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_load_pd(double __const *__p)
{
  return *(__m256d *)__p;
}

/// \brief Loads 8 single-precision floating point values from a 32-byte aligned
///    memory location pointed to by \a __p into a vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a memory location containing float values.
/// \returns A 256-bit vector of [8 x float] containing the moved values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_load_ps(float __const *__p)
{
  return *(__m256 *)__p;
}

/// \brief Loads 4 double-precision floating point values from an unaligned
///    memory location pointed to by \a __p into a vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location containing double-precision floating
///    point values.
/// \returns A 256-bit vector of [4 x double] containing the moved values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu_pd(double __const *__p)
{
  struct __loadu_pd {
    __m256d __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_pd*)__p)->__v;
}

/// \brief Loads 8 single-precision floating point values from an unaligned
///    memory location pointed to by \a __p into a vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location containing single-precision floating
///    point values.
/// \returns A 256-bit vector of [8 x float] containing the moved values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu_ps(float __const *__p)
{
  struct __loadu_ps {
    __m256 __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_ps*)__p)->__v;
}

/// \brief Loads 256 bits of integer data from a 32-byte aligned memory
///    location pointed to by \a __p into elements of a 256-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQA </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a 256-bit integer vector containing integer
///    values.
/// \returns A 256-bit integer vector containing the moved values.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_load_si256(__m256i __const *__p)
{
  return *__p;
}

/// \brief Loads 256 bits of integer data from an unaligned memory location
///    pointed to by \a __p into a 256-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQU </c> instruction.
///
/// \param __p
///    A pointer to a 256-bit integer vector containing integer values.
/// \returns A 256-bit integer vector containing the moved values.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu_si256(__m256i __const *__p)
{
  struct __loadu_si256 {
    __m256i __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_si256*)__p)->__v;
}

/// \brief Loads 256 bits of integer data from an unaligned memory location
///    pointed to by \a __p into a 256-bit integer vector. This intrinsic may
///    perform better than \c _mm256_loadu_si256 when the data crosses a cache
///    line boundary.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VLDDQU </c> instruction.
///
/// \param __p
///    A pointer to a 256-bit integer vector containing integer values.
/// \returns A 256-bit integer vector containing the moved values.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_lddqu_si256(__m256i __const *__p)
{
  return (__m256i)__builtin_ia32_lddqu256((char __const *)__p);
}


/// \brief Stores double-precision floating point values from a 256-bit vector
///    of [4 x double] to a 32-byte aligned memory location pointed to by
///    \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPD </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a memory location that will receive the
///    double-precision floaing point values.
/// \param __a
///    A 256-bit vector of [4 x double] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_store_pd(double *__p, __m256d __a)
{
  *(__m256d *)__p = __a;
}

/// \brief Stores single-precision floating point values from a 256-bit vector
///    of [8 x float] to a 32-byte aligned memory location pointed to by \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVAPS </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a memory location that will receive the
///    float values.
/// \param __a
///    A 256-bit vector of [8 x float] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_store_ps(float *__p, __m256 __a)
{
  *(__m256 *)__p = __a;
}

/// \brief Stores double-precision floating point values from a 256-bit vector
///    of [4 x double] to an unaligned memory location pointed to by \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the double-precision
///    floating point values.
/// \param __a
///    A 256-bit vector of [4 x double] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu_pd(double *__p, __m256d __a)
{
  struct __storeu_pd {
    __m256d __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__p)->__v = __a;
}

/// \brief Stores single-precision floating point values from a 256-bit vector
///    of [8 x float] to an unaligned memory location pointed to by \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVUPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __a
///    A 256-bit vector of [8 x float] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu_ps(float *__p, __m256 __a)
{
  struct __storeu_ps {
    __m256 __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}

/// \brief Stores integer values from a 256-bit integer vector to a 32-byte
///    aligned memory location pointed to by \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQA </c> instruction.
///
/// \param __p
///    A 32-byte aligned pointer to a memory location that will receive the
///    integer values.
/// \param __a
///    A 256-bit integer vector containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_store_si256(__m256i *__p, __m256i __a)
{
  *__p = __a;
}

/// \brief Stores integer values from a 256-bit integer vector to an unaligned
///    memory location pointed to by \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDQU </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the integer values.
/// \param __a
///    A 256-bit integer vector containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu_si256(__m256i *__p, __m256i __a)
{
  struct __storeu_si256 {
    __m256i __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si256*)__p)->__v = __a;
}


/// \brief Conditionally loads double-precision floating point elements from a
///    memory location pointed to by \a __p into a 128-bit vector of
///    [2 x double], depending on the mask bits associated with each data
///    element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location that contains the double-precision
///    floating point values.
/// \param __m
///    A 128-bit integer vector containing the mask. The most significant bit of
///    each data element represents the mask bits. If a mask bit is zero, the
///    corresponding value in the memory location is not loaded and the
///    corresponding field in the return value is set to zero.
/// \returns A 128-bit vector of [2 x double] containing the loaded values.
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_maskload_pd(double __const *__p, __m128i __m)
{
  return (__m128d)__builtin_ia32_maskloadpd((__const __v2df *)__p, (__v2di)__m);
}

/// \brief Conditionally loads double-precision floating point elements from a
///    memory location pointed to by \a __p into a 256-bit vector of
///    [4 x double], depending on the mask bits associated with each data
///    element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location that contains the double-precision
///    floating point values.
/// \param __m
///    A 256-bit integer vector of [4 x quadword] containing the mask. The most
///    significant bit of each quadword element represents the mask bits. If a
///    mask bit is zero, the corresponding value in the memory location is not
///    loaded and the corresponding field in the return value is set to zero.
/// \returns A 256-bit vector of [4 x double] containing the loaded values.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_maskload_pd(double __const *__p, __m256i __m)
{
  return (__m256d)__builtin_ia32_maskloadpd256((__const __v4df *)__p,
                                               (__v4di)__m);
}

/// \brief Conditionally loads single-precision floating point elements from a
///    memory location pointed to by \a __p into a 128-bit vector of
///    [4 x float], depending on the mask bits associated with each data
///    element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that contains the single-precision
///    floating point values.
/// \param __m
///    A 128-bit integer vector containing the mask. The most significant bit of
///    each data element represents the mask bits. If a mask bit is zero, the
///    corresponding value in the memory location is not loaded and the
///    corresponding field in the return value is set to zero.
/// \returns A 128-bit vector of [4 x float] containing the loaded values.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_maskload_ps(float __const *__p, __m128i __m)
{
  return (__m128)__builtin_ia32_maskloadps((__const __v4sf *)__p, (__v4si)__m);
}

/// \brief Conditionally loads single-precision floating point elements from a
///    memory location pointed to by \a __p into a 256-bit vector of
///    [8 x float], depending on the mask bits associated with each data
///    element.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that contains the single-precision
///    floating point values.
/// \param __m
///    A 256-bit integer vector of [8 x dword] containing the mask. The most
///    significant bit of each dword element represents the mask bits. If a mask
///    bit is zero, the corresponding value in the memory location is not loaded
///    and the corresponding field in the return value is set to zero.
/// \returns A 256-bit vector of [8 x float] containing the loaded values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_maskload_ps(float __const *__p, __m256i __m)
{
  return (__m256)__builtin_ia32_maskloadps256((__const __v8sf *)__p, (__v8si)__m);
}


/// \brief Moves single-precision floating point values from a 256-bit vector
///    of [8 x float] to a memory location pointed to by \a __p, according to
///    the specified mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __m
///    A 256-bit integer vector of [8 x dword] containing the mask. The most
///    significant bit of each dword element in the mask vector represents the
///    mask bits. If a mask bit is zero, the corresponding value from vector
///    \a __a is not stored and the corresponding field in the memory location
///    pointed to by \a __p is not changed.
/// \param __a
///    A 256-bit vector of [8 x float] containing the values to be stored.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_maskstore_ps(float *__p, __m256i __m, __m256 __a)
{
  __builtin_ia32_maskstoreps256((__v8sf *)__p, (__v8si)__m, (__v8sf)__a);
}

/// \brief Moves double-precision values from a 128-bit vector of [2 x double]
///    to a memory location pointed to by \a __p, according to the specified
///    mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __m
///    A 128-bit integer vector containing the mask. The most significant bit of
///    each field in the mask vector represents the mask bits. If a mask bit is
///    zero, the corresponding value from vector \a __a is not stored and the
///    corresponding field in the memory location pointed to by \a __p is not
///    changed.
/// \param __a
///    A 128-bit vector of [2 x double] containing the values to be stored.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_maskstore_pd(double *__p, __m128i __m, __m128d __a)
{
  __builtin_ia32_maskstorepd((__v2df *)__p, (__v2di)__m, (__v2df)__a);
}

/// \brief Moves double-precision values from a 256-bit vector of [4 x double]
///    to a memory location pointed to by \a __p, according to the specified
///    mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPD </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __m
///    A 256-bit integer vector of [4 x quadword] containing the mask. The most
///    significant bit of each quadword element in the mask vector represents
///    the mask bits. If a mask bit is zero, the corresponding value from vector
///    __a is not stored and the corresponding field in the memory location
///    pointed to by \a __p is not changed.
/// \param __a
///    A 256-bit vector of [4 x double] containing the values to be stored.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_maskstore_pd(double *__p, __m256i __m, __m256d __a)
{
  __builtin_ia32_maskstorepd256((__v4df *)__p, (__v4di)__m, (__v4df)__a);
}

/// \brief Moves single-precision floating point values from a 128-bit vector
///    of [4 x float] to a memory location pointed to by \a __p, according to
///    the specified mask.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMASKMOVPS </c> instruction.
///
/// \param __p
///    A pointer to a memory location that will receive the float values.
/// \param __m
///    A 128-bit integer vector containing the mask. The most significant bit of
///    each field in the mask vector represents the mask bits. If a mask bit is
///    zero, the corresponding value from vector __a is not stored and the
///    corresponding field in the memory location pointed to by \a __p is not
///    changed.
/// \param __a
///    A 128-bit vector of [4 x float] containing the values to be stored.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm_maskstore_ps(float *__p, __m128i __m, __m128 __a)
{
  __builtin_ia32_maskstoreps((__v4sf *)__p, (__v4si)__m, (__v4sf)__a);
}


/// \brief Moves integer data from a 256-bit integer vector to a 32-byte
///    aligned memory location. To minimize caching, the data is flagged as
///    non-temporal (unlikely to be used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTDQ </c> instruction.
///
/// \param __a
///    A pointer to a 32-byte aligned memory location that will receive the
///    integer values.
/// \param __b
///    A 256-bit integer vector containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_stream_si256(__m256i *__a, __m256i __b)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4di_aligned)__b, (__v4di_aligned*)__a);
}

/// \brief Moves double-precision values from a 256-bit vector of [4 x double]
///    to a 32-byte aligned memory location. To minimize caching, the data is
///    flagged as non-temporal (unlikely to be used again soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTPD </c> instruction.
///
/// \param __a
///    A pointer to a 32-byte aligned memory location that will receive the
///    double-precision floating-point values.
/// \param __b
///    A 256-bit vector of [4 x double] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_stream_pd(double *__a, __m256d __b)
{
  typedef __v4df __v4df_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4df_aligned)__b, (__v4df_aligned*)__a);
}

/// \brief Moves single-precision floating point values from a 256-bit vector
///    of [8 x float] to a 32-byte aligned memory location. To minimize
///    caching, the data is flagged as non-temporal (unlikely to be used again
///    soon).
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVNTPS </c> instruction.
///
/// \param __p
///    A pointer to a 32-byte aligned memory location that will receive the
///    single-precision floating point values.
/// \param __a
///    A 256-bit vector of [8 x float] containing the values to be moved.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_stream_ps(float *__p, __m256 __a)
{
  typedef __v8sf __v8sf_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v8sf_aligned)__a, (__v8sf_aligned*)__p);
}


/// \brief Create a 256-bit vector of [4 x double] with undefined values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 256-bit vector of [4 x double] containing undefined values.
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_undefined_pd(void)
{
  return (__m256d)__builtin_ia32_undef256();
}

/// \brief Create a 256-bit vector of [8 x float] with undefined values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 256-bit vector of [8 x float] containing undefined values.
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_undefined_ps(void)
{
  return (__m256)__builtin_ia32_undef256();
}

/// \brief Create a 256-bit integer vector with undefined values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \returns A 256-bit integer vector containing undefined values.
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_undefined_si256(void)
{
  return (__m256i)__builtin_ia32_undef256();
}

/// \brief Constructs a 256-bit floating-point vector of [4 x double]
///    initialized with the specified double-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD+VINSERTF128 </c>
///   instruction.
///
/// \param __a
///    A double-precision floating-point value used to initialize bits [255:192]
///    of the result.
/// \param __b
///    A double-precision floating-point value used to initialize bits [191:128]
///    of the result.
/// \param __c
///    A double-precision floating-point value used to initialize bits [127:64]
///    of the result.
/// \param __d
///    A double-precision floating-point value used to initialize bits [63:0]
///    of the result.
/// \returns An initialized 256-bit floating-point vector of [4 x double].
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_pd(double __a, double __b, double __c, double __d)
{
  return (__m256d){ __d, __c, __b, __a };
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] initialized
///    with the specified single-precision floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __a
///    A single-precision floating-point value used to initialize bits [255:224]
///    of the result.
/// \param __b
///    A single-precision floating-point value used to initialize bits [223:192]
///    of the result.
/// \param __c
///    A single-precision floating-point value used to initialize bits [191:160]
///    of the result.
/// \param __d
///    A single-precision floating-point value used to initialize bits [159:128]
///    of the result.
/// \param __e
///    A single-precision floating-point value used to initialize bits [127:96]
///    of the result.
/// \param __f
///    A single-precision floating-point value used to initialize bits [95:64]
///    of the result.
/// \param __g
///    A single-precision floating-point value used to initialize bits [63:32]
///    of the result.
/// \param __h
///    A single-precision floating-point value used to initialize bits [31:0]
///    of the result.
/// \returns An initialized 256-bit floating-point vector of [8 x float].
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_ps(float __a, float __b, float __c, float __d,
              float __e, float __f, float __g, float __h)
{
  return (__m256){ __h, __g, __f, __e, __d, __c, __b, __a };
}

/// \brief Constructs a 256-bit integer vector initialized with the specified
///    32-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __i0
///    A 32-bit integral value used to initialize bits [255:224] of the result.
/// \param __i1
///    A 32-bit integral value used to initialize bits [223:192] of the result.
/// \param __i2
///    A 32-bit integral value used to initialize bits [191:160] of the result.
/// \param __i3
///    A 32-bit integral value used to initialize bits [159:128] of the result.
/// \param __i4
///    A 32-bit integral value used to initialize bits [127:96] of the result.
/// \param __i5
///    A 32-bit integral value used to initialize bits [95:64] of the result.
/// \param __i6
///    A 32-bit integral value used to initialize bits [63:32] of the result.
/// \param __i7
///    A 32-bit integral value used to initialize bits [31:0] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_epi32(int __i0, int __i1, int __i2, int __i3,
                 int __i4, int __i5, int __i6, int __i7)
{
  return (__m256i)(__v8si){ __i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0 };
}

/// \brief Constructs a 256-bit integer vector initialized with the specified
///    16-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __w15
///    A 16-bit integral value used to initialize bits [255:240] of the result.
/// \param __w14
///    A 16-bit integral value used to initialize bits [239:224] of the result.
/// \param __w13
///    A 16-bit integral value used to initialize bits [223:208] of the result.
/// \param __w12
///    A 16-bit integral value used to initialize bits [207:192] of the result.
/// \param __w11
///    A 16-bit integral value used to initialize bits [191:176] of the result.
/// \param __w10
///    A 16-bit integral value used to initialize bits [175:160] of the result.
/// \param __w09
///    A 16-bit integral value used to initialize bits [159:144] of the result.
/// \param __w08
///    A 16-bit integral value used to initialize bits [143:128] of the result.
/// \param __w07
///    A 16-bit integral value used to initialize bits [127:112] of the result.
/// \param __w06
///    A 16-bit integral value used to initialize bits [111:96] of the result.
/// \param __w05
///    A 16-bit integral value used to initialize bits [95:80] of the result.
/// \param __w04
///    A 16-bit integral value used to initialize bits [79:64] of the result.
/// \param __w03
///    A 16-bit integral value used to initialize bits [63:48] of the result.
/// \param __w02
///    A 16-bit integral value used to initialize bits [47:32] of the result.
/// \param __w01
///    A 16-bit integral value used to initialize bits [31:16] of the result.
/// \param __w00
///    A 16-bit integral value used to initialize bits [15:0] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_epi16(short __w15, short __w14, short __w13, short __w12,
                 short __w11, short __w10, short __w09, short __w08,
                 short __w07, short __w06, short __w05, short __w04,
                 short __w03, short __w02, short __w01, short __w00)
{
  return (__m256i)(__v16hi){ __w00, __w01, __w02, __w03, __w04, __w05, __w06,
    __w07, __w08, __w09, __w10, __w11, __w12, __w13, __w14, __w15 };
}

/// \brief Constructs a 256-bit integer vector initialized with the specified
///    8-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __b31
///    An 8-bit integral value used to initialize bits [255:248] of the result.
/// \param __b30
///    An 8-bit integral value used to initialize bits [247:240] of the result.
/// \param __b29
///    An 8-bit integral value used to initialize bits [239:232] of the result.
/// \param __b28
///    An 8-bit integral value used to initialize bits [231:224] of the result.
/// \param __b27
///    An 8-bit integral value used to initialize bits [223:216] of the result.
/// \param __b26
///    An 8-bit integral value used to initialize bits [215:208] of the result.
/// \param __b25
///    An 8-bit integral value used to initialize bits [207:200] of the result.
/// \param __b24
///    An 8-bit integral value used to initialize bits [199:192] of the result.
/// \param __b23
///    An 8-bit integral value used to initialize bits [191:184] of the result.
/// \param __b22
///    An 8-bit integral value used to initialize bits [183:176] of the result.
/// \param __b21
///    An 8-bit integral value used to initialize bits [175:168] of the result.
/// \param __b20
///    An 8-bit integral value used to initialize bits [167:160] of the result.
/// \param __b19
///    An 8-bit integral value used to initialize bits [159:152] of the result.
/// \param __b18
///    An 8-bit integral value used to initialize bits [151:144] of the result.
/// \param __b17
///    An 8-bit integral value used to initialize bits [143:136] of the result.
/// \param __b16
///    An 8-bit integral value used to initialize bits [135:128] of the result.
/// \param __b15
///    An 8-bit integral value used to initialize bits [127:120] of the result.
/// \param __b14
///    An 8-bit integral value used to initialize bits [119:112] of the result.
/// \param __b13
///    An 8-bit integral value used to initialize bits [111:104] of the result.
/// \param __b12
///    An 8-bit integral value used to initialize bits [103:96] of the result.
/// \param __b11
///    An 8-bit integral value used to initialize bits [95:88] of the result.
/// \param __b10
///    An 8-bit integral value used to initialize bits [87:80] of the result.
/// \param __b09
///    An 8-bit integral value used to initialize bits [79:72] of the result.
/// \param __b08
///    An 8-bit integral value used to initialize bits [71:64] of the result.
/// \param __b07
///    An 8-bit integral value used to initialize bits [63:56] of the result.
/// \param __b06
///    An 8-bit integral value used to initialize bits [55:48] of the result.
/// \param __b05
///    An 8-bit integral value used to initialize bits [47:40] of the result.
/// \param __b04
///    An 8-bit integral value used to initialize bits [39:32] of the result.
/// \param __b03
///    An 8-bit integral value used to initialize bits [31:24] of the result.
/// \param __b02
///    An 8-bit integral value used to initialize bits [23:16] of the result.
/// \param __b01
///    An 8-bit integral value used to initialize bits [15:8] of the result.
/// \param __b00
///    An 8-bit integral value used to initialize bits [7:0] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_epi8(char __b31, char __b30, char __b29, char __b28,
                char __b27, char __b26, char __b25, char __b24,
                char __b23, char __b22, char __b21, char __b20,
                char __b19, char __b18, char __b17, char __b16,
                char __b15, char __b14, char __b13, char __b12,
                char __b11, char __b10, char __b09, char __b08,
                char __b07, char __b06, char __b05, char __b04,
                char __b03, char __b02, char __b01, char __b00)
{
  return (__m256i)(__v32qi){
    __b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
    __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
    __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
    __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31
  };
}

/// \brief Constructs a 256-bit integer vector initialized with the specified
///    64-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLQDQ+VINSERTF128 </c>
///   instruction.
///
/// \param __a
///    A 64-bit integral value used to initialize bits [255:192] of the result.
/// \param __b
///    A 64-bit integral value used to initialize bits [191:128] of the result.
/// \param __c
///    A 64-bit integral value used to initialize bits [127:64] of the result.
/// \param __d
///    A 64-bit integral value used to initialize bits [63:0] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return (__m256i)(__v4di){ __d, __c, __b, __a };
}


/// \brief Constructs a 256-bit floating-point vector of [4 x double],
///    initialized in reverse order with the specified double-precision
///    floating-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VUNPCKLPD+VINSERTF128 </c>
///   instruction.
///
/// \param __a
///    A double-precision floating-point value used to initialize bits [63:0]
///    of the result.
/// \param __b
///    A double-precision floating-point value used to initialize bits [127:64]
///    of the result.
/// \param __c
///    A double-precision floating-point value used to initialize bits [191:128]
///    of the result.
/// \param __d
///    A double-precision floating-point value used to initialize bits [255:192]
///    of the result.
/// \returns An initialized 256-bit floating-point vector of [4 x double].
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_pd(double __a, double __b, double __c, double __d)
{
  return (__m256d){ __a, __b, __c, __d };
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float],
///    initialized in reverse order with the specified single-precision
///    float-point values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __a
///    A single-precision floating-point value used to initialize bits [31:0]
///    of the result.
/// \param __b
///    A single-precision floating-point value used to initialize bits [63:32]
///    of the result.
/// \param __c
///    A single-precision floating-point value used to initialize bits [95:64]
///    of the result.
/// \param __d
///    A single-precision floating-point value used to initialize bits [127:96]
///    of the result.
/// \param __e
///    A single-precision floating-point value used to initialize bits [159:128]
///    of the result.
/// \param __f
///    A single-precision floating-point value used to initialize bits [191:160]
///    of the result.
/// \param __g
///    A single-precision floating-point value used to initialize bits [223:192]
///    of the result.
/// \param __h
///    A single-precision floating-point value used to initialize bits [255:224]
///    of the result.
/// \returns An initialized 256-bit floating-point vector of [8 x float].
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_ps(float __a, float __b, float __c, float __d,
               float __e, float __f, float __g, float __h)
{
  return (__m256){ __a, __b, __c, __d, __e, __f, __g, __h };
}

/// \brief Constructs a 256-bit integer vector, initialized in reverse order
///    with the specified 32-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __i0
///    A 32-bit integral value used to initialize bits [31:0] of the result.
/// \param __i1
///    A 32-bit integral value used to initialize bits [63:32] of the result.
/// \param __i2
///    A 32-bit integral value used to initialize bits [95:64] of the result.
/// \param __i3
///    A 32-bit integral value used to initialize bits [127:96] of the result.
/// \param __i4
///    A 32-bit integral value used to initialize bits [159:128] of the result.
/// \param __i5
///    A 32-bit integral value used to initialize bits [191:160] of the result.
/// \param __i6
///    A 32-bit integral value used to initialize bits [223:192] of the result.
/// \param __i7
///    A 32-bit integral value used to initialize bits [255:224] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_epi32(int __i0, int __i1, int __i2, int __i3,
                  int __i4, int __i5, int __i6, int __i7)
{
  return (__m256i)(__v8si){ __i0, __i1, __i2, __i3, __i4, __i5, __i6, __i7 };
}

/// \brief Constructs a 256-bit integer vector, initialized in reverse order
///    with the specified 16-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __w15
///    A 16-bit integral value used to initialize bits [15:0] of the result.
/// \param __w14
///    A 16-bit integral value used to initialize bits [31:16] of the result.
/// \param __w13
///    A 16-bit integral value used to initialize bits [47:32] of the result.
/// \param __w12
///    A 16-bit integral value used to initialize bits [63:48] of the result.
/// \param __w11
///    A 16-bit integral value used to initialize bits [79:64] of the result.
/// \param __w10
///    A 16-bit integral value used to initialize bits [95:80] of the result.
/// \param __w09
///    A 16-bit integral value used to initialize bits [111:96] of the result.
/// \param __w08
///    A 16-bit integral value used to initialize bits [127:112] of the result.
/// \param __w07
///    A 16-bit integral value used to initialize bits [143:128] of the result.
/// \param __w06
///    A 16-bit integral value used to initialize bits [159:144] of the result.
/// \param __w05
///    A 16-bit integral value used to initialize bits [175:160] of the result.
/// \param __w04
///    A 16-bit integral value used to initialize bits [191:176] of the result.
/// \param __w03
///    A 16-bit integral value used to initialize bits [207:192] of the result.
/// \param __w02
///    A 16-bit integral value used to initialize bits [223:208] of the result.
/// \param __w01
///    A 16-bit integral value used to initialize bits [239:224] of the result.
/// \param __w00
///    A 16-bit integral value used to initialize bits [255:240] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_epi16(short __w15, short __w14, short __w13, short __w12,
       short __w11, short __w10, short __w09, short __w08,
       short __w07, short __w06, short __w05, short __w04,
       short __w03, short __w02, short __w01, short __w00)
{
  return (__m256i)(__v16hi){ __w15, __w14, __w13, __w12, __w11, __w10, __w09,
    __w08, __w07, __w06, __w05, __w04, __w03, __w02, __w01, __w00 };
}

/// \brief Constructs a 256-bit integer vector, initialized in reverse order
///    with the specified 8-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic is a utility function and does not correspond to a specific
///   instruction.
///
/// \param __b31
///    An 8-bit integral value used to initialize bits [7:0] of the result.
/// \param __b30
///    An 8-bit integral value used to initialize bits [15:8] of the result.
/// \param __b29
///    An 8-bit integral value used to initialize bits [23:16] of the result.
/// \param __b28
///    An 8-bit integral value used to initialize bits [31:24] of the result.
/// \param __b27
///    An 8-bit integral value used to initialize bits [39:32] of the result.
/// \param __b26
///    An 8-bit integral value used to initialize bits [47:40] of the result.
/// \param __b25
///    An 8-bit integral value used to initialize bits [55:48] of the result.
/// \param __b24
///    An 8-bit integral value used to initialize bits [63:56] of the result.
/// \param __b23
///    An 8-bit integral value used to initialize bits [71:64] of the result.
/// \param __b22
///    An 8-bit integral value used to initialize bits [79:72] of the result.
/// \param __b21
///    An 8-bit integral value used to initialize bits [87:80] of the result.
/// \param __b20
///    An 8-bit integral value used to initialize bits [95:88] of the result.
/// \param __b19
///    An 8-bit integral value used to initialize bits [103:96] of the result.
/// \param __b18
///    An 8-bit integral value used to initialize bits [111:104] of the result.
/// \param __b17
///    An 8-bit integral value used to initialize bits [119:112] of the result.
/// \param __b16
///    An 8-bit integral value used to initialize bits [127:120] of the result.
/// \param __b15
///    An 8-bit integral value used to initialize bits [135:128] of the result.
/// \param __b14
///    An 8-bit integral value used to initialize bits [143:136] of the result.
/// \param __b13
///    An 8-bit integral value used to initialize bits [151:144] of the result.
/// \param __b12
///    An 8-bit integral value used to initialize bits [159:152] of the result.
/// \param __b11
///    An 8-bit integral value used to initialize bits [167:160] of the result.
/// \param __b10
///    An 8-bit integral value used to initialize bits [175:168] of the result.
/// \param __b09
///    An 8-bit integral value used to initialize bits [183:176] of the result.
/// \param __b08
///    An 8-bit integral value used to initialize bits [191:184] of the result.
/// \param __b07
///    An 8-bit integral value used to initialize bits [199:192] of the result.
/// \param __b06
///    An 8-bit integral value used to initialize bits [207:200] of the result.
/// \param __b05
///    An 8-bit integral value used to initialize bits [215:208] of the result.
/// \param __b04
///    An 8-bit integral value used to initialize bits [223:216] of the result.
/// \param __b03
///    An 8-bit integral value used to initialize bits [231:224] of the result.
/// \param __b02
///    An 8-bit integral value used to initialize bits [239:232] of the result.
/// \param __b01
///    An 8-bit integral value used to initialize bits [247:240] of the result.
/// \param __b00
///    An 8-bit integral value used to initialize bits [255:248] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_epi8(char __b31, char __b30, char __b29, char __b28,
                 char __b27, char __b26, char __b25, char __b24,
                 char __b23, char __b22, char __b21, char __b20,
                 char __b19, char __b18, char __b17, char __b16,
                 char __b15, char __b14, char __b13, char __b12,
                 char __b11, char __b10, char __b09, char __b08,
                 char __b07, char __b06, char __b05, char __b04,
                 char __b03, char __b02, char __b01, char __b00)
{
  return (__m256i)(__v32qi){
    __b31, __b30, __b29, __b28, __b27, __b26, __b25, __b24,
    __b23, __b22, __b21, __b20, __b19, __b18, __b17, __b16,
    __b15, __b14, __b13, __b12, __b11, __b10, __b09, __b08,
    __b07, __b06, __b05, __b04, __b03, __b02, __b01, __b00 };
}

/// \brief Constructs a 256-bit integer vector, initialized in reverse order
///    with the specified 64-bit integral values.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPUNPCKLQDQ+VINSERTF128 </c>
///   instruction.
///
/// \param __a
///    A 64-bit integral value used to initialize bits [63:0] of the result.
/// \param __b
///    A 64-bit integral value used to initialize bits [127:64] of the result.
/// \param __c
///    A 64-bit integral value used to initialize bits [191:128] of the result.
/// \param __d
///    A 64-bit integral value used to initialize bits [255:192] of the result.
/// \returns An initialized 256-bit integer vector.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return (__m256i)(__v4di){ __a, __b, __c, __d };
}


/// \brief Constructs a 256-bit floating-point vector of [4 x double], with each
///    of the four double-precision floating-point vector elements set to the
///    specified double-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP+VINSERTF128 </c> instruction.
///
/// \param __w
///    A double-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 256-bit floating-point vector of [4 x double].
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_pd(double __w)
{
  return (__m256d){ __w, __w, __w, __w };
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float], with each
///    of the eight single-precision floating-point vector elements set to the
///    specified single-precision floating-point value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS+VINSERTF128 </c>
///   instruction.
///
/// \param __w
///    A single-precision floating-point value used to initialize each vector
///    element of the result.
/// \returns An initialized 256-bit floating-point vector of [8 x float].
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_ps(float __w)
{
  return (__m256){ __w, __w, __w, __w, __w, __w, __w, __w };
}

/// \brief Constructs a 256-bit integer vector of [8 x i32], with each of the
///    32-bit integral vector elements set to the specified 32-bit integral
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPERMILPS+VINSERTF128 </c>
///   instruction.
///
/// \param __i
///    A 32-bit integral value used to initialize each vector element of the
///    result.
/// \returns An initialized 256-bit integer vector of [8 x i32].
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_epi32(int __i)
{
  return (__m256i)(__v8si){ __i, __i, __i, __i, __i, __i, __i, __i };
}

/// \brief Constructs a 256-bit integer vector of [16 x i16], with each of the
///    16-bit integral vector elements set to the specified 16-bit integral
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSHUFB+VINSERTF128 </c> instruction.
///
/// \param __w
///    A 16-bit integral value used to initialize each vector element of the
///    result.
/// \returns An initialized 256-bit integer vector of [16 x i16].
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_epi16(short __w)
{
  return (__m256i)(__v16hi){ __w, __w, __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w };
}

/// \brief Constructs a 256-bit integer vector of [32 x i8], with each of the
///    8-bit integral vector elements set to the specified 8-bit integral value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VPSHUFB+VINSERTF128 </c> instruction.
///
/// \param __b
///    An 8-bit integral value used to initialize each vector element of the
///    result.
/// \returns An initialized 256-bit integer vector of [32 x i8].
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_epi8(char __b)
{
  return (__m256i)(__v32qi){ __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
    __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
    __b, __b, __b, __b, __b, __b, __b };
}

/// \brief Constructs a 256-bit integer vector of [4 x i64], with each of the
///    64-bit integral vector elements set to the specified 64-bit integral
///    value.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VMOVDDUP+VINSERTF128 </c> instruction.
///
/// \param __q
///    A 64-bit integral value used to initialize each vector element of the
///    result.
/// \returns An initialized 256-bit integer vector of [4 x i64].
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set1_epi64x(long long __q)
{
  return (__m256i)(__v4di){ __q, __q, __q, __q };
}


/// \brief Constructs a 256-bit floating-point vector of [4 x double] with all
///    vector elements initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS </c> instruction.
///
/// \returns A 256-bit vector of [4 x double] with all elements set to zero.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setzero_pd(void)
{
  return (__m256d){ 0, 0, 0, 0 };
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] with all
///    vector elements initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS </c> instruction.
///
/// \returns A 256-bit vector of [8 x float] with all elements set to zero.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setzero_ps(void)
{
  return (__m256){ 0, 0, 0, 0, 0, 0, 0, 0 };
}

/// \brief Constructs a 256-bit integer vector initialized to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VXORPS </c> instruction.
///
/// \returns A 256-bit integer vector initialized to zero.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setzero_si256(void)
{
  return (__m256i){ 0LL, 0LL, 0LL, 0LL };
}


/// \brief Casts a 256-bit floating-point vector of [4 x double] into a 256-bit
///    floating-point vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [4 x double].
/// \returns A 256-bit floating-point vector of [8 x float] containing the same
///    bitwise pattern as the parameter.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castpd_ps(__m256d __a)
{
  return (__m256)__a;
}

/// \brief Casts a 256-bit floating-point vector of [4 x double] into a 256-bit
///    integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [4 x double].
/// \returns A 256-bit integer vector containing the same bitwise pattern as the
///    parameter.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castpd_si256(__m256d __a)
{
  return (__m256i)__a;
}

/// \brief Casts a 256-bit floating-point vector of [8 x float] into a 256-bit
///    floating-point vector of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [8 x float].
/// \returns A 256-bit floating-point vector of [4 x double] containing the same
///    bitwise pattern as the parameter.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castps_pd(__m256 __a)
{
  return (__m256d)__a;
}

/// \brief Casts a 256-bit floating-point vector of [8 x float] into a 256-bit
///    integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [8 x float].
/// \returns A 256-bit integer vector containing the same bitwise pattern as the
///    parameter.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castps_si256(__m256 __a)
{
  return (__m256i)__a;
}

/// \brief Casts a 256-bit integer vector into a 256-bit floating-point vector
///    of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \returns A 256-bit floating-point vector of [8 x float] containing the same
///    bitwise pattern as the parameter.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castsi256_ps(__m256i __a)
{
  return (__m256)__a;
}

/// \brief Casts a 256-bit integer vector into a 256-bit floating-point vector
///    of [4 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \returns A 256-bit floating-point vector of [4 x double] containing the same
///    bitwise pattern as the parameter.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castsi256_pd(__m256i __a)
{
  return (__m256d)__a;
}

/// \brief Returns the lower 128 bits of a 256-bit floating-point vector of
///    [4 x double] as a 128-bit floating-point vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [4 x double].
/// \returns A 128-bit floating-point vector of [2 x double] containing the
///    lower 128 bits of the parameter.
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castpd256_pd128(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 1);
}

/// \brief Returns the lower 128 bits of a 256-bit floating-point vector of
///    [8 x float] as a 128-bit floating-point vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit floating-point vector of [8 x float].
/// \returns A 128-bit floating-point vector of [4 x float] containing the
///    lower 128 bits of the parameter.
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castps256_ps128(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 1, 2, 3);
}

/// \brief Truncates a 256-bit integer vector into a 128-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \returns A 128-bit integer vector containing the lower 128 bits of the
///    parameter.
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castsi256_si128(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)__a, 0, 1);
}

/// \brief Constructs a 256-bit floating-point vector of [4 x double] from a
///    128-bit floating-point vector of [2 x double].
///
///    The lower 128 bits contain the value of the source vector. The contents
///    of the upper 128 bits are undefined.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 256-bit floating-point vector of [4 x double]. The lower 128 bits
///    contain the value of the parameter. The contents of the upper 128 bits
///    are undefined.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 1, -1, -1);
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] from a
///    128-bit floating-point vector of [4 x float].
///
///    The lower 128 bits contain the value of the source vector. The contents
///    of the upper 128 bits are undefined.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 256-bit floating-point vector of [8 x float]. The lower 128 bits
///    contain the value of the parameter. The contents of the upper 128 bits
///    are undefined.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1, 2, 3, -1, -1, -1, -1);
}

/// \brief Constructs a 256-bit integer vector from a 128-bit integer vector.
///
///    The lower 128 bits contain the value of the source vector. The contents
///    of the upper 128 bits are undefined.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 256-bit integer vector. The lower 128 bits contain the value of
///    the parameter. The contents of the upper 128 bits are undefined.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_castsi128_si256(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)__a, 0, 1, -1, -1);
}

/// \brief Constructs a 256-bit floating-point vector of [4 x double] from a
///    128-bit floating-point vector of [2 x double]. The lower 128 bits
///    contain the value of the source vector. The upper 128 bits are set
///    to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 256-bit floating-point vector of [4 x double]. The lower 128 bits
///    contain the value of the parameter. The upper 128 bits are set to zero.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zextpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3);
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] from a
///    128-bit floating-point vector of [4 x float]. The lower 128 bits contain
///    the value of the source vector. The upper 128 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 256-bit floating-point vector of [8 x float]. The lower 128 bits
///    contain the value of the parameter. The upper 128 bits are set to zero.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zextps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7);
}

/// \brief Constructs a 256-bit integer vector from a 128-bit integer vector.
///    The lower 128 bits contain the value of the source vector. The upper
///    128 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 256-bit integer vector. The lower 128 bits contain the value of
///    the parameter. The upper 128 bits are set to zero.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zextsi128_si256(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3);
}






/// \brief Constructs a new 256-bit vector of [8 x float] by first duplicating
///    a 256-bit vector of [8 x float] given in the first parameter, and then
///    replacing either the upper or the lower 128 bits with the contents of a
///    128-bit vector of [4 x float] in the second parameter.
///
///    The immediate integer parameter determines between the upper or the lower
///    128 bits.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256 _mm256_insertf128_ps(__m256 V1, __m128 V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param V1
///    A 256-bit vector of [8 x float]. This vector is copied to the result
///    first, and then either the upper or the lower 128 bits of the result will
///    be replaced by the contents of \a V2.
/// \param V2
///    A 128-bit vector of [4 x float]. The contents of this parameter are
///    written to either the upper or the lower 128 bits of the result depending
///    on the value of parameter \a M.
/// \param M
///    An immediate integer. The least significant bit determines how the values
///    from the two parameters are interleaved: \n
///    If bit [0] of \a M is 0, \a V2 are copied to bits [127:0] of the result,
///    and bits [255:128] of \a V1 are copied to bits [255:128] of the
///    result. \n
///    If bit [0] of \a M is 1, \a V2 are copied to bits [255:128] of the
///    result, and bits [127:0] of \a V1 are copied to bits [127:0] of the
///    result.
/// \returns A 256-bit vector of [8 x float] containing the interleaved values.

# 4683 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avxintrin.h" 3 4

/// \brief Constructs a new 256-bit vector of [4 x double] by first duplicating
///    a 256-bit vector of [4 x double] given in the first parameter, and then
///    replacing either the upper or the lower 128 bits with the contents of a
///    128-bit vector of [2 x double] in the second parameter.
///
///    The immediate integer parameter determines between the upper or the lower
///    128 bits.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256d _mm256_insertf128_pd(__m256d V1, __m128d V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param V1
///    A 256-bit vector of [4 x double]. This vector is copied to the result
///    first, and then either the upper or the lower 128 bits of the result will
///    be replaced by the contents of \a V2.
/// \param V2
///    A 128-bit vector of [2 x double]. The contents of this parameter are
///    written to either the upper or the lower 128 bits of the result depending
///    on the value of parameter \a M.
/// \param M
///    An immediate integer. The least significant bit determines how the values
///    from the two parameters are interleaved: \n
///    If bit [0] of \a M is 0, \a V2 are copied to bits [127:0] of the result,
///    and bits [255:128] of \a V1 are copied to bits [255:128] of the
///    result. \n
///    If bit [0] of \a M is 1, \a V2 are copied to bits [255:128] of the
///    result, and bits [127:0] of \a V1 are copied to bits [127:0] of the
///    result.
/// \returns A 256-bit vector of [4 x double] containing the interleaved values.









/// \brief Constructs a new 256-bit integer vector by first duplicating a
///    256-bit integer vector given in the first parameter, and then replacing
///    either the upper or the lower 128 bits with the contents of a 128-bit
///    integer vector in the second parameter.
///
///    The immediate integer parameter determines between the upper or the lower
///    128 bits.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m256i _mm256_insertf128_si256(__m256i V1, __m128i V2, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param V1
///    A 256-bit integer vector. This vector is copied to the result first, and
///    then either the upper or the lower 128 bits of the result will be
///    replaced by the contents of \a V2.
/// \param V2
///    A 128-bit integer vector. The contents of this parameter are written to
///    either the upper or the lower 128 bits of the result depending on the
///     value of parameter \a M.
/// \param M
///    An immediate integer. The least significant bit determines how the values
///    from the two parameters are interleaved: \n
///    If bit [0] of \a M is 0, \a V2 are copied to bits [127:0] of the result,
///    and bits [255:128] of \a V1 are copied to bits [255:128] of the
///    result. \n
///    If bit [0] of \a M is 1, \a V2 are copied to bits [255:128] of the
///    result, and bits [127:0] of \a V1 are copied to bits [127:0] of the
///    result.
/// \returns A 256-bit integer vector containing the interleaved values.














/// \brief Extracts either the upper or the lower 128 bits from a 256-bit vector
///    of [8 x float], as determined by the immediate integer parameter, and
///    returns the extracted bits as a 128-bit vector of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128 _mm256_extractf128_ps(__m256 V, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.
///
/// \param V
///    A 256-bit vector of [8 x float].
/// \param M
///    An immediate integer. The least significant bit determines which bits are
///    extracted from the first parameter: \n
///    If bit [0] of \a M is 0, bits [127:0] of \a V are copied to the
///    result. \n
///    If bit [0] of \a M is 1, bits [255:128] of \a V are copied to the result.
/// \returns A 128-bit vector of [4 x float] containing the extracted bits.









/// \brief Extracts either the upper or the lower 128 bits from a 256-bit vector
///    of [4 x double], as determined by the immediate integer parameter, and
///    returns the extracted bits as a 128-bit vector of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128d _mm256_extractf128_pd(__m256d V, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.
///
/// \param V
///    A 256-bit vector of [4 x double].
/// \param M
///    An immediate integer. The least significant bit determines which bits are
///    extracted from the first parameter: \n
///    If bit [0] of \a M is 0, bits [127:0] of \a V are copied to the
///    result. \n
///    If bit [0] of \a M is 1, bits [255:128] of \a V are copied to the result.
/// \returns A 128-bit vector of [2 x double] containing the extracted bits.







/// \brief Extracts either the upper or the lower 128 bits from a 256-bit
///    integer vector, as determined by the immediate integer parameter, and
///    returns the extracted bits as a 128-bit integer vector.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm256_extractf128_si256(__m256i V, __const int M);
/// \endcode
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction.
///
/// \param V
///    A 256-bit integer vector.
/// \param M
///    An immediate integer. The least significant bit determines which bits are
///    extracted from the first parameter:  \n
///    If bit [0] of \a M is 0, bits [127:0] of \a V are copied to the
///    result. \n
///    If bit [0] of \a M is 1, bits [255:128] of \a V are copied to the result.
/// \returns A 128-bit integer vector containing the extracted bits.








/// \brief Loads two 128-bit floating-point vectors of [4 x float] from
///    unaligned memory locations and constructs a 256-bit floating-point vector
///    of [8 x float] by concatenating the two 128-bit vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to load instructions followed by the
///   <c> VINSERTF128 </c> instruction.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location containing 4 consecutive
///    single-precision floating-point values. These values are to be copied to
///    bits[255:128] of the result. The address of the memory location does not
///    have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location containing 4 consecutive
///    single-precision floating-point values. These values are to be copied to
///    bits[127:0] of the result. The address of the memory location does not
///    have to be aligned.
/// \returns A 256-bit floating-point vector of [8 x float] containing the
///    concatenated result.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu2_m128(float __const *__addr_hi, float __const *__addr_lo)
{
  __m256 __v256 = _mm256_castps128_ps256(_mm_loadu_ps(__addr_lo));
  return __extension__ ({ (__m256)__builtin_shufflevector( (__v8sf)(__m256)(__v256), (__v8sf)_mm256_castps128_ps256((__m128)( _mm_loadu_ps(__addr_hi))), ((( 1) & 1) ? 0 : 8), ((( 1) & 1) ? 1 : 9), ((( 1) & 1) ? 2 : 10), ((( 1) & 1) ? 3 : 11), ((( 1) & 1) ? 8 : 4), ((( 1) & 1) ? 9 : 5), ((( 1) & 1) ? 10 : 6), ((( 1) & 1) ? 11 : 7) );});
}

/// \brief Loads two 128-bit floating-point vectors of [2 x double] from
///    unaligned memory locations and constructs a 256-bit floating-point vector
///    of [4 x double] by concatenating the two 128-bit vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to load instructions followed by the
///   <c> VINSERTF128 </c> instruction.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location containing two consecutive
///    double-precision floating-point values. These values are to be copied to
///    bits[255:128] of the result. The address of the memory location does not
///    have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location containing two consecutive
///    double-precision floating-point values. These values are to be copied to
///    bits[127:0] of the result. The address of the memory location does not
///    have to be aligned.
/// \returns A 256-bit floating-point vector of [4 x double] containing the
///    concatenated result.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu2_m128d(double __const *__addr_hi, double __const *__addr_lo)
{
  __m256d __v256 = _mm256_castpd128_pd256(_mm_loadu_pd(__addr_lo));
  return __extension__ ({ (__m256d)__builtin_shufflevector( (__v4df)(__m256d)(__v256), (__v4df)_mm256_castpd128_pd256((__m128d)( _mm_loadu_pd(__addr_hi))), ((( 1) & 1) ? 0 : 4), ((( 1) & 1) ? 1 : 5), ((( 1) & 1) ? 4 : 2), ((( 1) & 1) ? 5 : 3) );});
}

/// \brief Loads two 128-bit integer vectors from unaligned memory locations and
///    constructs a 256-bit integer vector by concatenating the two 128-bit
///    vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to load instructions followed by the
///   <c> VINSERTF128 </c> instruction.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location containing a 128-bit integer
///    vector. This vector is to be copied to bits[255:128] of the result. The
///    address of the memory location does not have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location containing a 128-bit integer
///    vector. This vector is to be copied to bits[127:0] of the result. The
///    address of the memory location does not have to be aligned.
/// \returns A 256-bit integer vector containing the concatenated result.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_loadu2_m128i(__m128i __const *__addr_hi, __m128i __const *__addr_lo)
{
  __m256i __v256 = _mm256_castsi128_si256(_mm_loadu_si128(__addr_lo));
  return __extension__ ({ (__m256i)__builtin_shufflevector( (__v4di)(__m256i)(__v256), (__v4di)_mm256_castsi128_si256((__m128i)( _mm_loadu_si128(__addr_hi))), ((( 1) & 1) ? 0 : 4), ((( 1) & 1) ? 1 : 5), ((( 1) & 1) ? 4 : 2), ((( 1) & 1) ? 5 : 3) );});
}


/// \brief Stores the upper and lower 128 bits of a 256-bit floating-point
///    vector of [8 x float] into two different unaligned memory locations.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the
///   store instructions.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location. Bits[255:128] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location. Bits[127:0] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __a
///    A 256-bit floating-point vector of [8 x float].
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu2_m128(float *__addr_hi, float *__addr_lo, __m256 __a)
{
  __m128 __v128;

  __v128 = _mm256_castps256_ps128(__a);
  _mm_storeu_ps(__addr_lo, __v128);
  __v128 = __extension__ ({ (__m128)__builtin_shufflevector( (__v8sf)(__m256)(__a), (__v8sf)(_mm256_undefined_ps()), ((( 1) & 1) ? 4 : 0), ((( 1) & 1) ? 5 : 1), ((( 1) & 1) ? 6 : 2), ((( 1) & 1) ? 7 : 3) );});
  _mm_storeu_ps(__addr_hi, __v128);
}

/// \brief Stores the upper and lower 128 bits of a 256-bit floating-point
///    vector of [4 x double] into two different unaligned memory locations.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the
///   store instructions.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location. Bits[255:128] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location. Bits[127:0] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __a
///    A 256-bit floating-point vector of [4 x double].
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu2_m128d(double *__addr_hi, double *__addr_lo, __m256d __a)
{
  __m128d __v128;

  __v128 = _mm256_castpd256_pd128(__a);
  _mm_storeu_pd(__addr_lo, __v128);
  __v128 = __extension__ ({ (__m128d)__builtin_shufflevector( (__v4df)(__m256d)(__a), (__v4df)(_mm256_undefined_pd()), ((( 1) & 1) ? 2 : 0), ((( 1) & 1) ? 3 : 1) );});
  _mm_storeu_pd(__addr_hi, __v128);
}

/// \brief Stores the upper and lower 128 bits of a 256-bit integer vector into
///    two different unaligned memory locations.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VEXTRACTF128 </c> instruction and the
///   store instructions.
///
/// \param __addr_hi
///    A pointer to a 128-bit memory location. Bits[255:128] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __addr_lo
///    A pointer to a 128-bit memory location. Bits[127:0] of \a __a are to be
///    copied to this memory location. The address of this memory location does
///    not have to be aligned.
/// \param __a
///    A 256-bit integer vector.
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_storeu2_m128i(__m128i *__addr_hi, __m128i *__addr_lo, __m256i __a)
{
  __m128i __v128;

  __v128 = _mm256_castsi256_si128(__a);
  _mm_storeu_si128(__addr_lo, __v128);
  __v128 = __extension__ ({ (__m128i)__builtin_shufflevector( (__v4di)(__m256i)(__a), (__v4di)(_mm256_undefined_si256()), ((( 1) & 1) ? 2 : 0), ((( 1) & 1) ? 3 : 1) );});
  _mm_storeu_si128(__addr_hi, __v128);
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] by
///    concatenating two 128-bit floating-point vectors of [4 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __hi
///    A 128-bit floating-point vector of [4 x float] to be copied to the upper
///    128 bits of the result.
/// \param __lo
///    A 128-bit floating-point vector of [4 x float] to be copied to the lower
///    128 bits of the result.
/// \returns A 256-bit floating-point vector of [8 x float] containing the
///    concatenated result.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_m128 (__m128 __hi, __m128 __lo)
{
  return (__m256) __builtin_shufflevector((__v4sf)__lo, (__v4sf)__hi, 0, 1, 2, 3, 4, 5, 6, 7);
}

/// \brief Constructs a 256-bit floating-point vector of [4 x double] by
///    concatenating two 128-bit floating-point vectors of [2 x double].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __hi
///    A 128-bit floating-point vector of [2 x double] to be copied to the upper
///    128 bits of the result.
/// \param __lo
///    A 128-bit floating-point vector of [2 x double] to be copied to the lower
///    128 bits of the result.
/// \returns A 256-bit floating-point vector of [4 x double] containing the
///    concatenated result.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_m128d (__m128d __hi, __m128d __lo)
{
  return (__m256d)_mm256_set_m128((__m128)__hi, (__m128)__lo);
}

/// \brief Constructs a 256-bit integer vector by concatenating two 128-bit
///    integer vectors.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __hi
///    A 128-bit integer vector to be copied to the upper 128 bits of the
///    result.
/// \param __lo
///    A 128-bit integer vector to be copied to the lower 128 bits of the
///    result.
/// \returns A 256-bit integer vector containing the concatenated result.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_set_m128i (__m128i __hi, __m128i __lo)
{
  return (__m256i)_mm256_set_m128((__m128)__hi, (__m128)__lo);
}

/// \brief Constructs a 256-bit floating-point vector of [8 x float] by
///    concatenating two 128-bit floating-point vectors of [4 x float]. This is
///    similar to _mm256_set_m128, but the order of the input parameters is
///    swapped.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __lo
///    A 128-bit floating-point vector of [4 x float] to be copied to the lower
///    128 bits of the result.
/// \param __hi
///    A 128-bit floating-point vector of [4 x float] to be copied to the upper
///    128 bits of the result.
/// \returns A 256-bit floating-point vector of [8 x float] containing the
///    concatenated result.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_m128 (__m128 __lo, __m128 __hi)
{
  return _mm256_set_m128(__hi, __lo);
}

/// \brief Constructs a 256-bit floating-point vector of [4 x double] by
///    concatenating two 128-bit floating-point vectors of [2 x double]. This is
///    similar to _mm256_set_m128d, but the order of the input parameters is
///    swapped.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __lo
///    A 128-bit floating-point vector of [2 x double] to be copied to the lower
///    128 bits of the result.
/// \param __hi
///    A 128-bit floating-point vector of [2 x double] to be copied to the upper
///    128 bits of the result.
/// \returns A 256-bit floating-point vector of [4 x double] containing the
///    concatenated result.
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_m128d (__m128d __lo, __m128d __hi)
{
  return (__m256d)_mm256_set_m128((__m128)__hi, (__m128)__lo);
}

/// \brief Constructs a 256-bit integer vector by concatenating two 128-bit
///    integer vectors. This is similar to _mm256_set_m128i, but the order of
///    the input parameters is swapped.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VINSERTF128 </c> instruction.
///
/// \param __lo
///    A 128-bit integer vector to be copied to the lower 128 bits of the
///    result.
/// \param __hi
///    A 128-bit integer vector to be copied to the upper 128 bits of the
///    result.
/// \returns A 256-bit integer vector containing the concatenated result.
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_setr_m128i (__m128i __lo, __m128i __hi)
{
  return (__m256i)_mm256_set_m128((__m128)__hi, (__m128)__lo);
}




# 67 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4

















static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_abs_epi8(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsb256((__v32qi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_abs_epi16(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsw256((__v16hi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_abs_epi32(__m256i __a)
{
    return (__m256i)__builtin_ia32_pabsd256((__v8si)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_packs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packsswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_packs_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packssdw256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_packus_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packuswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_packus_epi32(__m256i __V1, __m256i __V2)
{
  return (__m256i) __builtin_ia32_packusdw256((__v8si)__V1, (__v8si)__V2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_add_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a + (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_add_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a + (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_add_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a + (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_add_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a + (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_adds_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_adds_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_adds_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddusb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_adds_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_paddusw256((__v16hi)__a, (__v16hi)__b);
}





static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_and_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_andnot_si256(__m256i __a, __m256i __b)
{
  return (__m256i)(~(__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_avg_epu8(__m256i __a, __m256i __b)
{
  typedef unsigned short __v32hu __attribute__((__vector_size__(64)));
  return (__m256i)__builtin_convertvector(
               ((__builtin_convertvector((__v32qu)__a, __v32hu) +
                 __builtin_convertvector((__v32qu)__b, __v32hu)) + 1)
                 >> 1, __v32qu);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_avg_epu16(__m256i __a, __m256i __b)
{
  typedef unsigned int __v16su __attribute__((__vector_size__(64)));
  return (__m256i)__builtin_convertvector(
               ((__builtin_convertvector((__v16hu)__a, __v16su) +
                 __builtin_convertvector((__v16hu)__b, __v16su)) + 1)
                 >> 1, __v16hu);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_blendv_epi8(__m256i __V1, __m256i __V2, __m256i __M)
{
  return (__m256i)__builtin_ia32_pblendvb256((__v32qi)__V1, (__v32qi)__V2,
                                              (__v32qi)__M);
}


# 191 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpeq_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a == (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpeq_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a == (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpeq_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a == (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpeq_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a == (__v4di)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpgt_epi8(__m256i __a, __m256i __b)
{
  

  return (__m256i)((__v32qs)__a > (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpgt_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a > (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpgt_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a > (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cmpgt_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a > (__v4di)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hadd_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hadd_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hadds_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hsub_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hsub_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_hsubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_maddubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_pmaddubsw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_madd_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaddwd256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxsd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxub256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxuw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_max_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaxud256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminsd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminub256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminuw256 ((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_min_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pminud256((__v8si)__a, (__v8si)__b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_movemask_epi8(__m256i __a)
{
  return __builtin_ia32_pmovmskb256((__v32qi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi8_epi16(__m128i __V)
{
  

  return (__m256i)__builtin_convertvector((__v16qs)__V, __v16hi);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi8_epi32(__m128i __V)
{
  

  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi8_epi64(__m128i __V)
{
  

  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hi)__V, __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepi32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4si)__V, __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu8_epi16(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v16qu)__V, __v16hi);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu8_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu8_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hu)__V, __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_cvtepu32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4su)__V, __v4di);
}

static __inline__  __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mul_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmuldq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mulhrs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhrsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mulhi_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhuw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mulhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mullo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a * (__v16hu)__b);
}

static __inline__  __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mullo_epi32 (__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a * (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_mul_epu32(__m256i __a, __m256i __b)
{
  return __builtin_ia32_pmuludq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_or_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a | (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sad_epu8(__m256i __a, __m256i __b)
{
  return __builtin_ia32_psadbw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_shuffle_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pshufb256((__v32qi)__a, (__v32qi)__b);
}


# 517 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4


# 531 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4


# 545 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sign_epi8(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sign_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sign_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignd256((__v8si)__a, (__v8si)__b);
}


# 600 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4



static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_slli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psllwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sll_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psllw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_slli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_pslldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sll_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_pslld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_slli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psllqi256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sll_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psllq256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srai_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrawi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sra_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psraw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srai_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psradi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sra_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrad256((__v8si)__a, (__v4si)__count);
}


# 699 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4



static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrlwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srl_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srl_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psrlqi256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srl_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psrlq256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sub_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a - (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sub_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a - (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sub_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a - (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sub_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a - (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_subs_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubsb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_subs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_subs_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubusb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_subs_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_psubusw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpackhi_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 8, 32+8, 9, 32+9, 10, 32+10, 11, 32+11, 12, 32+12, 13, 32+13, 14, 32+14, 15, 32+15, 24, 32+24, 25, 32+25, 26, 32+26, 27, 32+27, 28, 32+28, 29, 32+29, 30, 32+30, 31, 32+31);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpackhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpackhi_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 2, 8+2, 3, 8+3, 6, 8+6, 7, 8+7);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpackhi_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 1, 4+1, 3, 4+3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpacklo_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 0, 32+0, 1, 32+1, 2, 32+2, 3, 32+3, 4, 32+4, 5, 32+5, 6, 32+6, 7, 32+7, 16, 32+16, 17, 32+17, 18, 32+18, 19, 32+19, 20, 32+20, 21, 32+21, 22, 32+22, 23, 32+23);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpacklo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpacklo_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 0, 8+0, 1, 8+1, 4, 8+4, 5, 8+5);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_unpacklo_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 0, 4+0, 2, 4+2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_xor_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a ^ (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_stream_load_si256(__m256i __const *__V)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  return (__m256i)__builtin_nontemporal_load((__const __v4di_aligned *)__V);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastss_ps(__m128 __X)
{
  return (__m128)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastsd_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastss_ps(__m128 __X)
{
  return (__m256)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastsd_pd(__m128d __X)
{
  return (__m256d)__builtin_shufflevector((__v2df)__X, (__v2df)__X, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastsi128_si256(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 1, 0, 1);
}










# 896 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx2intrin.h" 3 4

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastb_epi8(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastw_epi16(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastd_epi32(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_broadcastq_epi64(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastb_epi8(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastw_epi16(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastd_epi32(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_broadcastq_epi64(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_permutevar8x32_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_permvarsi256((__v8si)__a, (__v8si)__b);
}









static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_permutevar8x32_ps(__m256 __a, __m256i __b)
{
  return (__m256)__builtin_ia32_permvarsf256((__v8sf)__a, (__v8si)__b);
}


























static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_maskload_epi32(int __const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadd256((__const __v8si *)__X, (__v8si)__M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_maskload_epi64(long long __const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadq256((__const __v4di *)__X, (__v4di)__M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_maskload_epi32(int __const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadd((__const __v4si *)__X, (__v4si)__M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_maskload_epi64(long long __const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadq((__const __v2di *)__X, (__v2di)__M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_maskstore_epi32(int *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstored256((__v8si *)__X, (__v8si)__M, (__v8si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_maskstore_epi64(long long *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstoreq256((__v4di *)__X, (__v4di)__M, (__v4di)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_maskstore_epi32(int *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstored((__v4si *)__X, (__v4si)__M, (__v4si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_maskstore_epi64(long long *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstoreq(( __v2di *)__X, (__v2di)__M, (__v2di)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sllv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_sllv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_sllv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv4di((__v4di)__X, (__v4di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_sllv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv2di((__v2di)__X, (__v2di)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srav_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrav8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_srav_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrav4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srlv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_srlv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm256_srlv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv4di((__v4di)__X, (__v4di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2")))
_mm_srlv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv2di((__v2di)__X, (__v2di)__Y);
}



















































































































































































































# 71 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





/// \brief Converts a 256-bit vector of [8 x float] into a 128-bit vector
///    containing 16-bit half-precision float values.
///
/// \headerfile <x86intrin.h>
///
/// \code
/// __m128i _mm256_cvtps_ph(__m256 a, __const int imm);
/// \endcode
///
/// This intrinsic corresponds to the <c> VCVTPS2PH </c> instruction.
///
/// \param a
///    A 256-bit vector containing 32-bit single-precision float values to be
///    converted to 16-bit half-precision float values.
/// \param imm
///    An immediate value controlling rounding using bits [2:0]: \n
///    000: Nearest \n
///    001: Down \n
///    010: Up \n
///    011: Truncate \n
///    1XX: Use MXCSR.RC for rounding
/// \returns A 128-bit vector containing the converted 16-bit half-precision
///    float values.



/// \brief Converts a 128-bit vector containing 16-bit half-precision float
///    values into a 256-bit vector of [8 x float].
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> VCVTPH2PS </c> instruction.
///
/// \param __a
///    A 128-bit vector containing 16-bit half-precision float values to be
///    converted to 32-bit single-precision float values.
/// \returns A vector of [8 x float] containing the converted 32-bit
///    single-precision float values.
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("f16c")))
_mm256_cvtph_ps(__m128i __a)
{
  return (__m256)__builtin_ia32_vcvtph2ps256((__v8hi)__a);
}




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/vpclmulqdqintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/vpclmulqdqintrin.h" 3 4





















# 123 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/bmiintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/bmiintrin.h" 3 4






























/// \brief Counts the number of trailing zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> TZCNT </c> instruction.
///
/// \param __X
///    An unsigned 16-bit integer whose trailing zeros are to be counted.
/// \returns An unsigned 16-bit integer containing the number of trailing zero
///    bits in the operand.
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__tzcnt_u16(unsigned short __X)
{
  return __X ? __builtin_ctzs(__X) : 16;
}

/// \brief Performs a bitwise AND of the second operand with the one's
///    complement of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> ANDN </c> instruction.
///
/// \param __X
///    An unsigned integer containing one of the operands.
/// \param __Y
///    An unsigned integer containing one of the operands.
/// \returns An unsigned integer containing the bitwise AND of the second
///    operand with the one's complement of the first operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u32(unsigned int __X, unsigned int __Y)
{
  return ~__X & __Y;
}


/// \brief Extracts the specified bits from the first operand and returns them
///    in the least significant bits of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BEXTR </c> instruction.
///
/// \param __X
///    An unsigned integer whose bits are to be extracted.
/// \param __Y
///    An unsigned integer used to specify which bits are extracted. Bits [7:0]
///    specify the index of the least significant bit. Bits [15:8] specify the
///    number of bits to be extracted.
/// \returns An unsigned integer whose least significant bits contain the
///    extracted bits.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bextr_u32(__X, __Y);
}


/// \brief Extracts the specified bits from the first operand and returns them
///    in the least significant bits of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BEXTR </c> instruction.
///
/// \param __X
///    An unsigned integer whose bits are to be extracted.
/// \param __Y
///    An unsigned integer used to specify the index of the least significant
///    bit for the bits to be extracted. Bits [7:0] specify the index.
/// \param __Z
///    An unsigned integer used to specify the number of bits to be extracted.
///    Bits [7:0] specify the number of bits.
/// \returns An unsigned integer whose least significant bits contain the
///    extracted bits.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u32(unsigned int __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

/// \brief Clears all bits in the source except for the least significant bit
///    containing a value of 1 and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSI </c> instruction.
///
/// \param __X
///    An unsigned integer whose bits are to be cleared.
/// \returns An unsigned integer containing the result of clearing the bits from
///    the source operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u32(unsigned int __X)
{
  return __X & -__X;
}

/// \brief Creates a mask whose bits are set to 1, using bit 0 up to and
///    including the least significant bit that is set to 1 in the source
///    operand and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSMSK </c> instruction.
///
/// \param __X
///    An unsigned integer used to create the mask.
/// \returns An unsigned integer containing the newly created mask.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u32(unsigned int __X)
{
  return __X ^ (__X - 1);
}

/// \brief Clears the least significant bit that is set to 1 in the source
///    operand and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSR </c> instruction.
///
/// \param __X
///    An unsigned integer containing the operand to be cleared.
/// \returns An unsigned integer containing the result of clearing the source
///    operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u32(unsigned int __X)
{
  return __X & (__X - 1);
}

/// \brief Counts the number of trailing zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> TZCNT </c> instruction.
///
/// \param __X
///    An unsigned 32-bit integer whose trailing zeros are to be counted.
/// \returns An unsigned 32-bit integer containing the number of trailing zero
///    bits in the operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__tzcnt_u32(unsigned int __X)
{
  return __X ? __builtin_ctz(__X) : 32;
}

/// \brief Counts the number of trailing zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> TZCNT </c> instruction.
///
/// \param __X
///    An unsigned 32-bit integer whose trailing zeros are to be counted.
/// \returns An 32-bit integer containing the number of trailing zero bits in
///    the operand.
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_32(unsigned int __X)
{
  return __X ? __builtin_ctz(__X) : 32;
}














/// \brief Performs a bitwise AND of the second operand with the one's
///    complement of the first operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> ANDN </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer containing one of the operands.
/// \param __Y
///    An unsigned 64-bit integer containing one of the operands.
/// \returns An unsigned 64-bit integer containing the bitwise AND of the second
///    operand with the one's complement of the first operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u64 (unsigned long long __X, unsigned long long __Y)
{
  return ~__X & __Y;
}


/// \brief Extracts the specified bits from the first operand and returns them
///    in the least significant bits of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BEXTR </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose bits are to be extracted.
/// \param __Y
///    An unsigned 64-bit integer used to specify which bits are extracted. Bits
///    [7:0] specify the index of the least significant bit. Bits [15:8] specify
///    the number of bits to be extracted.
/// \returns An unsigned 64-bit integer whose least significant bits contain the
///    extracted bits.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bextr_u64(__X, __Y);
}


/// \brief Extracts the specified bits from the first operand and returns them
///     in the least significant bits of the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BEXTR </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose bits are to be extracted.
/// \param __Y
///    An unsigned integer used to specify the index of the least significant
///    bit for the bits to be extracted. Bits [7:0] specify the index.
/// \param __Z
///    An unsigned integer used to specify the number of bits to be extracted.
///    Bits [7:0] specify the number of bits.
/// \returns An unsigned 64-bit integer whose least significant bits contain the
///    extracted bits.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u64(unsigned long long __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}

/// \brief Clears all bits in the source except for the least significant bit
///    containing a value of 1 and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSI </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose bits are to be cleared.
/// \returns An unsigned 64-bit integer containing the result of clearing the
///    bits from the source operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u64(unsigned long long __X)
{
  return __X & -__X;
}

/// \brief Creates a mask whose bits are set to 1, using bit 0 up to and
///    including the least significant bit that is set to 1 in the source
///    operand and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSMSK </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer used to create the mask.
/// \returns An unsigned 64-bit integer containing the newly created mask.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u64(unsigned long long __X)
{
  return __X ^ (__X - 1);
}

/// \brief Clears the least significant bit that is set to 1 in the source
///    operand and returns the result.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> BLSR </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer containing the operand to be cleared.
/// \returns An unsigned 64-bit integer containing the result of clearing the
///    source operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u64(unsigned long long __X)
{
  return __X & (__X - 1);
}

/// \brief Counts the number of trailing zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> TZCNT </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose trailing zeros are to be counted.
/// \returns An unsigned 64-bit integer containing the number of trailing zero
///    bits in the operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__tzcnt_u64(unsigned long long __X)
{
  return __X ? __builtin_ctzll(__X) : 64;
}

/// \brief Counts the number of trailing zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> TZCNT </c> instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose trailing zeros are to be counted.
/// \returns An 64-bit integer containing the number of trailing zero bits in
///    the operand.
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_64(unsigned long long __X)
{
  return __X ? __builtin_ctzll(__X) : 64;
}







# 127 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/bmi2intrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/bmi2intrin.h" 3 4












static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bzhi_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pdep_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pext_si(__X, __Y);
}



static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bzhi_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pdep_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pext_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_mulx_u64 (unsigned long long __X, unsigned long long __Y,
	   unsigned long long *__P)
{
  unsigned __int128 __res = (unsigned __int128) __X * __Y;
  *__P = (unsigned long long) (__res >> 64);
  return (unsigned long long) __res;
}


# 92 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/bmi2intrin.h" 3 4




# 131 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/lzcntintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/lzcntintrin.h" 3 4












/// \brief Counts the number of leading zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c LZCNT instruction.
///
/// \param __X
///    An unsigned 16-bit integer whose leading zeros are to be counted.
/// \returns An unsigned 16-bit integer containing the number of leading zero
///    bits in the operand.
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
__lzcnt16(unsigned short __X)
{
  return __X ? __builtin_clzs(__X) : 16;
}

/// \brief Counts the number of leading zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c LZCNT instruction.
///
/// \param __X
///    An unsigned 32-bit integer whose leading zeros are to be counted.
/// \returns An unsigned 32-bit integer containing the number of leading zero
///    bits in the operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
__lzcnt32(unsigned int __X)
{
  return __X ? __builtin_clz(__X) : 32;
}

/// \brief Counts the number of leading zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c LZCNT instruction.
///
/// \param __X
///    An unsigned 32-bit integer whose leading zeros are to be counted.
/// \returns An unsigned 32-bit integer containing the number of leading zero
///    bits in the operand.
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u32(unsigned int __X)
{
  return __X ? __builtin_clz(__X) : 32;
}


/// \brief Counts the number of leading zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c LZCNT instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose leading zeros are to be counted.
/// \returns An unsigned 64-bit integer containing the number of leading zero
///    bits in the operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
__lzcnt64(unsigned long long __X)
{
  return __X ? __builtin_clzll(__X) : 64;
}

/// \brief Counts the number of leading zero bits in the operand.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the \c LZCNT instruction.
///
/// \param __X
///    An unsigned 64-bit integer whose leading zeros are to be counted.
/// \returns An unsigned 64-bit integer containing the number of leading zero
///    bits in the operand.
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u64(unsigned long long __X)
{
  return __X ? __builtin_clzll(__X) : 64;
}





# 135 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/fmaintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/fmaintrin.h" 3 4












static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fnmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma")))
_mm256_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}




# 139 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4








typedef char __v64qi __attribute__((__vector_size__(64)));
typedef short __v32hi __attribute__((__vector_size__(64)));
typedef double __v8df __attribute__((__vector_size__(64)));
typedef float __v16sf __attribute__((__vector_size__(64)));
typedef long long __v8di __attribute__((__vector_size__(64)));
typedef int __v16si __attribute__((__vector_size__(64)));


typedef unsigned char __v64qu __attribute__((__vector_size__(64)));
typedef unsigned short __v32hu __attribute__((__vector_size__(64)));
typedef unsigned long long __v8du __attribute__((__vector_size__(64)));
typedef unsigned int __v16su __attribute__((__vector_size__(64)));

typedef float __m512 __attribute__((__vector_size__(64)));
typedef double __m512d __attribute__((__vector_size__(64)));
typedef long long __m512i __attribute__((__vector_size__(64)));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;









typedef enum {
    _MM_CMPINT_EQ,      
    _MM_CMPINT_LT,      
    _MM_CMPINT_LE,      
    _MM_CMPINT_UNUSED,
    _MM_CMPINT_NE,      
    _MM_CMPINT_NLT,     

    _MM_CMPINT_NLE      

} _MM_CMPINT_ENUM;

typedef enum
{
  _MM_PERM_AAAA = 0x00, _MM_PERM_AAAB = 0x01, _MM_PERM_AAAC = 0x02,
  _MM_PERM_AAAD = 0x03, _MM_PERM_AABA = 0x04, _MM_PERM_AABB = 0x05,
  _MM_PERM_AABC = 0x06, _MM_PERM_AABD = 0x07, _MM_PERM_AACA = 0x08,
  _MM_PERM_AACB = 0x09, _MM_PERM_AACC = 0x0A, _MM_PERM_AACD = 0x0B,
  _MM_PERM_AADA = 0x0C, _MM_PERM_AADB = 0x0D, _MM_PERM_AADC = 0x0E,
  _MM_PERM_AADD = 0x0F, _MM_PERM_ABAA = 0x10, _MM_PERM_ABAB = 0x11,
  _MM_PERM_ABAC = 0x12, _MM_PERM_ABAD = 0x13, _MM_PERM_ABBA = 0x14,
  _MM_PERM_ABBB = 0x15, _MM_PERM_ABBC = 0x16, _MM_PERM_ABBD = 0x17,
  _MM_PERM_ABCA = 0x18, _MM_PERM_ABCB = 0x19, _MM_PERM_ABCC = 0x1A,
  _MM_PERM_ABCD = 0x1B, _MM_PERM_ABDA = 0x1C, _MM_PERM_ABDB = 0x1D,
  _MM_PERM_ABDC = 0x1E, _MM_PERM_ABDD = 0x1F, _MM_PERM_ACAA = 0x20,
  _MM_PERM_ACAB = 0x21, _MM_PERM_ACAC = 0x22, _MM_PERM_ACAD = 0x23,
  _MM_PERM_ACBA = 0x24, _MM_PERM_ACBB = 0x25, _MM_PERM_ACBC = 0x26,
  _MM_PERM_ACBD = 0x27, _MM_PERM_ACCA = 0x28, _MM_PERM_ACCB = 0x29,
  _MM_PERM_ACCC = 0x2A, _MM_PERM_ACCD = 0x2B, _MM_PERM_ACDA = 0x2C,
  _MM_PERM_ACDB = 0x2D, _MM_PERM_ACDC = 0x2E, _MM_PERM_ACDD = 0x2F,
  _MM_PERM_ADAA = 0x30, _MM_PERM_ADAB = 0x31, _MM_PERM_ADAC = 0x32,
  _MM_PERM_ADAD = 0x33, _MM_PERM_ADBA = 0x34, _MM_PERM_ADBB = 0x35,
  _MM_PERM_ADBC = 0x36, _MM_PERM_ADBD = 0x37, _MM_PERM_ADCA = 0x38,
  _MM_PERM_ADCB = 0x39, _MM_PERM_ADCC = 0x3A, _MM_PERM_ADCD = 0x3B,
  _MM_PERM_ADDA = 0x3C, _MM_PERM_ADDB = 0x3D, _MM_PERM_ADDC = 0x3E,
  _MM_PERM_ADDD = 0x3F, _MM_PERM_BAAA = 0x40, _MM_PERM_BAAB = 0x41,
  _MM_PERM_BAAC = 0x42, _MM_PERM_BAAD = 0x43, _MM_PERM_BABA = 0x44,
  _MM_PERM_BABB = 0x45, _MM_PERM_BABC = 0x46, _MM_PERM_BABD = 0x47,
  _MM_PERM_BACA = 0x48, _MM_PERM_BACB = 0x49, _MM_PERM_BACC = 0x4A,
  _MM_PERM_BACD = 0x4B, _MM_PERM_BADA = 0x4C, _MM_PERM_BADB = 0x4D,
  _MM_PERM_BADC = 0x4E, _MM_PERM_BADD = 0x4F, _MM_PERM_BBAA = 0x50,
  _MM_PERM_BBAB = 0x51, _MM_PERM_BBAC = 0x52, _MM_PERM_BBAD = 0x53,
  _MM_PERM_BBBA = 0x54, _MM_PERM_BBBB = 0x55, _MM_PERM_BBBC = 0x56,
  _MM_PERM_BBBD = 0x57, _MM_PERM_BBCA = 0x58, _MM_PERM_BBCB = 0x59,
  _MM_PERM_BBCC = 0x5A, _MM_PERM_BBCD = 0x5B, _MM_PERM_BBDA = 0x5C,
  _MM_PERM_BBDB = 0x5D, _MM_PERM_BBDC = 0x5E, _MM_PERM_BBDD = 0x5F,
  _MM_PERM_BCAA = 0x60, _MM_PERM_BCAB = 0x61, _MM_PERM_BCAC = 0x62,
  _MM_PERM_BCAD = 0x63, _MM_PERM_BCBA = 0x64, _MM_PERM_BCBB = 0x65,
  _MM_PERM_BCBC = 0x66, _MM_PERM_BCBD = 0x67, _MM_PERM_BCCA = 0x68,
  _MM_PERM_BCCB = 0x69, _MM_PERM_BCCC = 0x6A, _MM_PERM_BCCD = 0x6B,
  _MM_PERM_BCDA = 0x6C, _MM_PERM_BCDB = 0x6D, _MM_PERM_BCDC = 0x6E,
  _MM_PERM_BCDD = 0x6F, _MM_PERM_BDAA = 0x70, _MM_PERM_BDAB = 0x71,
  _MM_PERM_BDAC = 0x72, _MM_PERM_BDAD = 0x73, _MM_PERM_BDBA = 0x74,
  _MM_PERM_BDBB = 0x75, _MM_PERM_BDBC = 0x76, _MM_PERM_BDBD = 0x77,
  _MM_PERM_BDCA = 0x78, _MM_PERM_BDCB = 0x79, _MM_PERM_BDCC = 0x7A,
  _MM_PERM_BDCD = 0x7B, _MM_PERM_BDDA = 0x7C, _MM_PERM_BDDB = 0x7D,
  _MM_PERM_BDDC = 0x7E, _MM_PERM_BDDD = 0x7F, _MM_PERM_CAAA = 0x80,
  _MM_PERM_CAAB = 0x81, _MM_PERM_CAAC = 0x82, _MM_PERM_CAAD = 0x83,
  _MM_PERM_CABA = 0x84, _MM_PERM_CABB = 0x85, _MM_PERM_CABC = 0x86,
  _MM_PERM_CABD = 0x87, _MM_PERM_CACA = 0x88, _MM_PERM_CACB = 0x89,
  _MM_PERM_CACC = 0x8A, _MM_PERM_CACD = 0x8B, _MM_PERM_CADA = 0x8C,
  _MM_PERM_CADB = 0x8D, _MM_PERM_CADC = 0x8E, _MM_PERM_CADD = 0x8F,
  _MM_PERM_CBAA = 0x90, _MM_PERM_CBAB = 0x91, _MM_PERM_CBAC = 0x92,
  _MM_PERM_CBAD = 0x93, _MM_PERM_CBBA = 0x94, _MM_PERM_CBBB = 0x95,
  _MM_PERM_CBBC = 0x96, _MM_PERM_CBBD = 0x97, _MM_PERM_CBCA = 0x98,
  _MM_PERM_CBCB = 0x99, _MM_PERM_CBCC = 0x9A, _MM_PERM_CBCD = 0x9B,
  _MM_PERM_CBDA = 0x9C, _MM_PERM_CBDB = 0x9D, _MM_PERM_CBDC = 0x9E,
  _MM_PERM_CBDD = 0x9F, _MM_PERM_CCAA = 0xA0, _MM_PERM_CCAB = 0xA1,
  _MM_PERM_CCAC = 0xA2, _MM_PERM_CCAD = 0xA3, _MM_PERM_CCBA = 0xA4,
  _MM_PERM_CCBB = 0xA5, _MM_PERM_CCBC = 0xA6, _MM_PERM_CCBD = 0xA7,
  _MM_PERM_CCCA = 0xA8, _MM_PERM_CCCB = 0xA9, _MM_PERM_CCCC = 0xAA,
  _MM_PERM_CCCD = 0xAB, _MM_PERM_CCDA = 0xAC, _MM_PERM_CCDB = 0xAD,
  _MM_PERM_CCDC = 0xAE, _MM_PERM_CCDD = 0xAF, _MM_PERM_CDAA = 0xB0,
  _MM_PERM_CDAB = 0xB1, _MM_PERM_CDAC = 0xB2, _MM_PERM_CDAD = 0xB3,
  _MM_PERM_CDBA = 0xB4, _MM_PERM_CDBB = 0xB5, _MM_PERM_CDBC = 0xB6,
  _MM_PERM_CDBD = 0xB7, _MM_PERM_CDCA = 0xB8, _MM_PERM_CDCB = 0xB9,
  _MM_PERM_CDCC = 0xBA, _MM_PERM_CDCD = 0xBB, _MM_PERM_CDDA = 0xBC,
  _MM_PERM_CDDB = 0xBD, _MM_PERM_CDDC = 0xBE, _MM_PERM_CDDD = 0xBF,
  _MM_PERM_DAAA = 0xC0, _MM_PERM_DAAB = 0xC1, _MM_PERM_DAAC = 0xC2,
  _MM_PERM_DAAD = 0xC3, _MM_PERM_DABA = 0xC4, _MM_PERM_DABB = 0xC5,
  _MM_PERM_DABC = 0xC6, _MM_PERM_DABD = 0xC7, _MM_PERM_DACA = 0xC8,
  _MM_PERM_DACB = 0xC9, _MM_PERM_DACC = 0xCA, _MM_PERM_DACD = 0xCB,
  _MM_PERM_DADA = 0xCC, _MM_PERM_DADB = 0xCD, _MM_PERM_DADC = 0xCE,
  _MM_PERM_DADD = 0xCF, _MM_PERM_DBAA = 0xD0, _MM_PERM_DBAB = 0xD1,
  _MM_PERM_DBAC = 0xD2, _MM_PERM_DBAD = 0xD3, _MM_PERM_DBBA = 0xD4,
  _MM_PERM_DBBB = 0xD5, _MM_PERM_DBBC = 0xD6, _MM_PERM_DBBD = 0xD7,
  _MM_PERM_DBCA = 0xD8, _MM_PERM_DBCB = 0xD9, _MM_PERM_DBCC = 0xDA,
  _MM_PERM_DBCD = 0xDB, _MM_PERM_DBDA = 0xDC, _MM_PERM_DBDB = 0xDD,
  _MM_PERM_DBDC = 0xDE, _MM_PERM_DBDD = 0xDF, _MM_PERM_DCAA = 0xE0,
  _MM_PERM_DCAB = 0xE1, _MM_PERM_DCAC = 0xE2, _MM_PERM_DCAD = 0xE3,
  _MM_PERM_DCBA = 0xE4, _MM_PERM_DCBB = 0xE5, _MM_PERM_DCBC = 0xE6,
  _MM_PERM_DCBD = 0xE7, _MM_PERM_DCCA = 0xE8, _MM_PERM_DCCB = 0xE9,
  _MM_PERM_DCCC = 0xEA, _MM_PERM_DCCD = 0xEB, _MM_PERM_DCDA = 0xEC,
  _MM_PERM_DCDB = 0xED, _MM_PERM_DCDC = 0xEE, _MM_PERM_DCDD = 0xEF,
  _MM_PERM_DDAA = 0xF0, _MM_PERM_DDAB = 0xF1, _MM_PERM_DDAC = 0xF2,
  _MM_PERM_DDAD = 0xF3, _MM_PERM_DDBA = 0xF4, _MM_PERM_DDBB = 0xF5,
  _MM_PERM_DDBC = 0xF6, _MM_PERM_DDBD = 0xF7, _MM_PERM_DDCA = 0xF8,
  _MM_PERM_DDCB = 0xF9, _MM_PERM_DDCC = 0xFA, _MM_PERM_DDCD = 0xFB,
  _MM_PERM_DDDA = 0xFC, _MM_PERM_DDDB = 0xFD, _MM_PERM_DDDC = 0xFE,
  _MM_PERM_DDDD = 0xFF
} _MM_PERM_ENUM;

typedef enum
{
  _MM_MANT_NORM_1_2,    
  _MM_MANT_NORM_p5_2,   
  _MM_MANT_NORM_p5_1,   
  _MM_MANT_NORM_p75_1p5   
} _MM_MANTISSA_NORM_ENUM;

typedef enum
{
  _MM_MANT_SIGN_src,    
  _MM_MANT_SIGN_zero,   
  _MM_MANT_SIGN_nan   
} _MM_MANTISSA_SIGN_ENUM;






static  __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_setzero_si512(void)
{
  return (__m512i)(__v8di){ 0, 0, 0, 0, 0, 0, 0, 0 };
}



static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_undefined_pd(void)
{
  return (__m512d)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_undefined(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_undefined_ps(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_undefined_epi32(void)
{
  return (__m512i)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcastd_epi32 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si) __A,
                                          (__v4si)_mm_undefined_si128(),
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcastd_epi32 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcastd_epi32 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcastq_epi64 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di) __A,
                                          (__v2di) _mm_undefined_si128(),
                                          0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcastq_epi64 (__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) __O);

}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) _mm512_setzero_si512());
}


static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_setzero_ps(void)
{
  return (__m512){ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}



static  __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_setzero_pd(void)
{
  return (__m512d){ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_ps(float __w)
{
  return (__m512){ __w, __w, __w, __w, __w, __w, __w, __w,
                   __w, __w, __w, __w, __w, __w, __w, __w  };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_pd(double __w)
{
  return (__m512d){ __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_epi8(char __w)
{
  return (__m512i)(__v64qi){ __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w  };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_epi16(short __w)
{
  return (__m512i)(__v32hi){ __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w,
                             __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_epi32(int __s)
{
  return (__m512i)(__v16si){ __s, __s, __s, __s, __s, __s, __s, __s,
                             __s, __s, __s, __s, __s, __s, __s, __s };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_set1_epi32(__mmask16 __M, int __A) 
{
  return (__m512i)__builtin_ia32_selectd_512(__M, 
                                             (__v16si)_mm512_set1_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set1_epi64(long long __d)
{
  return (__m512i)(__v8di){ __d, __d, __d, __d, __d, __d, __d, __d };
}


static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_set1_epi64(__mmask8 __M, long long __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di)_mm512_set1_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}


static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcastss_ps(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf) __A,
                                         (__v4sf)_mm_undefined_ps(),
                                         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set4_epi32 (int __A, int __B, int __C, int __D)
{
  return  (__m512i)(__v16si)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set4_epi64 (long long __A, long long __B, long long __C,
       long long __D)
{
  return  (__m512i) (__v8di)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set4_pd (double __A, double __B, double __C, double __D)
{
  return  (__m512d)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set4_ps (float __A, float __B, float __C, float __D)
{
  return  (__m512)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}













static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcastsd_pd(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df) __A,
                                          (__v2df) _mm_undefined_pd(),
                                          0, 0, 0, 0, 0, 0, 0, 0);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, -1, -1, -1, -1);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps256_ps512(__m256 __a)
{
  return __builtin_shufflevector(__a, __a, 0,  1,  2,  3,  4,  5,  6,  7,
                                          -1, -1, -1, -1, -1, -1, -1, -1);
}

static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd512_pd128(__m512d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd512_pd256 (__m512d __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps512_ps128(__m512 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps512_ps256 (__m512 __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd_ps (__m512d __A)
{
  return (__m512) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd_si512 (__m512d __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castpd128_pd512 (__m128d __A)
{
  return __builtin_shufflevector( __A, __A, 0, 1, -1, -1, -1, -1, -1, -1);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps_pd (__m512 __A)
{
  return (__m512d) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps_si512 (__m512 __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castps128_ps512 (__m128 __A)
{
    return  __builtin_shufflevector( __A, __A, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi128_si512 (__m128i __A)
{
   return  __builtin_shufflevector( __A, __A, 0, 1, -1, -1, -1, -1, -1, -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi256_si512 (__m256i __A)
{
   return  __builtin_shufflevector( __A, __A, 0, 1, 2, 3, -1, -1, -1, -1);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi512_ps (__m512i __A)
{
  return (__m512) (__A);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi512_pd (__m512i __A)
{
  return (__m512d) (__A);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi512_si128 (__m512i __A)
{
  return (__m128i)__builtin_shufflevector(__A, __A , 0, 1);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_castsi512_si256 (__m512i __A)
{
  return (__m256i)__builtin_shufflevector(__A, __A , 0, 1, 2, 3);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_int2mask(int __a)
{
  return (__mmask16)__a;
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2int(__mmask16 __a)
{
  return (int)__a;
}

/// \brief Constructs a 512-bit floating-point vector of [8 x double] from a
///    128-bit floating-point vector of [2 x double]. The lower 128 bits
///    contain the value of the source vector. The upper 384 bits are set
///    to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [2 x double].
/// \returns A 512-bit floating-point vector of [8 x double]. The lower 128 bits
///    contain the value of the parameter. The upper 384 bits are set to zero.
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextpd128_pd512(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3, 2, 3, 2, 3);
}

/// \brief Constructs a 512-bit floating-point vector of [8 x double] from a
///    256-bit floating-point vector of [4 x double]. The lower 256 bits
///    contain the value of the source vector. The upper 256 bits are set
///    to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit vector of [4 x double].
/// \returns A 512-bit floating-point vector of [8 x double]. The lower 256 bits
///    contain the value of the parameter. The upper 256 bits are set to zero.
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)_mm256_setzero_pd(), 0, 1, 2, 3, 4, 5, 6, 7);
}

/// \brief Constructs a 512-bit floating-point vector of [16 x float] from a
///    128-bit floating-point vector of [4 x float]. The lower 128 bits contain
///    the value of the source vector. The upper 384 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit vector of [4 x float].
/// \returns A 512-bit floating-point vector of [16 x float]. The lower 128 bits
///    contain the value of the parameter. The upper 384 bits are set to zero.
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextps128_ps512(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7);
}

/// \brief Constructs a 512-bit floating-point vector of [16 x float] from a
///    256-bit floating-point vector of [8 x float]. The lower 256 bits contain
///    the value of the source vector. The upper 256 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit vector of [8 x float].
/// \returns A 512-bit floating-point vector of [16 x float]. The lower 256 bits
///    contain the value of the parameter. The upper 256 bits are set to zero.
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextps256_ps512(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)_mm256_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

/// \brief Constructs a 512-bit integer vector from a 128-bit integer vector.
///    The lower 128 bits contain the value of the source vector. The upper
///    384 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 128-bit integer vector.
/// \returns A 512-bit integer vector. The lower 128 bits contain the value of
///    the parameter. The upper 384 bits are set to zero.
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextsi128_si512(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3, 2, 3, 2, 3);
}

/// \brief Constructs a 512-bit integer vector from a 256-bit integer vector.
///    The lower 256 bits contain the value of the source vector. The upper
///    256 bits are set to zero.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic has no corresponding instruction.
///
/// \param __a
///    A 256-bit integer vector.
/// \returns A 512-bit integer vector. The lower 256 bits contain the value of
///    the parameter. The upper 256 bits are set to zero.
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_zextsi256_si512(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)_mm256_setzero_si256(), 0, 1, 2, 3, 4, 5, 6, 7);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_and_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a & (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_and_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                (__v16si) _mm512_and_epi32(__a, __b),
                (__v16si) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_and_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi32(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_and_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_and_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
    return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __k,
                (__v8di) _mm512_and_epi64(__a, __b),
                (__v8di) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_and_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi64(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_andnot_si512 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)(__A) & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_andnot_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v16su)(__A) & (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_andnot_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_andnot_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_andnot_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi32(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_andnot_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)(__A) & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_andnot_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_andnot_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_andnot_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi64(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_or_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a | (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_or_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                             (__v16si)_mm512_or_epi32(__a, __b),
                                             (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_or_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_or_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_or_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_or_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_or_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_xor_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a ^ (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_xor_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                            (__v16si)_mm512_xor_epi32(__a, __b),
                                            (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_xor_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_xor_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_xor_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_xor_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_xor_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_and_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_or_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_xor_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_add_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a + (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_add_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a + (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mul_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a * (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mul_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a * (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sub_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a - (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sub_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a - (__v16sf)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_add_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A + (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_add_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_add_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sub_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A - (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sub_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sub_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_add_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A + (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_add_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_add_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sub_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A - (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sub_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sub_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}



















static  __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df)
             _mm512_setzero_pd (),
             (__mmask8) -1,
             0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
                  (__v8df) __B,
                  (__v8df) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512_mask ((__v8df) __A,
                  (__v8df) __B,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U,
                  0x04);
}



















static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf)
            _mm512_setzero_ps (),
            (__mmask16) -1,
            0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
                 (__v16sf) __B,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512_mask ((__v16sf) __A,
                 (__v16sf) __B,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_max_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_max_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_max_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_max_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_epi32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsd512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_epu32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxud512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_epi64(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_max_epu64(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_max_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_max_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di)
                   _mm512_setzero_si512 (),
                   __M);
}



















static  __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
             (__v8df) __B,
             (__v8df)
             _mm512_setzero_pd (),
             (__mmask8) -1,
             0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
                  (__v8df) __B,
                  (__v8df) __W,
                  (__mmask8) __U,
                  0x04);
}



















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512_mask ((__v8df) __A,
                  (__v8df) __B,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U,
                  0x04);
}

static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
            (__v16sf) __B,
            (__v16sf)
            _mm512_setzero_ps (),
            (__mmask16) -1,
            0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
                 (__v16sf) __B,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512_mask ((__v16sf) __A,
                 (__v16sf) __B,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_min_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_min_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_min_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_min_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_epi32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsd512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_epu32(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminud512_mask ((__v16si) __A,
                   (__v16si) __B,
                   (__v16si)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_epi64(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_min_epu64(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_min_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di) __W, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_min_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuq512_mask ((__v8di) __A,
                   (__v8di) __B,
                   (__v8di)
                   _mm512_setzero_si512 (),
                   __M);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mul_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuldq512((__v16si)__X, (__v16si) __Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mul_epi32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mul_epi32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mul_epu32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuludq512((__v16si)__X, (__v16si)__Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mul_epu32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mul_epu32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mullo_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A * (__v16su) __B);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mullo_epi32(__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mullo_epi32(__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)__W);
}
















static  __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sqrt_pd(__m512d __a)
{
  return (__m512d)__builtin_ia32_sqrtpd512_mask((__v8df)__a,
                                                (__v8df) _mm512_setzero_pd (),
                                                (__mmask8) -1,
                                                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sqrt_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
                   (__v8df) __W,
                   (__mmask8) __U,
                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sqrt_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_sqrtpd512_mask ((__v8df) __A,
                   (__v8df)
                   _mm512_setzero_pd (),
                   (__mmask8) __U,
                   0x04);
}
















static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sqrt_ps(__m512 __a)
{
  return (__m512)__builtin_ia32_sqrtps512_mask((__v16sf)__a,
                                               (__v16sf) _mm512_setzero_ps (),
                                               (__mmask16) -1,
                                               0x04);
}

static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sqrt_ps(__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_sqrtps512_mask((__v16sf)__A,
                                               (__v16sf) __W,
                                               (__mmask16) __U,
                                               0x04);
}

static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sqrt_ps( __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_sqrtps512_mask((__v16sf)__A,
                                               (__v16sf) _mm512_setzero_ps (),
                                               (__mmask16) __U,
                                               0x04);
}

static  __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rsqrt14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                 (__v8df)
                 _mm512_setzero_pd (),
                 (__mmask8) -1);}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rsqrt14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rsqrt14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rsqrt14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rsqrt14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rsqrt14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static  __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_rsqrt14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_rsqrt14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_rsqrt14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static  __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_rsqrt14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd_mask ((__v2df) __A,
              (__v2df) __B,
              (__v2df)
              _mm_setzero_pd (),
              (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_rsqrt14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_rsqrt14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static  __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rcp14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
               (__v8df)
               _mm512_setzero_pd (),
               (__mmask8) -1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rcp14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rcp14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U);
}

static  __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rcp14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
              (__v16sf)
              _mm512_setzero_ps (),
              (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rcp14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rcp14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf)
                   _mm512_setzero_ps (),
                   (__mmask16) __U);
}

static  __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_rcp14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
                 (__v4sf) __B,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_rcp14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_rcp14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static  __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_rcp14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd_mask ((__v2df) __A,
            (__v2df) __B,
            (__v2df)
            _mm_setzero_pd (),
            (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_rcp14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_rcp14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_floor_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x01),
                                                  (__v16sf) __A, -1,
                                                  0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_floor_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x01),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_floor_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x01),
                                                   (__v8df) __A, -1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_floor_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x01),
                (__v8df) __W, __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_ceil_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x02),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_ceil_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x02),
                                                  (__v16sf) __A, -1,
                                                  0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_ceil_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x02),
                                                   (__v8df) __A, -1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_ceil_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x02),
                (__v8df) __W, __U,
                0x04);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_abs_epi64(__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
             (__v8di)
             _mm512_setzero_si512 (),
             (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_abs_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_abs_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsq512_mask ((__v8di) __A,
                  (__v8di)
                  _mm512_setzero_si512 (),
                  (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_abs_epi32(__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
             (__v16si)
             _mm512_setzero_si512 (),
             (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_abs_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
                  (__v16si) __W,
                  (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_abs_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsd512_mask ((__v16si) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_add_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_addss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_add_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_addss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_add_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_addsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_add_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_addsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}


















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_add_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_add_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_add_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_add_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}





































static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_sub_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_subss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_sub_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_subss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}


















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_sub_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_subsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_sub_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_subsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sub_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sub_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sub_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sub_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}





































static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_mul_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_mulss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_mul_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_mulss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}


















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_mul_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_mulsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_mul_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_mulsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mul_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mul_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mul_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mul_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}





































static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_div_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_divss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_div_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_divss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf)  _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_div_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_divsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_div_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_divsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)  _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_div_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a/(__v8df)__b);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_div_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_div_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_div_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a/(__v16sf)__b);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_div_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_div_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}























































































































































































static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask (-(__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fnmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask (-(__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fnmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}





















































































static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask (-(__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fnmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask (-(__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fnmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}


















































static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) -1,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmaddsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask3 ((__v8df) __A,
                                                        (__v8df) __B,
                                                        (__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmaddsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                        (__v8df) __B,
                                                        (__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) -1,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmsubadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmsubadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                        (__v8df) __B,
                                                        -(__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}


















































static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmaddsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask3 ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmaddsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fmsubadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_fmsubadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       -(__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}








static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask3 ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}








static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask3 ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}








static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubaddpd512_mask3 ((__v8df) __A,
                                                        (__v8df) __B,
                                                        (__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}








static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubaddps512_mask3 ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}








static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fnmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmaddpd512_mask ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}








static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fnmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmaddps512_mask ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fnmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfnmsubpd512_mask3 ((__v8df) __A,
                                                      (__v8df) __B,
                                                      (__v8df) __C,
                                                      (__mmask8) __U,
                                                      0x04);
}















static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_fnmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask3_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfnmsubps512_mask3 ((__v16sf) __A,
                                                     (__v16sf) __B,
                                                     (__v16sf) __C,
                                                     (__mmask16) __U,
                                                     0x04);
}





static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutex2var_epi32(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_mask ((__v16si) __I
                                                        ,
                                                       (__v16si) __A,
                                                       (__v16si) __B,
                                                       (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutex2var_epi32 (__m512i __A, __mmask16 __U,
                                __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_mask ((__v16si) __I
                                                         ,
                                                        (__v16si) __A,
                                                        (__v16si) __B,
                                                        (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutex2var_epi32 (__mmask16 __U, __m512i __A,
                                 __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2vard512_maskz ((__v16si) __I
                                                         ,
                                                        (__v16si) __A,
                                                        (__v16si) __B,
                                                        (__mmask16) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutex2var_epi64(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_mask ((__v8di) __I
                                                        ,
                                                       (__v8di) __A,
                                                       (__v8di) __B,
                                                       (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutex2var_epi64 (__m512i __A, __mmask8 __U, __m512i __I,
                                __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_mask ((__v8di) __I
                                                        ,
                                                       (__v8di) __A,
                                                       (__v8di) __B,
                                                       (__mmask8) __U);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutex2var_epi64 (__mmask8 __U, __m512i __A,
         __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varq512_maskz ((__v8di) __I
                                                         ,
                                                        (__v8di) __A,
                                                        (__v8di) __B,
                                                        (__mmask8) __U);
}


# 3521 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 3551 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4


















































static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_blend_pd(__mmask8 __U, __m512d __A, __m512d __W)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
                 (__v8df) __W,
                 (__v8df) __A);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_blend_ps(__mmask16 __U, __m512 __A, __m512 __W)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
                (__v16sf) __W,
                (__v16sf) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_blend_epi64(__mmask8 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                (__v8di) __W,
                (__v8di) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_blend_epi32(__mmask16 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                (__v16si) __W,
                (__v16si) __A);
}



































































































































static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvttps_epu32(__m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvttps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) __W,
                   (__mmask16) __U,
                   0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvttps_epu32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) _mm512_setzero_si512 (),
                   (__mmask16) __U,
                   0x04);
}































static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu32_ps (__m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
                 (__v16sf) _mm512_undefined_ps (),
                 (__mmask16) -1,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtudq2ps512_mask ((__v16si) __A,
                 (__v16sf) _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8si)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepi32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepi32_pd(__W, __U, _mm512_castsi512_si256(__A));
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32_ps (__m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
                (__v16sf) _mm512_undefined_ps (),
                (__mmask16) -1,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
                (__v16sf) __W,
                (__mmask16) __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512) __builtin_ia32_cvtdq2ps512_mask ((__v16si) __A,
                (__v16sf) _mm512_setzero_ps (),
                (__mmask16) __U,
                0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8su)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepu32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepu32_pd(__W, __U, _mm512_castsi512_si256(__A));
}
















static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtpd_ps (__m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_undefined_ps (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtpd_ps (__m256 __W, __mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtpd_ps (__mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_setzero_ps (),
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtpd_pslo (__m512d __A)
{
  return (__m512) __builtin_shufflevector((__v8sf) _mm512_cvtpd_ps(__A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtpd_pslo (__m512 __W, __mmask8 __U,__m512d __A)
{
  return (__m512) __builtin_shufflevector (
                (__v8sf) _mm512_mask_cvtpd_ps (_mm512_castps512_ps256(__W),
                                               __U, __A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}















































static  __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtph_ps(__m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtph_ps (__m512 __W, __mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtph_ps (__mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}
















static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvttpd_epi32(__m512d __a)
{
  return (__m256i)__builtin_ia32_cvttpd2dq512_mask((__v8df) __a,
                                                   (__v8si)_mm256_setzero_si256(),
                                                   (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvttpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvttpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}
















static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvttps_epi32(__m512 __a)
{
  return (__m512i)
    __builtin_ia32_cvttps2dq512_mask((__v16sf) __a,
                                     (__v16si) _mm512_setzero_si512 (),
                                     (__mmask16) -1, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvttps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvttps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) _mm512_setzero_si512 (),
                  (__mmask16) __U,
                  0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtps_epi32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) _mm512_undefined_epi32 (),
                 (__mmask16) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si)
                 _mm512_setzero_si512 (),
                 (__mmask16) __U,
                 0x04);
}
















static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtpd_epi32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_undefined_si256 (),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U,
                 0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtps_epu32 ( __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,                   (__v16si)                   _mm512_undefined_epi32 (),                   (__mmask16) -1,                   0x04);}






static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtps_epu32 ( __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U ,
                  0x04);
}
















static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsd_f64(__m512d __a)
{
  return __a[0];
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtss_f32(__m512 __a)
{
  return __a[0];
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpackhi_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpackhi_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpackhi_pd(__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpacklo_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpacklo_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpacklo_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpackhi_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         2,    18,    3,    19,
                                         2+4,  18+4,  3+4,  19+4,
                                         2+8,  18+8,  3+8,  19+8,
                                         2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpackhi_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpackhi_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpacklo_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         0,    16,    1,    17,
                                         0+4,  16+4,  1+4,  17+4,
                                         0+8,  16+8,  1+8,  17+8,
                                         0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpacklo_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpacklo_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpackhi_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          2,    18,    3,    19,
                                          2+4,  18+4,  3+4,  19+4,
                                          2+8,  18+8,  3+8,  19+8,
                                          2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpackhi_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpackhi_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpacklo_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          0,    16,    1,    17,
                                          0+4,  16+4,  1+4,  17+4,
                                          0+8,  16+8,  1+8,  17+8,
                                          0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpacklo_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpacklo_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpackhi_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpackhi_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpackhi_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_unpacklo_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_unpacklo_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_unpacklo_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}




static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_loadu_si512 (void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((__const int *) __P,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) -1);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_loadu_epi32 (__m512i __W, __mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((__const int *) __P,
                  (__v16si) __W,
                  (__mmask16) __U);
}


static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_loadu_epi32(__mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((__const int *)__P,
                                                     (__v16si)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask16) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_loadu_epi64 (__m512i __W, __mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((__const long long *) __P,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_loadu_epi64(__mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((__const long long *)__P,
                                                     (__v8di)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_loadu_ps (__m512 __W, __mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((__const float *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_loadu_ps(__mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((__const float *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_loadu_pd (__m512d __W, __mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((__const double *) __P,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_loadu_pd(__mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((__const double *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_loadu_pd(void __const *__p)
{
  struct __loadu_pd {
    __m512d __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_pd*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_loadu_ps(void __const *__p)
{
  struct __loadu_ps {
    __m512 __v;
  } __attribute__((__packed__, __may_alias__));
  return ((struct __loadu_ps*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_load_ps(void __const *__p)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((__const __v16sf *)__p,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) -1);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_load_ps (__m512 __W, __mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((__const __v16sf *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_load_ps(__mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((__const __v16sf *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_load_pd(void __const *__p)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((__const __v8df *)__p,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) -1);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_load_pd (__m512d __W, __mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((__const __v8df *) __P,
                          (__v8df) __W,
                          (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_load_pd(__mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((__const __v8df *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_load_si512 (void __const *__P)
{
  return *(__m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_load_epi32 (void __const *__P)
{
  return *(__m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_load_epi64 (void __const *__P)
{
  return *(__m512i *) __P;
}



static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_storeu_epi64(void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_storedqudi512_mask ((long long *)__P, (__v8di) __A,
                                     (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_storeu_si512 (void *__P, __m512i __A)
{
  __builtin_ia32_storedqusi512_mask ((int *) __P, (__v16si) __A,
            (__mmask16) -1);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_storeu_epi32(void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_storedqusi512_mask ((int *)__P, (__v16si) __A,
                                     (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_storeu_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeupd512_mask ((double *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_storeu_pd(void *__P, __m512d __A)
{
  __builtin_ia32_storeupd512_mask((double *)__P, (__v8df)__A, (__mmask8)-1);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_storeu_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeups512_mask ((float *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_storeu_ps(void *__P, __m512 __A)
{
  __builtin_ia32_storeups512_mask((float *)__P, (__v16sf)__A, (__mmask16)-1);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_store_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeapd512_mask ((__v8df *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_store_pd(void *__P, __m512d __A)
{
  *(__m512d*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_store_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeaps512_mask ((__v16sf *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_store_ps(void *__P, __m512 __A)
{
  *(__m512*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_store_si512 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_store_epi32 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_store_epi64 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}



static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_knot(__mmask16 __M)
{
  return __builtin_ia32_knothi(__M);
}




# 4840 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4


# 4865 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4


# 4890 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4


# 4915 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi8_epi32(__m128i __A)
{
  

  return (__m512i)__builtin_convertvector((__v16qs)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi8_epi64(__m128i __A)
{
  

  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__A, (__v16qs)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8si)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hi)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hi)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu8_epi32(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v16qu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu8_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__A, (__v16qu)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8su)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepu16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hu)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepu16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rorv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rorv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rorv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rorv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rorv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rorv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prorvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) __U);
}







































































static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rolv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rolv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rolv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvd512_mask ((__v16si) __A,
              (__v16si) __B,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_rolv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_rolv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_rolv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_prolvq512_mask ((__v8di) __A,
              (__v8di) __B,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) __U);
}






























static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_slli_epi32(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_pslldi512((__v16si)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_slli_epi32(__m512i __W, __mmask16 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_slli_epi32(__mmask16 __U, __m512i __A, int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_slli_epi64(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psllqi512((__v8di)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_slli_epi64(__m512i __W, __mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_slli_epi64(__mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srli_epi32(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psrldi512((__v16si)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srli_epi32(__m512i __W, __mmask16 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srli_epi32(__mmask16 __U, __m512i __A, int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srli_epi64(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psrlqi512((__v8di)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srli_epi64(__m512i __W, __mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srli_epi64(__mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_load_epi32 (__m512i __W, __mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((__const __v16si *) __P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_load_epi32 (__mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((__const __v16si *) __P,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_store_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_movdqa32store512_mask ((__v16si *) __P, (__v16si) __A,
          (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mov_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mov_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mov_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mov_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_load_epi64 (__m512i __W, __mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((__const __v8di *) __P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_load_epi64 (__mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((__const __v8di *) __P,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_store_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_movdqa64store512_mask ((__v8di *) __P, (__v8di) __A,
          (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_movedup_pd (__m512d __A)
{
  return (__m512d)__builtin_shufflevector((__v8df)__A, (__v8df)__A,
                                          0, 0, 2, 2, 4, 4, 6, 6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_movedup_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_movedup_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}






































































































































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_getexp_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd128_round_mask ((__v2df) __A,
                 (__v2df) __B, (__v2df) _mm_setzero_pd(), (__mmask8) -1, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_getexp_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_getexp_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U,
          0x04);
}













static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_getexp_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
                (__v4sf) __B, (__v4sf)  _mm_setzero_ps(), (__mmask8) -1, 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_getexp_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_getexp_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_pd (),
          (__mmask8) __U,
          0x04);
}

































































































static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kmov (__mmask16 __A)
{
  return  __A;
}














static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2_permutex2var_epi32 (__m512i __A, __m512i __I,
         __mmask16 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2vard512_mask ((__v16si) __A,
                   (__v16si) __I
                    ,
                   (__v16si) __B,
                   (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sll_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_pslld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sll_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sll_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sll_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sll_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sll_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sll_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sll_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sllv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sllv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sllv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sllv_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sllv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sllv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sra_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrad512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sra_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sra_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_sra_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_sra_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_sra_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srav_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srav_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srav_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srav_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srav_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srav_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srl_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srl_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srl_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srl_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srl_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srl_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srlv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srlv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srlv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srlv_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srlv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srlv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}



















































static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvtsd2usi32 ((__v2df) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvtsd2usi64 ((__v2df)
                 __A,
                 0x04);
}



















static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvtss2usi32 ((__v4sf) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvtss2usi64 ((__v4sf)
                 __A,
                 0x04);
}








static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttsd_i32 (__m128d __A)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A,
              0x04);
}








static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttsd_i64 (__m128d __A)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvttsd2usi32 ((__v2df) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvttsd2usi64 ((__v2df)
                  __A,
                  0x04);
}








static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttss_i32 (__m128 __A)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A,
              0x04);
}








static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttss_i64 (__m128 __A)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvttss2usi32 ((__v4sf) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvttss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvttss2usi64 ((__v4sf)
                  __A,
                  0x04);
}


static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2_permutex2var_pd (__m512d __A, __m512i __I, __mmask8 __U,
            __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermi2varpd512_mask ((__v8df) __A,
              (__v8di) __I
               ,
              (__v8df) __B,
              (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2_permutex2var_ps (__m512 __A, __m512i __I, __mmask16 __U,
            __m512 __B)
{
  return (__m512) __builtin_ia32_vpermi2varps512_mask ((__v16sf) __A,
                   (__v16si) __I
                    ,
                   (__v16sf) __B,
                   (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2_permutex2var_epi64 (__m512i __A, __m512i __I,
         __mmask8 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varq512_mask ((__v8di) __A,
                   (__v8di) __I
                    ,
                   (__v8di) __B,
                   (__mmask8) __U);
}


# 6434 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 6464 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4











static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutevar_pd(__m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_vpermilvarpd512((__v8df)__A, (__v8di)__C);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutevar_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutevar_pd(__mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutevar_ps(__m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_vpermilvarps512((__v16sf)__A, (__v16si)__C);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutevar_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutevar_ps(__mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutex2var_pd(__m512d __A, __m512i __I, __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_mask ((__v8di) __I
                     ,
                    (__v8df) __A,
                    (__v8df) __B,
                    (__mmask8) -1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutex2var_pd (__m512d __A, __mmask8 __U, __m512i __I, __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_mask ((__v8di) __I
                     ,
                    (__v8df) __A,
                    (__v8df) __B,
                    (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutex2var_pd (__mmask8 __U, __m512d __A, __m512i __I,
            __m512d __B)
{
  return (__m512d) __builtin_ia32_vpermt2varpd512_maskz ((__v8di) __I
                                                          ,
                                                         (__v8df) __A,
                                                         (__v8df) __B,
                                                         (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutex2var_ps(__m512 __A, __m512i __I, __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_mask ((__v16si) __I
                                                          ,
                                                         (__v16sf) __A,
                                                         (__v16sf) __B,
                                                         (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutex2var_ps (__m512 __A, __mmask16 __U, __m512i __I, __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_mask ((__v16si) __I
                                                          ,
                                                         (__v16sf) __A,
                                                         (__v16sf) __B,
                                                         (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutex2var_ps (__mmask16 __U, __m512 __A, __m512i __I,
            __m512 __B)
{
  return (__m512) __builtin_ia32_vpermt2varps512_maskz ((__v16si) __I
                                                         ,
                                                        (__v16sf) __A,
                                                        (__v16sf) __B,
                                                        (__mmask16) __U);
}

















static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvttpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvttpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvttpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}







































































































static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_scalef_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_scalef_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_scalef_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}



















static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_scalef_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_scalef_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_scalef_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_scalef_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefsd_round_mask ((__v2df) __A,
              (__v2df)( __B), (__v2df) _mm_setzero_pd(),
              (__mmask8) -1,
              0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_scalef_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_scalef_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}













static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_scalef_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefss_round_mask ((__v4sf) __A,
             (__v4sf)( __B), (__v4sf) _mm_setzero_ps(),
             (__mmask8) -1,
             0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_scalef_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_scalef_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}








static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srai_epi32(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psradi512((__v16si)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srai_epi32(__m512i __W, __mmask16 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,                                           (__v16si)_mm512_srai_epi32(__A, __B),                                           (__v16si)__W);


}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srai_epi32(__mmask16 __U, __m512i __A, int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,                                           (__v16si)_mm512_srai_epi32(__A, __B),                                           (__v16si)_mm512_setzero_si512());


}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_srai_epi64(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psraqi512((__v8di)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_srai_epi64(__m512i __W, __mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,                                            (__v8di)_mm512_srai_epi64(__A, __B),                                            (__v8di)__W);


}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_srai_epi64(__mmask8 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,                                            (__v8di)_mm512_srai_epi64(__A, __B),                                            (__v8di)_mm512_setzero_si512());


}


# 6967 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 6989 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7011 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7033 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7055 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7085 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_sqrt_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_sqrt_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}













static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_sqrt_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_sqrt_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcast_f32x4(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcast_f32x4(__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcast_f32x4(__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcast_f64x4(__m256d __A)
{
  return (__m512d)__builtin_shufflevector((__v4df)__A, (__v4df)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcast_f64x4(__m512d __O, __mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcast_f64x4(__mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcast_i32x4(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcast_i32x4(__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcast_i32x4(__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_broadcast_i64x4(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v4di)__A, (__v4di)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcast_i64x4(__m512i __O, __mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcast_i64x4(__mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcastsd_pd (__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) __O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) _mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_broadcastss_ps (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) __O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_broadcastss_ps (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) _mm512_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtsepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_undefined_si256 (),
               (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtsepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtsepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_undefined_si256 (),
               (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtsepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_storeu_epi32 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtsepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtsepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtusepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtusepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtusepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_undefined_si256 (),
                (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtusepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtusepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtusepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtusepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_undefined_si256 (),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtusepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqd512mem_mask ((__v8si*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtusepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtusepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtusepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqw512mem_mask ((__v8hi*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_undefined_si256 (),
              (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi32_storeu_epi16 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdw512mem_mask ((__v16hi *) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_undefined_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}






































# 7783 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7805 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7835 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 7865 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4








































































































static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_getexp_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_getexp_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_getexp_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
















static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_getexp_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_getexp_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_getexp_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}

















































































































































































static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
          (__v4sf) __A,
          (__v4sf) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
 return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask3 ((__v4sf) __W,
          (__v4sf) __X,
          (__v4sf) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
          (__v4sf) __A,
          -(__v4sf) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
 return (__m128) __builtin_ia32_vfmaddss3_maskz ((__v4sf) __A,
          (__v4sf) __B,
          -(__v4sf) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
 return (__m128) __builtin_ia32_vfmsubss3_mask3 ((__v4sf) __W,
          (__v4sf) __X,
          (__v4sf) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fnmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
          -(__v4sf) __A,
          (__v4sf) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fnmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
 return (__m128) __builtin_ia32_vfmaddss3_maskz (-(__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fnmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask3 (-(__v4sf) __W,
          (__v4sf) __X,
          (__v4sf) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fnmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_vfmaddss3_mask ((__v4sf) __W,
          -(__v4sf) __A,
          -(__v4sf) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fnmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
 return (__m128) __builtin_ia32_vfmaddss3_maskz (-(__v4sf) __A,
          (__v4sf) __B,
          -(__v4sf) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fnmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
 return (__m128) __builtin_ia32_vfnmsubss3_mask3 ((__v4sf) __W,
          (__v4sf) __X,
          (__v4sf) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask ( (__v2df) __W,
          (__v2df) __A,
          (__v2df) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_maskz ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask3 ((__v2df) __W,
          (__v2df) __X,
          (__v2df) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask ( (__v2df) __W,
          (__v2df) __A,
          -(__v2df) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_maskz ( (__v2df) __A,
          (__v2df) __B,
          -(__v2df) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
 return (__m128d) __builtin_ia32_vfmsubsd3_mask3 ((__v2df) __W,
          (__v2df) __X,
          (__v2df) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fnmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask ( (__v2df) __W,
          -(__v2df) __A,
          (__v2df) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fnmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_maskz ( -(__v2df) __A,
          (__v2df) __B,
          (__v2df) __C,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fnmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask3 (-(__v2df) __W,
          (__v2df) __X,
          (__v2df) __Y,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_fnmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_mask ( (__v2df) __W,
          -(__v2df) __A,
          -(__v2df) __B,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_fnmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
 return (__m128d) __builtin_ia32_vfmaddsd3_maskz ( -(__v2df) __A,
          (__v2df) __B,
          -(__v2df) __C,
          (__mmask8) __U,
          0x04);
}








static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask3_fnmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
 return (__m128d) __builtin_ia32_vfnmsubsd3_mask3 ((__v2df) (__W),
          (__v2df) __X,
          (__v2df) (__Y),
          (__mmask8) __U,
          0x04);
}








# 8610 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4












# 8632 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4











static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutexvar_pd (__m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
                 (__v8di) __X,
                 (__v8df) _mm512_undefined_pd (),
                 (__mmask8) -1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutexvar_pd (__m512d __W, __mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
                 (__v8di) __X,
                 (__v8df) __W,
                 (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutexvar_pd (__mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d) __builtin_ia32_permvardf512_mask ((__v8df) __Y,
                 (__v8di) __X,
                 (__v8df) _mm512_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutexvar_epi64 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
                 (__v8di) __X,
                 (__v8di) _mm512_setzero_si512 (),
                 __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutexvar_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
                 (__v8di) __X,
                 (__v8di) _mm512_undefined_epi32 (),
                 (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutexvar_epi64 (__m512i __W, __mmask8 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvardi512_mask ((__v8di) __Y,
                 (__v8di) __X,
                 (__v8di) __W,
                 __M);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutexvar_ps (__m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
                (__v16si) __X,
                (__v16sf) _mm512_undefined_ps (),
                (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutexvar_ps (__m512 __W, __mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
                (__v16si) __X,
                (__v16sf) __W,
                (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutexvar_ps (__mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512) __builtin_ia32_permvarsf512_mask ((__v16sf) __Y,
                (__v16si) __X,
                (__v16sf) _mm512_setzero_ps (),
                (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_permutexvar_epi32 (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
                 (__v16si) __X,
                 (__v16si) _mm512_setzero_si512 (),
                 __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_permutexvar_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
                 (__v16si) __X,
                 (__v16si) _mm512_undefined_epi32 (),
                 (__mmask16) -1);
}



static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_permutexvar_epi32 (__m512i __W, __mmask16 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i) __builtin_ia32_permvarsi512_mask ((__v16si) __Y,
                 (__v16si) __X,
                 (__v16si) __W,
                 __M);
}



static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kand (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kandn (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandnhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_korhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kortestc (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestchi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kortestz (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestzhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kunpackb (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kunpckhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kxnor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxnorhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kxor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxorhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_stream_si512 (__m512i * __P, __m512i __A)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8di_aligned)__A, (__v8di_aligned*)__P);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_stream_load_si512 (void __const *__P)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  return (__m512i) __builtin_nontemporal_load((__const __v8di_aligned *)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_stream_pd (double *__P, __m512d __A)
{
  typedef __v8df __v8df_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8df_aligned)__A, (__v8df_aligned*)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_stream_ps (float *__P, __m512 __A)
{
  typedef __v16sf __v16sf_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v16sf_aligned)__A, (__v16sf_aligned*)__P);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compress_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_compress_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compress_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_compress_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di)
                  _mm512_setzero_si512 (),
                  (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compress_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_compress_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compress_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si) __W,
                  (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_compress_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U);
}















































static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_test_epi32_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpneq_epi32_mask (_mm512_and_epi32(__A, __B),
                                   _mm512_setzero_si512());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_test_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpneq_epi32_mask (__U, _mm512_and_epi32 (__A, __B),
                                        _mm512_setzero_si512());
}

static __inline __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_test_epi64_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpneq_epi64_mask (_mm512_and_epi32 (__A, __B),
                                   _mm512_setzero_si512());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_test_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpneq_epi64_mask (__U, _mm512_and_epi32 (__A, __B),
                                        _mm512_setzero_si512());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_testn_epi32_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpeq_epi32_mask (_mm512_and_epi32 (__A, __B),
                                  _mm512_setzero_si512());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_testn_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpeq_epi32_mask (__U, _mm512_and_epi32 (__A, __B),
                                       _mm512_setzero_si512());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_testn_epi64_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpeq_epi64_mask (_mm512_and_epi32 (__A, __B),
                                  _mm512_setzero_si512());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_testn_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpeq_epi64_mask (__U, _mm512_and_epi32 (__A, __B),
                                       _mm512_setzero_si512());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_movehdup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_movehdup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_movehdup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_moveldup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_moveldup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_moveldup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_move_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  __m128 res = __A;
  res[0] = (__U & 1) ? __B[0] : __W[0];
  return res;
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_move_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  __m128 res = __A;
  res[0] = (__U & 1) ? __B[0] : 0;
  return res;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_move_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  __m128d res = __A;
  res[0] = (__U & 1) ? __B[0] : __W[0];
  return res;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_move_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  __m128d res = __A;
  res[0] = (__U & 1) ? __B[0] : 0;
  return res;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_store_ss (float * __W, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storess128_mask ((__v16sf *)__W,
                (__v16sf) _mm512_castps128_ps512(__A),
                (__mmask16) __U & (__mmask16)1);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_store_sd (double * __W, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storesd128_mask ((__v8df *)__W,
                (__v8df) _mm512_castpd128_pd512(__A),
                (__mmask8) __U & 1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_load_ss (__m128 __W, __mmask8 __U, __const float* __A)
{
  __m128 src = (__v4sf) __builtin_shufflevector((__v4sf) __W,
                                                (__v4sf) {0.0, 0.0, 0.0, 0.0},
                                                0, 4, 4, 4);

  return (__m128) __builtin_shufflevector(
                           __builtin_ia32_loadss128_mask ((__v16sf *) __A,
                                      (__v16sf) _mm512_castps128_ps512(src),
                                      (__mmask16) __U & 1),
                           _mm512_undefined_ps(), 0, 1, 2, 3);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_load_ss (__mmask8 __U, __const float* __A)
{
  return (__m128) __builtin_shufflevector(
                           __builtin_ia32_loadss128_mask ((__v16sf *) __A,
                                      (__v16sf) _mm512_setzero_ps(),
                                      (__mmask16) __U & 1),
                           _mm512_undefined_ps(), 0, 1, 2, 3);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_load_sd (__m128d __W, __mmask8 __U, __const double* __A)
{
  __m128d src = (__v2df) __builtin_shufflevector((__v2df) __W,
                                                 (__v2df) {0.0, 0.0}, 0, 2);

  return (__m128d) __builtin_shufflevector(
                            __builtin_ia32_loadsd128_mask ((__v8df *) __A,
                                      (__v8df) _mm512_castpd128_pd512(src),
                                      (__mmask8) __U & 1),
                            _mm512_undefined_pd(), 0, 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_load_sd (__mmask8 __U, __const double* __A)
{
  return (__m128d) __builtin_shufflevector(
                            __builtin_ia32_loadsd128_mask ((__v8df *) __A,
                                      (__v8df) _mm512_setzero_pd(),
                                      (__mmask8) __U & 1),
                            _mm512_undefined_pd(), 0, 1);
}


# 9163 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4











static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expand_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expand_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expand_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) __W,
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expand_epi64 ( __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expandloadu_pd(__m512d __W, __mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((__const __v8df *)__P,
              (__v8df) __W,
              (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expandloadu_pd(__mmask8 __U, void __const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((__const __v8df *)__P,
              (__v8df) _mm512_setzero_pd(),
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expandloadu_epi64(__m512i __W, __mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((__const __v8di *)__P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expandloadu_epi64(__mmask8 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((__const __v8di *)__P,
              (__v8di) _mm512_setzero_pd(),
              (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expandloadu_ps(__m512 __W, __mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((__const __v16sf *)__P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expandloadu_ps(__mmask16 __U, void __const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((__const __v16sf *)__P,
                   (__v16sf) _mm512_setzero_ps(),
                   (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expandloadu_epi32(__m512i __W, __mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((__const __v16si *)__P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expandloadu_epi32(__mmask16 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((__const __v16si *)__P,
              (__v16si) _mm512_setzero_ps(),
              (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expand_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expand_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps(),
               (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_expand_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) __W,
                (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_expand_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) _mm512_setzero_ps(),
                (__mmask16) __U);
}
















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtps_pd (__m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
                (__v8df)
                _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtps_pd (__m512d __W, __mmask8 __U, __m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_cvtps_pd (__mmask8 __U, __m256 __A)
{
  return (__m512d) __builtin_ia32_cvtps2pd512_mask ((__v8sf) __A,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_cvtpslo_pd (__m512 __A)
{
  return (__m512) _mm512_cvtps_pd(_mm512_castps512_ps256(__A));
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_cvtpslo_pd (__m512d __W, __mmask8 __U, __m512 __A)
{
  return (__m512) _mm512_mask_cvtps_pd(__W, __U, _mm512_castps512_ps256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mov_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) __W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mov_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) _mm512_setzero_pd ());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_mov_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) __W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_maskz_mov_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) _mm512_setzero_ps ());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_compressstoredf512_mask ((__v8df *) __P, (__v8df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_compressstoredi512_mask ((__v8di *) __P, (__v8di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compressstoreu_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_compressstoresf512_mask ((__v16sf *) __P, (__v16sf) __A,
            (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_compressstoreu_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_compressstoresi512_mask ((__v16si *) __P, (__v16si) __A,
            (__mmask16) __U);
}



















static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_cvtsd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)(__A),
                                             (__v2df)(__B),
                                             (__v4sf)(__W),
                                             (__mmask8)(__U), 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_cvtsd_ss (__mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)(__A),
                                             (__v2df)(__B),
                                             (__v4sf)_mm_setzero_ps(),
                                             (__mmask8)(__U), 0x04);
}


# 9464 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4













































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_mask_cvtss_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)(__A),
                                              (__v4sf)(__B),
                                              (__v2df)(__W),
                                              (__mmask8)(__U), 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_maskz_cvtss_sd (__mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)(__A),
                                              (__v4sf)(__B),
                                              (__v2df)_mm_setzero_pd(),
                                              (__mmask8)(__U), 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtu32_sd (__m128d __A, unsigned __B)
{
  return (__m128d) __builtin_ia32_cvtusi2sd32 ((__v2df) __A, __B);
}






static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtu64_sd (__m128d __A, unsigned long long __B)
{
  return (__m128d) __builtin_ia32_cvtusi2sd64 ((__v2df) __A, __B,
                 0x04);
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtu32_ss (__m128 __A, unsigned __B)
{
  return (__m128) __builtin_ia32_cvtusi2ss32 ((__v4sf) __A, __B,
                0x04);
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_cvtu64_ss (__m128 __A, unsigned long long __B)
{
  return (__m128) __builtin_ia32_cvtusi2ss64 ((__v4sf) __A, __B,
                0x04);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_set1_epi32 (__m512i __O, __mmask16 __M, int __A)
{
  return (__m512i) __builtin_ia32_selectd_512(__M,
                                              (__v16si) _mm512_set1_epi32(__A),
                                              (__v16si) __O);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_set1_epi64 (__m512i __O, __mmask8 __M, long long __A)
{
  return (__m512i) __builtin_ia32_selectq_512(__M,
                                              (__v8di) _mm512_set1_epi64(__A),
                                              (__v8di) __O);
}


static  __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_epi8 (char __e63, char __e62, char __e61, char __e60, char __e59,
    char __e58, char __e57, char __e56, char __e55, char __e54, char __e53,
    char __e52, char __e51, char __e50, char __e49, char __e48, char __e47,
    char __e46, char __e45, char __e44, char __e43, char __e42, char __e41,
    char __e40, char __e39, char __e38, char __e37, char __e36, char __e35,
    char __e34, char __e33, char __e32, char __e31, char __e30, char __e29,
    char __e28, char __e27, char __e26, char __e25, char __e24, char __e23,
    char __e22, char __e21, char __e20, char __e19, char __e18, char __e17,
    char __e16, char __e15, char __e14, char __e13, char __e12, char __e11,
    char __e10, char __e9, char __e8, char __e7, char __e6, char __e5,
    char __e4, char __e3, char __e2, char __e1, char __e0) {

  return __extension__ (__m512i)(__v64qi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31,
     __e32, __e33, __e34, __e35, __e36, __e37, __e38, __e39,
     __e40, __e41, __e42, __e43, __e44, __e45, __e46, __e47,
     __e48, __e49, __e50, __e51, __e52, __e53, __e54, __e55,
     __e56, __e57, __e58, __e59, __e60, __e61, __e62, __e63};
}

static  __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_epi16(short __e31, short __e30, short __e29, short __e28,
    short __e27, short __e26, short __e25, short __e24, short __e23,
    short __e22, short __e21, short __e20, short __e19, short __e18,
    short __e17, short __e16, short __e15, short __e14, short __e13,
    short __e12, short __e11, short __e10, short __e9, short __e8,
    short __e7, short __e6, short __e5, short __e4, short __e3,
    short __e2, short __e1, short __e0) {
  return __extension__ (__m512i)(__v32hi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31 };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_epi32 (int __A, int __B, int __C, int __D,
     int __E, int __F, int __G, int __H,
     int __I, int __J, int __K, int __L,
     int __M, int __N, int __O, int __P)
{
  return __extension__ (__m512i)(__v16si)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}






static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_epi64 (long long __A, long long __B, long long __C,
     long long __D, long long __E, long long __F,
     long long __G, long long __H)
{
  return __extension__ (__m512i) (__v8di)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_pd (double __A, double __B, double __C, double __D,
        double __E, double __F, double __G, double __H)
{
  return __extension__ (__m512d)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_set_ps (float __A, float __B, float __C, float __D,
        float __E, float __F, float __G, float __H,
        float __I, float __J, float __K, float __L,
        float __M, float __N, float __O, float __P)
{
  return __extension__ (__m512)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}





static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_abs_ps(__m512 __A)
{
  return (__m512)_mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_abs_ps(__m512 __W, __mmask16 __K, __m512 __A)
{
  return (__m512)_mm512_mask_and_epi32((__m512i)__W, __K, _mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_abs_pd(__m512d __A)
{
  return (__m512d)_mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_abs_pd(__m512d __W, __mmask8 __K, __m512d __A)
{
  return (__m512d)_mm512_mask_and_epi64((__v8di)__W, __K, _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A);
}

// Vector-reduction arithmetic accepts vectors as inputs and produces scalars as
// outputs. This class of vector operation forms the basis of many scientific
// computations. In vector-reduction arithmetic, the evaluation off is
// independent of the order of the input elements of V.

// Used bisection method. At each step, we partition the vector with previous
// step in half, and the operation is performed on its two halves.
// This takes log2(n) steps where n is the number of elements in the vector.

// Vec512 - Vector with size 512.
// Operator - Can be one of following: +,*,&,|
// T2  - Can get 'i' for int and 'f' for float.
// T1 - Can get 'i' for int and 'd' for double.


# 9746 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_add_epi64(__m512i __W) {
  __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  + __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  + __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_mul_epi64(__m512i __W) {
  __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  * __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  * __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_and_epi64(__m512i __W) {
  __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  & __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  & __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  & __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_or_epi64(__m512i __W) {
  __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  | __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  | __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  | __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; });
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_add_pd(__m512d __W) {
  __extension__({ __m256## d Vec256 = __builtin_shufflevector( (__v8d## f)__W, (__v8d## f)__W, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8d## f)__W, (__v8d## f)__W, 4, 5, 6, 7); __m128## d Vec128 = __builtin_shufflevector( (__v4d## f)Vec256, (__v4d## f)Vec256, 0, 1)  + __builtin_shufflevector( (__v4d## f)Vec256, (__v4d## f)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## f)Vec128, (__v2d## f)Vec128, 0, -1)  + __builtin_shufflevector((__v2d## f)Vec128, (__v2d## f)Vec128, 1, -1); return Vec128[0]; });
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_mul_pd(__m512d __W) {
  __extension__({ __m256## d Vec256 = __builtin_shufflevector( (__v8d## f)__W, (__v8d## f)__W, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8d## f)__W, (__v8d## f)__W, 4, 5, 6, 7); __m128## d Vec128 = __builtin_shufflevector( (__v4d## f)Vec256, (__v4d## f)Vec256, 0, 1)  * __builtin_shufflevector( (__v4d## f)Vec256, (__v4d## f)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## f)Vec128, (__v2d## f)Vec128, 0, -1)  * __builtin_shufflevector((__v2d## f)Vec128, (__v2d## f)Vec128, 1, -1); return Vec128[0]; });
}

// Vec512 - Vector with size 512.
// Vec512Neutral - All vector elements set to the identity element.
// Identity element: {+,0},{*,1},{&,0xFFFFFFFFFFFFFFFF},{|,0}
// Operator - Can be one of following: +,*,&,|
// Mask - Intrinsic Mask
// T2  - Can get 'i' for int and 'f' for float.
// T1 - Can get 'i' for int and 'd' for packed double-precision.
// T3 - Can be Pd for packed double or q for q-word.


# 9789 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_add_epi64(__mmask8 __M, __m512i __W) {
  __extension__({ __W = __builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__W, (__v8d## i) _mm512_set1_epi64(0)); __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  + __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  + __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; }); });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_mul_epi64(__mmask8 __M, __m512i __W) {
  __extension__({ __W = __builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__W, (__v8d## i) _mm512_set1_epi64(1)); __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8d## i)__W, (__v8d## i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 0, 1)  * __builtin_shufflevector( (__v4d## i)Vec256, (__v4d## i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 0, -1)  * __builtin_shufflevector((__v2d## i)Vec128, (__v2d## i)Vec128, 1, -1); return Vec128[0]; }); });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_and_epi64(__mmask8 __M, __m512i __W) {
  __extension__({ __W = __builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d##  i)__W, (__v8d##  i) _mm512_set1_epi64(0xFFFFFFFFFFFFFFFF)); __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d##  i)__W, (__v8d##  i)__W, 0, 1, 2, 3) 
                                    
# 9802 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  & __builtin_shufflevector( (__v8d##  i)__W, (__v8d##  i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d##  i)Vec256, (__v4d##  i)Vec256, 0, 1) 
                                    
# 9802 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  & __builtin_shufflevector( (__v4d##  i)Vec256, (__v4d##  i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d##  i)Vec128, (__v2d##  i)Vec128, 0, -1) 
                                    
# 9802 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  & __builtin_shufflevector((__v2d##  i)Vec128, (__v2d##  i)Vec128, 1, -1); return Vec128[0]; }); });

}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_or_epi64(__mmask8 __M, __m512i __W) {
  __extension__({ __W = __builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i) _mm512_set1_epi64(0)); __extension__({ __m256## i Vec256 = __builtin_shufflevector( (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, 0, 1, 2, 3)  | __builtin_shufflevector( (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v8d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, 4, 5, 6, 7); __m128## i Vec128 = __builtin_shufflevector( (__v4d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, (__v4d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, 0, 1)  | __builtin_shufflevector( (__v4d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, (__v4d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v2d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 0, -1)  | __builtin_shufflevector((__v2d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v2d##
                                    
# 9808 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 1, -1); return Vec128[0]; }); });

}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_add_pd(__mmask8 __M, __m512d __W) {
  __extension__({ __W = __builtin_ia32_select## pd##_512( (__mmask8) __M, (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f) _mm512_set1_pd(0)); __extension__({ __m256## d Vec256 = __builtin_shufflevector( (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, 4, 5, 6, 7); __m128## d Vec128 = __builtin_shufflevector( (__v4d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, (__v4d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, 0, 1)  + __builtin_shufflevector( (__v4d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, (__v4d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, (__v2d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, 0, -1)  + __builtin_shufflevector((__v2d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, (__v2d##
                                    
# 9814 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, 1, -1); return Vec128[0]; }); });

}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_mul_pd(__mmask8 __M, __m512d __W) {
  __extension__({ __W = __builtin_ia32_select## pd##_512( (__mmask8) __M, (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f) _mm512_set1_pd(1)); __extension__({ __m256## d Vec256 = __builtin_shufflevector( (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, (__v8d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)__W, 4, 5, 6, 7); __m128## d Vec128 = __builtin_shufflevector( (__v4d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, (__v4d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, 0, 1)  * __builtin_shufflevector( (__v4d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, (__v4d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec256, 2, 3); Vec128 = __builtin_shufflevector((__v2d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, (__v2d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, 0, -1)  * __builtin_shufflevector((__v2d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, (__v2d##
                                    
# 9820 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  f)Vec128, 1, -1); return Vec128[0]; }); });

}

// Vec512 - Vector with size 512.
// Operator - Can be one of following: +,*,&,|
// T2 - Can get 'i' for int and ' ' for packed single.
// T1 - Can get 'i' for int and 'f' for float.


# 9870 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_add_epi32(__m512i __W) {
  __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  + __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  + __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  + __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_mul_epi32(__m512i __W) {
  __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  * __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  * __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  * __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_and_epi32(__m512i __W) {
  __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  & __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  & __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  & __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  & __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_or_epi32(__m512i __W) {
  __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  | __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  | __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  | __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  | __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_add_ps(__m512 __W) {
  __extension__({ __m256##  Vec256 = (__m256## )(__builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 0, 1, 2, 3, 4, 5, 6, 7)  + __builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128##  Vec128 = (__m128## )(__builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, 1, -1, -1)  + __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, -1, -1, -1)  + __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_mul_ps(__m512 __W) {
  __extension__({ __m256##  Vec256 = (__m256## )(__builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 0, 1, 2, 3, 4, 5, 6, 7)  * __builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128##  Vec128 = (__m128## )(__builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, 1, -1, -1)  * __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, -1, -1, -1)  * __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 1, -1, -1, -1)); return Vec128[0]; });
}

// Vec512 - Vector with size 512.
// Vec512Neutral - All vector elements set to the identity element.
// Identity element: {+,0},{*,1},{&,0xFFFFFFFF},{|,0}
// Operator - Can be one of following: +,*,&,|
// Mask - Intrinsic Mask
// T2  - Can get 'i' for int and 'f' for float.
// T1 - Can get 'i' for int and 'd' for double.
// T3 - Can be Ps for packed single or d for d-word.


# 9919 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_add_epi32( __mmask16 __M, __m512i __W) {
  __extension__({ __W = (__m512## i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__W, (__v16s## i) _mm512_set1_epi32(0)); __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  + __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  + __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  + __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_mul_epi32( __mmask16 __M, __m512i __W) {
  __extension__({ __W = (__m512## i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__W, (__v16s## i) _mm512_set1_epi32(1)); __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  * __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  * __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  * __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_and_epi32( __mmask16 __M, __m512i __W) {
  __extension__({ __W = (__m512## i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i) _mm512_set1_epi32(0xFFFFFFFF)); __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  & __builtin_shufflevector( (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, (__v16s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, (__v8s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, 0, 1, 2, 3)  & __builtin_shufflevector( (__v8s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, (__v8s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 0, 1, -1, -1)  & __builtin_shufflevector( (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 0, -1, -1, -1)  & __builtin_shufflevector( (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, (__v4s##
                                    
# 9932 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });

}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_or_epi32(__mmask16 __M, __m512i __W) {
  __extension__({ __W = (__m512## i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__W, (__v16s## i) _mm512_set1_epi32(0)); __extension__({ __m256## i Vec256 = (__m256## i)(__builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 0, 1, 2, 3, 4, 5, 6, 7)  | __builtin_shufflevector( (__v16s## i)__W, (__v16s## i)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128## i Vec128 = (__m128## i)(__builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 0, 1, 2, 3)  | __builtin_shufflevector( (__v8s## i)Vec256, (__v8s## i)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, 1, -1, -1)  | __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## i)(__builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 0, -1, -1, -1)  | __builtin_shufflevector( (__v4s## i)Vec128, (__v4s## i)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_add_ps(__mmask16 __M, __m512 __W) {
  __extension__({ __W = (__m512## )__builtin_ia32_select## ps##_512( (__mmask16) __M, (__v16s## f)__W, (__v16s## f) _mm512_set1_ps(0)); __extension__({ __m256##  Vec256 = (__m256## )(__builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 0, 1, 2, 3, 4, 5, 6, 7)  + __builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128##  Vec128 = (__m128## )(__builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 0, 1, 2, 3)  + __builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, 1, -1, -1)  + __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, -1, -1, -1)  + __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_mul_ps(__mmask16 __M, __m512 __W) {
  __extension__({ __W = (__m512## )__builtin_ia32_select## ps##_512( (__mmask16) __M, (__v16s## f)__W, (__v16s## f) _mm512_set1_ps(1)); __extension__({ __m256##  Vec256 = (__m256## )(__builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 0, 1, 2, 3, 4, 5, 6, 7)  * __builtin_shufflevector( (__v16s## f)__W, (__v16s## f)__W, 8, 9, 10, 11, 12, 13, 14, 15)); __m128##  Vec128 = (__m128## )(__builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 0, 1, 2, 3)  * __builtin_shufflevector( (__v8s## f)Vec256, (__v8s## f)Vec256, 4, 5, 6, 7)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, 1, -1, -1)  * __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 2, 3, -1, -1)); Vec128 = (__m128## )(__builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 0, -1, -1, -1)  * __builtin_shufflevector( (__v4s## f)Vec128, (__v4s## f)Vec128, 1, -1, -1, -1)); return Vec128[0]; }); });
}

// Used bisection method. At each step, we partition the vector with previous
// step in half, and the operation is performed on its two halves.
// This takes log2(n) steps where n is the number of elements in the vector.
// This macro uses only intrinsics from the AVX512F feature.

// Vec512 - Vector with size of 512.
// IntrinName - Can be one of following: {max|min}_{epi64|epu64|pd} for example:
//              __mm512_max_epi64
// T1 - Can get 'i' for int and 'd' for double.[__m512{i|d}]
// T2 - Can get 'i' for int and 'f' for float. [__v8d{i|f}]


# 9994 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_max_epi64(__m512i __V) {
  __extension__({ __V = _mm512_## max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_max_epu64(__m512i __V) {
  __extension__({ __V = _mm512_## max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_max_pd(__m512d __V) {
  __extension__({ __V = _mm512_## max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_min_epi64
(__m512i __V) {
  __extension__({ __V = _mm512_## min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_min_epu64(__m512i __V) {
  __extension__({ __V = _mm512_## min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_min_pd(__m512d __V) {
  __extension__({ __V = _mm512_## min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_## min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; });
}

// Vec512 - Vector with size 512.
// Vec512Neutral - A 512 length vector with elements set to the identity element
// Identity element: {max_epi,0x8000000000000000}
//                   {max_epu,0x0000000000000000}
//                   {max_pd, 0xFFF0000000000000}
//                   {min_epi,0x7FFFFFFFFFFFFFFF}
//                   {min_epu,0xFFFFFFFFFFFFFFFF}
//                   {min_pd, 0x7FF0000000000000}
//
// IntrinName - Can be one of following: {max|min}_{epi64|epu64|pd} for example:
//              __mm512_max_epi64
// T1 - Can get 'i' for int and 'd' for double.[__m512{i|d}]
// T2 - Can get 'i' for int and 'f' for float. [__v8d{i|f}]
// T3 - Can get 'q' q word and 'pd' for packed double.
//      [__builtin_ia32_select{q|pd}_512]
// Mask - Intrinsic Mask


# 10051 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_epi64(__mmask8 __M, __m512i __V) {
  __extension__({ __V = (__m512## i)__builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__V, (__v8d## i) _mm512_set1_epi64(0x8000000000000000)); __extension__({ __V = _mm512_##
                                  
# 10054 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10054 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10054 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_epu64(__mmask8 __M, __m512i __V) {
  __extension__({ __V = (__m512## i)__builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__V, (__v8d## i) _mm512_set1_epi64(0x0000000000000000)); __extension__({ __V = _mm512_##
                                  
# 10060 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10060 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10060 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_pd(__mmask8 __M, __m512d __V) {
  __extension__({ __V = (__m512## d)__builtin_ia32_select## pd##_512( (__mmask8) __M, (__v8d## f)__V, (__v8d## f) -_mm512_set1_pd(__builtin_inf())); __extension__({ __V = _mm512_##
                                  
# 10066 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10066 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10066 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  max_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_epi64(__mmask8 __M, __m512i __V) {
  __extension__({ __V = (__m512## i)__builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__V, (__v8d## i) _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF)); __extension__({ __V = _mm512_##
                                  
# 10072 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10072 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10072 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epi64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_epu64(__mmask8 __M, __m512i __V) {
  __extension__({ __V = (__m512## i)__builtin_ia32_select## q##_512( (__mmask8) __M, (__v8d## i)__V, (__v8d## i) _mm512_set1_epi64(0xFFFFFFFFFFFFFFFF)); __extension__({ __V = _mm512_##
                                  
# 10078 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10078 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10078 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_epu64( (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v8d## i)__V, (__v8d## i)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_pd(__mmask8 __M, __m512d __V) {
  __extension__({ __V = (__m512## d)__builtin_ia32_select## pd##_512( (__mmask8) __M, (__v8d## f)__V, (__v8d## f) _mm512_set1_pd(__builtin_inf())); __extension__({ __V = _mm512_##
                                  
# 10084 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, 2, 3, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 4, 5, 6, 7, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10084 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, 1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 2, 3, -1, -1, -1, -1, -1, -1)); __V = _mm512_##
                                  
# 10084 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  min_pd( (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 0, -1, -1, -1, -1, -1, -1, -1), (__m512## d)__builtin_shufflevector( (__v8d## f)__V, (__v8d## f)__V, 1, -1, -1, -1, -1, -1, -1, -1)) ; return __V[0]; }); });

}

// Vec512 - Vector with size 512.
// IntrinName - Can be one of following: {max|min}_{epi32|epu32|ps} for example:
//              __mm512_max_epi32
// T1 - Can get 'i' for int and ' ' .[__m512{i|}]
// T2 - Can get 'i' for int and 'f' for float.[__v16s{i|f}]


# 10141 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_max_epi32(__m512i a) {
  __extension__({ a = _mm512_## max_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_max_epu32(__m512i a) {
  __extension__({ a = _mm512_## max_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_max_ps(__m512 a) {
  __extension__({ a = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_min_epi32(__m512i a) {
  __extension__({ a = _mm512_## min_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epi32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_reduce_min_epu32(__m512i a) {
  __extension__({ a = _mm512_## min_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_epu32( (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## i)__builtin_shufflevector( (__v16s## i)a, (__v16s## i)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"))) _mm512_reduce_min_ps(__m512 a) {
  __extension__({ a = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); a = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)a, (__v16s## f)a, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return a[0]; });
}

// Vec512 - Vector with size 512.
// Vec512Neutral - A 512 length vector with elements set to the identity element
// Identity element: {max_epi,0x80000000}
//                   {max_epu,0x00000000}
//                   {max_ps, 0xFF800000}
//                   {min_epi,0x7FFFFFFF}
//                   {min_epu,0xFFFFFFFF}
//                   {min_ps, 0x7F800000}
//
// IntrinName - Can be one of following: {max|min}_{epi32|epu32|ps} for example:
//              __mm512_max_epi32
// T1 - Can get 'i' for int and ' ' .[__m512{i|}]
// T2 - Can get 'i' for int and 'f' for float.[__v16s{i|f}]
// T3 - Can get 'q' q word and 'pd' for packed double.
//      [__builtin_ia32_select{q|pd}_512]
// Mask - Intrinsic Mask


# 10194 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_epi32(__mmask16 __M, __m512i __V) {
  __extension__({ __V = (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__V, (__v16s## i) _mm512_set1_epi32(0x80000000)); __extension__({ __V = _mm512_## max_epi32( (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epi32( (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epi32( (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epi32( (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10197 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_epu32(__mmask16 __M, __m512i __V) {
  __extension__({ __V = (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__V, (__v16s## i) _mm512_set1_epi32(0x00000000)); __extension__({ __V = _mm512_## max_epu32( (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epu32( (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epu32( (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_epu32( (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10203 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_max_ps(__mmask16 __M, __m512 __V) {
  __extension__({ __V = (__m512## )__builtin_ia32_select##
                                  
# 10209 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  ps##_512( (__mmask16) __M, (__v16s## f)__V, (__v16s## f)-_mm512_set1_ps(__builtin_inff())); __extension__({ __V = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## max_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_epi32(__mmask16 __M, __m512i __V) {
  __extension__({ __V = (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__V, (__v16s## i) _mm512_set1_epi32(0x7FFFFFFF)); __extension__({ __V = _mm512_## min_epi32( (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epi32( (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epi32( (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epi32( (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_epu32(__mmask16 __M, __m512i __V) {
  __extension__({ __V = (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_ia32_select## d##_512( (__mmask16) __M, (__v16s## i)__V, (__v16s## i) _mm512_set1_epi32(0xFFFFFFFF)); __extension__({ __V = _mm512_## min_epu32( (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epu32( (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epu32( (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_epu32( (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512##
                                  
# 10221 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  i)__builtin_shufflevector( (__v16s## i)__V, (__v16s## i)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask_reduce_min_ps(__mmask16 __M, __m512 __V) {
  __extension__({ __V = (__m512## )__builtin_ia32_select##
                                  
# 10227 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512fintrin.h" 3 4
  ps##_512( (__mmask16) __M, (__v16s## f)__V, (__v16s## f) _mm512_set1_ps(__builtin_inff())); __extension__({ __V = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 8, 9, 10, 11, 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); __V = _mm512_## min_ps( (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1), (__m512## )__builtin_shufflevector( (__v16s## f)__V, (__v16s## f)__V, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1)); return __V[0]; }); });

}




# 143 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4












static  __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm_setzero_di(void) {
  return (__m128i)(__v2di){ 0LL, 0LL};
}




# 65 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 90 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 115 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 140 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 165 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 190 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 215 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4


# 240 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_add_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_add_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_add_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_add_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sub_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sub_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sub_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sub_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_add_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_add_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_add_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_add_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sub_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sub_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sub_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sub_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mul_epi32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mul_epi32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mul_epi32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mul_epi32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mul_epu32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mul_epu32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mul_epu32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mul_epu32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mullo_epi32(__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mullo_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mullo_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mullo_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_and_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_and_si256(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_and_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_and_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_and_si128(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_and_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_andnot_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                          (__v8si)_mm256_andnot_si256(__A, __B),
                                          (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_andnot_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi32(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_andnot_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_andnot_si128(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_andnot_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_or_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_or_si256(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_or_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_or_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_or_si128(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_or_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_xor_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_xor_si256(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_xor_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_xor_epi32(__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_xor_si128(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_xor_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_and_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_and_si256(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_and_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_and_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_and_si128(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_and_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_andnot_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                          (__v4di)_mm256_andnot_si256(__A, __B),
                                          (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_andnot_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi64(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_andnot_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_andnot_si128(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_andnot_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_or_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_or_si256(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_or_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_or_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_or_si128(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_or_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_xor_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_xor_si256(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_xor_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_xor_epi64(__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_xor_si128(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_xor_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi64(_mm_setzero_si128(), __U, __A, __B);
}

























































































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask ((__v2df) __A,
                                                    (__v2df) __B,
                                                    (__v2df) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask3 ((__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_maskz ((__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask ((__v2df) __A,
                                                    (__v2df) __B,
                                                    -(__v2df) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_maskz ((__v2df) __A,
                                                     (__v2df) __B,
                                                     -(__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_mask3 (-(__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fnmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_maskz (-(__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fnmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddpd128_maskz (-(__v2df) __A,
                                                     (__v2df) __B,
                                                     -(__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask ((__v4df) __A,
                                                    (__v4df) __B,
                                                    (__v4df) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask3 ((__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_maskz ((__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask ((__v4df) __A,
                                                    (__v4df) __B,
                                                    -(__v4df) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_maskz ((__v4df) __A,
                                                     (__v4df) __B,
                                                     -(__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_mask3 (-(__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fnmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_maskz (-(__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fnmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddpd256_maskz (-(__v4df) __A,
                                                     (__v4df) __B,
                                                     -(__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask ((__v4sf) __A,
                                                   (__v4sf) __B,
                                                   (__v4sf) __C,
                                                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask3 ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_maskz ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask ((__v4sf) __A,
                                                   (__v4sf) __B,
                                                   -(__v4sf) __C,
                                                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_maskz ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    -(__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddps128_mask3 (-(__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fnmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_maskz (-(__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fnmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddps128_maskz (-(__v4sf) __A,
                                                    (__v4sf) __B,
                                                    -(__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C,
                                                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask3 ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_maskz ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C,
                                                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_maskz ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    -(__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmaddps256_mask3 (-(__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fnmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_maskz (-(__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fnmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddps256_maskz (-(__v8sf) __A,
                                                    (__v8sf) __B,
                                                    -(__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmaddsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask ((__v2df) __A,
                                                       (__v2df) __B,
                                                       (__v2df) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask3 ((__v2df) __A,
                                                        (__v2df) __B,
                                                        (__v2df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmaddsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_maskz ((__v2df) __A,
                                                        (__v2df) __B,
                                                        (__v2df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmsubadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_mask ((__v2df) __A,
                                                       (__v2df) __B,
                                                       -(__v2df) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmsubadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfmaddsubpd128_maskz ((__v2df) __A,
                                                        (__v2df) __B,
                                                        -(__v2df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmaddsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask ((__v4df) __A,
                                                       (__v4df) __B,
                                                       (__v4df) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask3 ((__v4df) __A,
                                                        (__v4df) __B,
                                                        (__v4df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmaddsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_maskz ((__v4df) __A,
                                                        (__v4df) __B,
                                                        (__v4df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmsubadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_mask ((__v4df) __A,
                                                       (__v4df) __B,
                                                       -(__v4df) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmsubadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfmaddsubpd256_maskz ((__v4df) __A,
                                                        (__v4df) __B,
                                                        -(__v4df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmaddsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask ((__v4sf) __A,
                                                      (__v4sf) __B,
                                                      (__v4sf) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask3 ((__v4sf) __A,
                                                       (__v4sf) __B,
                                                       (__v4sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmaddsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_maskz ((__v4sf) __A,
                                                       (__v4sf) __B,
                                                       (__v4sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fmsubadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_mask ((__v4sf) __A,
                                                      (__v4sf) __B,
                                                      -(__v4sf) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_fmsubadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfmaddsubps128_maskz ((__v4sf) __A,
                                                       (__v4sf) __B,
                                                       -(__v4sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmaddsub_ps(__m256 __A, __mmask8 __U, __m256 __B,
                         __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask ((__v8sf) __A,
                                                      (__v8sf) __B,
                                                      (__v8sf) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask3 ((__v8sf) __A,
                                                       (__v8sf) __B,
                                                       (__v8sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmaddsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_maskz ((__v8sf) __A,
                                                       (__v8sf) __B,
                                                       (__v8sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fmsubadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_mask ((__v8sf) __A,
                                                      (__v8sf) __B,
                                                      -(__v8sf) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_fmsubadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfmaddsubps256_maskz ((__v8sf) __A,
                                                       (__v8sf) __B,
                                                       -(__v8sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubpd128_mask3 ((__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmsubpd256_mask3 ((__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubps128_mask3 ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmsubps256_mask3 ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfmsubaddpd128_mask3 ((__v2df) __A,
                                                        (__v2df) __B,
                                                        (__v2df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfmsubaddpd256_mask3 ((__v4df) __A,
                                                        (__v4df) __B,
                                                        (__v4df) __C,
                                                        (__mmask8)
                                                        __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfmsubaddps128_mask3 ((__v4sf) __A,
                                                       (__v4sf) __B,
                                                       (__v4sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfmsubaddps256_mask3 ((__v8sf) __A,
                                                       (__v8sf) __B,
                                                       (__v8sf) __C,
                                                       (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fnmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmaddpd128_mask ((__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fnmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmaddpd256_mask ((__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fnmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmaddps128_mask ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fnmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmaddps256_mask ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fnmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_vfnmsubpd128_mask ((__v2df) __A,
                                                     (__v2df) __B,
                                                     (__v2df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_vfnmsubpd128_mask3 ((__v2df) __A,
                                                      (__v2df) __B,
                                                      (__v2df) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fnmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_vfnmsubpd256_mask ((__v4df) __A,
                                                     (__v4df) __B,
                                                     (__v4df) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_vfnmsubpd256_mask3 ((__v4df) __A,
                                                      (__v4df) __B,
                                                      (__v4df) __C,
                                                      (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_fnmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_vfnmsubps128_mask ((__v4sf) __A,
                                                    (__v4sf) __B,
                                                    (__v4sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask3_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_vfnmsubps128_mask3 ((__v4sf) __A,
                                                     (__v4sf) __B,
                                                     (__v4sf) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_fnmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_vfnmsubps256_mask ((__v8sf) __A,
                                                    (__v8sf) __B,
                                                    (__v8sf) __C,
                                                    (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask3_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_vfnmsubps256_mask3 ((__v8sf) __A,
                                                     (__v8sf) __B,
                                                     (__v8sf) __C,
                                                     (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_add_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_add_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_add_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_add_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_add_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_add_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_add_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_add_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_blend_epi32 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                (__v4si) __W,
                (__v4si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_blend_epi32 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                (__v8si) __W,
                (__v8si) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_blend_pd (__mmask8 __U, __m128d __A, __m128d __W) {
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
                 (__v2df) __W,
                 (__v2df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_blend_pd (__mmask8 __U, __m256d __A, __m256d __W) {
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
                 (__v4df) __W,
                 (__v4df) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_blend_ps (__mmask8 __U, __m128 __A, __m128 __W) {
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
                (__v4sf) __W,
                (__v4sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_blend_ps (__mmask8 __U, __m256 __A, __m256 __W) {
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
                (__v8sf) __W,
                (__v8sf) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_blend_epi64 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                (__v2di) __W,
                (__v2di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_blend_epi64 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                (__v4di) __W,
                (__v4di) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compress_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df) __W,
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_compress_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df)
                  _mm_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compress_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df) __W,
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_compress_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df)
                  _mm256_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compress_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_compress_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compress_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_compress_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compress_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_compress_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compress_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_compress_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf)
                 _mm256_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compress_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_compress_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compress_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_compress_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m128d __A) {
  __builtin_ia32_compressstoredf128_mask ((__v2df *) __P,
            (__v2df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m256d __A) {
  __builtin_ia32_compressstoredf256_mask ((__v4df *) __P,
            (__v4df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoredi128_mask ((__v2di *) __P,
            (__v2di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoredi256_mask ((__v4di *) __P,
            (__v4di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m128 __A) {
  __builtin_ia32_compressstoresf128_mask ((__v4sf *) __P,
            (__v4sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m256 __A) {
  __builtin_ia32_compressstoresf256_mask ((__v8sf *) __P,
            (__v8sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoresi128_mask ((__v4si *) __P,
            (__v4si) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoresi256_mask ((__v8si *) __P,
            (__v8si) __A,
            (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtdq2ps128_mask ((__v4si) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi32_ps (__mmask16 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtdq2ps128_mask ((__v4si) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256) __builtin_ia32_cvtdq2ps256_mask ((__v8si) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi32_ps (__mmask16 __U, __m256i __A) {
  return (__m256) __builtin_ia32_cvtdq2ps256_mask ((__v8si) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si)
                _mm_setzero_si128 (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq256_mask ((__v4df) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq256_mask ((__v4df) __A,
                (__v4si)
                _mm_setzero_si128 (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf) __W,
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtpd_ps (__mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m256d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps256_mask ((__v4df) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtpd_ps (__mmask8 __U, __m256d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps256_mask ((__v4df) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2dq128_mask ((__v4sf) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2dq128_mask ((__v4sf) __A,
                (__v4si)
                _mm_setzero_si128 (),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2dq256_mask ((__v8sf) __A,
                (__v8si) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2dq256_mask ((__v8sf) __A,
                (__v8si)
                _mm256_setzero_si256 (),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtps_pd (__m128d __W, __mmask8 __U, __m128 __A) {
  return (__m128d) __builtin_ia32_cvtps2pd128_mask ((__v4sf) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m128d) __builtin_ia32_cvtps2pd128_mask ((__v4sf) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtps_pd (__m256d __W, __mmask8 __U, __m128 __A) {
  return (__m256d) __builtin_ia32_cvtps2pd256_mask ((__v4sf) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m256d) __builtin_ia32_cvtps2pd256_mask ((__v4sf) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvttpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq256_mask ((__v4df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvttpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvttpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvttpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvttpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvttpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvttps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2dq128_mask ((__v4sf) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvttps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2dq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvttps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2dq256_mask ((__v8sf) __A,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvttps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2dq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvttps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvttps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvttps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvttps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvttps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvttps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepu32_pd (__m128i __A) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4su)__A, (__v4su)__A, 0, 1), __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepu32_pd (__m128i __A) {
  return (__m256d)__builtin_convertvector((__v4su)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepu32_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu32_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtudq2ps128_mask ((__v4si) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepu32_ps (__m256i __A) {
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu32_ps (__mmask8 __U, __m256i __A) {
  return (__m256) __builtin_ia32_cvtudq2ps256_mask ((__v8si) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_div_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_div_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_div_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_div_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_div_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_div_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_div_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_div_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expand_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expand_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expand_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expand_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expand_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expand_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expand_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expand_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expandloadu_pd (__m128d __W, __mmask8 __U, void __const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((__v2df *) __P,
              (__v2df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expandloadu_pd (__mmask8 __U, void __const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((__v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expandloadu_pd (__m256d __W, __mmask8 __U, void __const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((__v4df *) __P,
              (__v4df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expandloadu_pd (__mmask8 __U, void __const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((__v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expandloadu_epi64 (__m128i __W, __mmask8 __U, void __const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((__v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expandloadu_epi64 (__mmask8 __U, void __const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((__v2di *) __P,
               (__v2di)
               _mm_setzero_si128 (),
               (__mmask8)
               __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expandloadu_epi64 (__m256i __W, __mmask8 __U,
             void __const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((__v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expandloadu_epi64 (__mmask8 __U, void __const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((__v4di *) __P,
               (__v4di)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expandloadu_ps (__m128 __W, __mmask8 __U, void __const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((__v4sf *) __P,
                   (__v4sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expandloadu_ps (__mmask8 __U, void __const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((__v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expandloadu_ps (__m256 __W, __mmask8 __U, void __const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((__v8sf *) __P,
                   (__v8sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expandloadu_ps (__mmask8 __U, void __const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((__v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expandloadu_epi32 (__m128i __W, __mmask8 __U, void __const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((__v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expandloadu_epi32 (__mmask8 __U, void __const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((__v4si *) __P,
               (__v4si)
               _mm_setzero_si128 (),
               (__mmask8)     __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expandloadu_epi32 (__m256i __W, __mmask8 __U,
             void __const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((__v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expandloadu_epi32 (__mmask8 __U, void __const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((__v8si *) __P,
               (__v8si)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expand_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expand_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expand_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expand_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_expand_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_expand_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_expand_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                (__v8si) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_expand_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_getexp_pd (__m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_getexp_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_getexp_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_getexp_pd (__m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_getexp_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_getexp_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_getexp_ps (__m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_getexp_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_getexp_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_getexp_ps (__m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_getexp_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_getexp_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mul_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mul_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mul_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mul_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mul_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mul_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mul_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mul_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_abs_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_abs_epi32(__mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_abs_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask16)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_abs_epi32(__mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask16)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_abs_epi64 (__m128i __A) {
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
             (__v2di)
             _mm_setzero_si128 (),
             (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_abs_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
             (__v2di) __W,
             (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_abs_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_pabsq128_mask ((__v2di) __A,
             (__v2di)
             _mm_setzero_si128 (),
             (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_abs_epi64 (__m256i __A) {
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
             (__v4di)
             _mm256_setzero_si256 (),
             (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_abs_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
             (__v4di) __W,
             (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_abs_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_pabsq256_mask ((__v4di) __A,
             (__v4di)
             _mm256_setzero_si256 (),
             (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_epi64 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_max_epi64 (__m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_epi64 (__m256i __W, __mmask8 __M, __m256i __A,
           __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_max_epi64 (__m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_max_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_max_epu64 (__m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_max_epu64 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B) {
  return (__m128i) __builtin_ia32_pmaxuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_max_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_max_epu64 (__m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_max_epu64 (__m256i __W, __mmask8 __M, __m256i __A,
           __m256i __B) {
  return (__m256i) __builtin_ia32_pmaxuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_min_epi64 (__m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_epi64 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B) {
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pminsq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_min_epi64 (__m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_epi64 (__m256i __W, __mmask8 __M, __m256i __A,
           __m256i __B) {
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pminsq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_min_epu64 (__m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_min_epu64 (__m128i __W, __mmask8 __M, __m128i __A,
        __m128i __B) {
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_min_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i) __builtin_ia32_pminuq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_min_epu64 (__m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_min_epu64 (__m256i __W, __mmask8 __M, __m256i __A,
           __m256i __B) {
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_min_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i) __builtin_ia32_pminuq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              __M);
}











































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_scalef_pd (__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_scalef_pd (__m128d __W, __mmask8 __U, __m128d __A,
        __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_scalef_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_scalef_pd (__m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_scalef_pd (__m256d __W, __mmask8 __U, __m256d __A,
           __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_scalef_pd (__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_scalef_ps (__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_scalef_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_scalef_ps (__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_scalef_ps (__m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_scalef_ps (__m256 __W, __mmask8 __U, __m256 __A,
           __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_scalef_ps (__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}

































































































































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sqrt_pd(__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_sqrt_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sqrt_pd(__mmask8 __U, __m128d __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_sqrt_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sqrt_pd(__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_sqrt_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sqrt_pd(__mmask8 __U, __m256d __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_sqrt_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sqrt_ps(__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_sqrt_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sqrt_ps(__mmask8 __U, __m128 __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_sqrt_ps(__A),
                                             (__v4sf)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sqrt_ps(__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_sqrt_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sqrt_ps(__mmask8 __U, __m256 __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_sqrt_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sub_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_sub_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sub_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_sub_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sub_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_sub_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sub_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_sub_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sub_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_sub_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sub_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_sub_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sub_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_sub_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sub_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_sub_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask2_permutex2var_epi32 (__m128i __A, __m128i __I, __mmask8 __U,
            __m128i __B) {
  return (__m128i) __builtin_ia32_vpermi2vard128_mask ((__v4si) __A,
                   (__v4si) __I
                    ,
                   (__v4si) __B,
                   (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask2_permutex2var_epi32 (__m256i __A, __m256i __I,
         __mmask8 __U, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermi2vard256_mask ((__v8si) __A,
                   (__v8si) __I
                    ,
                   (__v8si) __B,
                   (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask2_permutex2var_pd (__m128d __A, __m128i __I, __mmask8 __U,
         __m128d __B) {
  return (__m128d) __builtin_ia32_vpermi2varpd128_mask ((__v2df) __A,
              (__v2di) __I
               ,
              (__v2df) __B,
              (__mmask8)
              __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask2_permutex2var_pd (__m256d __A, __m256i __I, __mmask8 __U,
            __m256d __B) {
  return (__m256d) __builtin_ia32_vpermi2varpd256_mask ((__v4df) __A,
              (__v4di) __I
               ,
              (__v4df) __B,
              (__mmask8)
              __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask2_permutex2var_ps (__m128 __A, __m128i __I, __mmask8 __U,
         __m128 __B) {
  return (__m128) __builtin_ia32_vpermi2varps128_mask ((__v4sf) __A,
                   (__v4si) __I
                    ,
                   (__v4sf) __B,
                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask2_permutex2var_ps (__m256 __A, __m256i __I, __mmask8 __U,
            __m256 __B) {
  return (__m256) __builtin_ia32_vpermi2varps256_mask ((__v8sf) __A,
                   (__v8si) __I
                    ,
                   (__v8sf) __B,
                   (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask2_permutex2var_epi64 (__m128i __A, __m128i __I, __mmask8 __U,
            __m128i __B) {
  return (__m128i) __builtin_ia32_vpermi2varq128_mask ((__v2di) __A,
                   (__v2di) __I
                    ,
                   (__v2di) __B,
                   (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask2_permutex2var_epi64 (__m256i __A, __m256i __I,
         __mmask8 __U, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermi2varq256_mask ((__v4di) __A,
                   (__v4di) __I
                    ,
                   (__v4di) __B,
                   (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_permutex2var_epi32 (__m128i __A, __m128i __I, __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2vard128_mask ((__v4si) __I
                    ,
                   (__v4si) __A,
                   (__v4si) __B,
                   (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutex2var_epi32 (__m128i __A, __mmask8 __U, __m128i __I,
           __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2vard128_mask ((__v4si) __I
                    ,
                   (__v4si) __A,
                   (__v4si) __B,
                   (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutex2var_epi32 (__mmask8 __U, __m128i __A, __m128i __I,
            __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2vard128_maskz ((__v4si) __I
               ,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutex2var_epi32 (__m256i __A, __m256i __I, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2vard256_mask ((__v8si) __I
                    ,
                   (__v8si) __A,
                   (__v8si) __B,
                   (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutex2var_epi32 (__m256i __A, __mmask8 __U, __m256i __I,
        __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2vard256_mask ((__v8si) __I
                    ,
                   (__v8si) __A,
                   (__v8si) __B,
                   (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutex2var_epi32 (__mmask8 __U, __m256i __A,
         __m256i __I, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2vard256_maskz ((__v8si) __I
               ,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8)
              __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_permutex2var_pd (__m128d __A, __m128i __I, __m128d __B) {
  return (__m128d) __builtin_ia32_vpermt2varpd128_mask ((__v2di) __I
               ,
              (__v2df) __A,
              (__v2df) __B,
              (__mmask8) -
              1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutex2var_pd (__m128d __A, __mmask8 __U, __m128i __I,
        __m128d __B) {
  return (__m128d) __builtin_ia32_vpermt2varpd128_mask ((__v2di) __I
               ,
              (__v2df) __A,
              (__v2df) __B,
              (__mmask8)
              __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutex2var_pd (__mmask8 __U, __m128d __A, __m128i __I,
         __m128d __B) {
  return (__m128d) __builtin_ia32_vpermt2varpd128_maskz ((__v2di) __I
                ,
               (__v2df) __A,
               (__v2df) __B,
               (__mmask8)
               __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutex2var_pd (__m256d __A, __m256i __I, __m256d __B) {
  return (__m256d) __builtin_ia32_vpermt2varpd256_mask ((__v4di) __I
               ,
              (__v4df) __A,
              (__v4df) __B,
              (__mmask8) -
              1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutex2var_pd (__m256d __A, __mmask8 __U, __m256i __I,
           __m256d __B) {
  return (__m256d) __builtin_ia32_vpermt2varpd256_mask ((__v4di) __I
               ,
              (__v4df) __A,
              (__v4df) __B,
              (__mmask8)
              __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutex2var_pd (__mmask8 __U, __m256d __A, __m256i __I,
            __m256d __B) {
  return (__m256d) __builtin_ia32_vpermt2varpd256_maskz ((__v4di) __I
                ,
               (__v4df) __A,
               (__v4df) __B,
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_permutex2var_ps (__m128 __A, __m128i __I, __m128 __B) {
  return (__m128) __builtin_ia32_vpermt2varps128_mask ((__v4si) __I
                    ,
                   (__v4sf) __A,
                   (__v4sf) __B,
                   (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutex2var_ps (__m128 __A, __mmask8 __U, __m128i __I,
        __m128 __B) {
  return (__m128) __builtin_ia32_vpermt2varps128_mask ((__v4si) __I
                    ,
                   (__v4sf) __A,
                   (__v4sf) __B,
                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutex2var_ps (__mmask8 __U, __m128 __A, __m128i __I,
         __m128 __B) {
  return (__m128) __builtin_ia32_vpermt2varps128_maskz ((__v4si) __I
               ,
              (__v4sf) __A,
              (__v4sf) __B,
              (__mmask8)
              __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutex2var_ps (__m256 __A, __m256i __I, __m256 __B) {
  return (__m256) __builtin_ia32_vpermt2varps256_mask ((__v8si) __I
                    ,
                   (__v8sf) __A,
                   (__v8sf) __B,
                   (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutex2var_ps (__m256 __A, __mmask8 __U, __m256i __I,
           __m256 __B) {
  return (__m256) __builtin_ia32_vpermt2varps256_mask ((__v8si) __I
                    ,
                   (__v8sf) __A,
                   (__v8sf) __B,
                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutex2var_ps (__mmask8 __U, __m256 __A, __m256i __I,
            __m256 __B) {
  return (__m256) __builtin_ia32_vpermt2varps256_maskz ((__v8si) __I
               ,
              (__v8sf) __A,
              (__v8sf) __B,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_permutex2var_epi64 (__m128i __A, __m128i __I, __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2varq128_mask ((__v2di) __I
                    ,
                   (__v2di) __A,
                   (__v2di) __B,
                   (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutex2var_epi64 (__m128i __A, __mmask8 __U, __m128i __I,
           __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2varq128_mask ((__v2di) __I
                    ,
                   (__v2di) __A,
                   (__v2di) __B,
                   (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutex2var_epi64 (__mmask8 __U, __m128i __A, __m128i __I,
            __m128i __B) {
  return (__m128i) __builtin_ia32_vpermt2varq128_maskz ((__v2di) __I
               ,
              (__v2di) __A,
              (__v2di) __B,
              (__mmask8)
              __U);
}


static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutex2var_epi64 (__m256i __A, __m256i __I, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2varq256_mask ((__v4di) __I
                    ,
                   (__v4di) __A,
                   (__v4di) __B,
                   (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutex2var_epi64 (__m256i __A, __mmask8 __U, __m256i __I,
        __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2varq256_mask ((__v4di) __I
                    ,
                   (__v4di) __A,
                   (__v4di) __B,
                   (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutex2var_epi64 (__mmask8 __U, __m256i __A,
         __m256i __I, __m256i __B) {
  return (__m256i) __builtin_ia32_vpermt2varq256_maskz ((__v4di) __I
               ,
              (__v4di) __A,
              (__v4di) __B,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepi8_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi8_epi32(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepi8_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi8_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepi8_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi8_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepi8_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi8_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi8_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi8_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi8_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi32_epi64(__X),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi32_epi64(__X),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi32_epi64(__X),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi32_epi64(__X),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepi16_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi16_epi32(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepi16_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepi16_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi16_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepi16_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi16_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepi16_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi16_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepi16_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepu8_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepu8_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu8_epi32(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepu8_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepu8_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu8_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu8_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu8_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu8_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu32_epi64(__X),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu32_epi64(__X),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu32_epi64(__X),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu32_epi64(__X),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepu16_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtepu16_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepu16_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtepu16_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepu16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu16_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_cvtepu16_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepu16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu16_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_cvtepu16_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}


























































static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rolv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rolv_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si) __W,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rolv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rolv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rolv_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si) __W,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rolv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rolv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_di (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rolv_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rolv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prolvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_di (),
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rolv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rolv_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rolv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prolvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) __U);
}

























































static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sll_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sll_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sll_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sll_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_slli_epi32(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_slli_epi32(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_slli_epi32(__m256i __W, __mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_slli_epi32(__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sll_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sll_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sll_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sll_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_slli_epi64(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_slli_epi64(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, __B),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_slli_epi64(__m256i __W, __mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_slli_epi64(__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rorv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rorv_epi32 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si) __W,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rorv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvd128_mask ((__v4si) __A,
              (__v4si) __B,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rorv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rorv_epi32 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si) __W,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rorv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvd256_mask ((__v8si) __A,
              (__v8si) __B,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rorv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_di (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rorv_epi64 (__m128i __W, __mmask8 __U, __m128i __A,
         __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di) __W,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rorv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_prorvq128_mask ((__v2di) __A,
              (__v2di) __B,
              (__v2di)
              _mm_setzero_di (),
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rorv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rorv_epi64 (__m256i __W, __mmask8 __U, __m256i __A,
      __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di) __W,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rorv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_prorvq256_mask ((__v4di) __A,
              (__v4di) __B,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sllv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sllv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sllv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sllv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sllv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sllv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sllv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sllv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srlv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srlv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srlv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srlv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srlv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srlv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srlv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srlv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srl_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srl_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srl_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srl_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srli_epi32(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srli_epi32(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srli_epi32(__m256i __W, __mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srli_epi32(__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srl_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srl_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srl_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srl_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srli_epi64(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srli_epi64(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, __B),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srli_epi64(__m256i __W, __mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srli_epi64(__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srav_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srav_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srav_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srav_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_srav_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psravq128((__v2di)__X, (__v2di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srav_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srav_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_srav_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psravq256((__v4di)__X, (__v4di) __Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srav_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srav_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mov_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mov_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) _mm_setzero_si128 ());
}


static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mov_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mov_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) _mm256_setzero_si256 ());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_load_epi32 (__m128i __W, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((__v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_load_epi32 (__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((__v4si *) __P,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_load_epi32 (__m256i __W, __mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((__v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_load_epi32 (__mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((__v8si *) __P,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_store_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa32store128_mask ((__v4si *) __P,
          (__v4si) __A,
          (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_store_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa32store256_mask ((__v8si *) __P,
          (__v8si) __A,
          (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mov_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mov_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) _mm_setzero_di ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mov_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mov_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) _mm256_setzero_si256 ());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_load_epi64 (__m128i __W, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((__v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_load_epi64 (__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((__v2di *) __P,
              (__v2di)
              _mm_setzero_di (),
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_load_epi64 (__m256i __W, __mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((__v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_load_epi64 (__mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((__v4di *) __P,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_store_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa64store128_mask ((__v2di *) __P,
          (__v2di) __A,
          (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_store_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa64store256_mask ((__v4di *) __P,
          (__v4di) __A,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_movedup_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_movedup_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_movedup_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_movedup_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_set1_epi32(__m128i __O, __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_set1_epi32(__m256i __O, __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)_mm256_setzero_si256());
}



static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_set1_epi64 (__m128i __O, __mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_set1_epi64 (__m256i __O, __mmask8 __M, long long __A)
{
  return (__m256i) __builtin_ia32_selectq_256(__M,
                                              (__v4di) _mm256_set1_epi64x(__A),
                                              (__v4di) __O) ;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
   return (__m256i) __builtin_ia32_selectq_256(__M,
                                               (__v4di) _mm256_set1_epi64x(__A),
                                               (__v4di) _mm256_setzero_si256());
}
  










































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_load_pd (__m128d __W, __mmask8 __U, void __const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((__v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_load_pd (__mmask8 __U, void __const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((__v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_load_pd (__m256d __W, __mmask8 __U, void __const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((__v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_load_pd (__mmask8 __U, void __const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((__v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_load_ps (__m128 __W, __mmask8 __U, void __const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((__v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_load_ps (__mmask8 __U, void __const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((__v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_load_ps (__m256 __W, __mmask8 __U, void __const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((__v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_load_ps (__mmask8 __U, void __const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((__v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_loadu_epi64 (__m128i __W, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((__v2di *) __P,
                 (__v2di) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_loadu_epi64 (__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((__v2di *) __P,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_loadu_epi64 (__m256i __W, __mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((__v4di *) __P,
                 (__v4di) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_loadu_epi64 (__mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((__v4di *) __P,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_loadu_epi32 (__m128i __W, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((__v4si *) __P,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_loadu_epi32 (__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((__v4si *) __P,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_loadu_epi32 (__m256i __W, __mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((__v8si *) __P,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_loadu_epi32 (__mmask8 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((__v8si *) __P,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_loadu_pd (__m128d __W, __mmask8 __U, void __const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((__v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_loadu_pd (__mmask8 __U, void __const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((__v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_loadu_pd (__m256d __W, __mmask8 __U, void __const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((__v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_loadu_pd (__mmask8 __U, void __const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((__v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_loadu_ps (__m128 __W, __mmask8 __U, void __const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((__v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_loadu_ps (__mmask8 __U, void __const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((__v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_loadu_ps (__m256 __W, __mmask8 __U, void __const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((__v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_loadu_ps (__mmask8 __U, void __const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((__v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_store_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeapd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_store_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeapd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_store_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeaps128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_store_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeaps256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_storeu_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqudi128_mask ((__v2di *) __P,
             (__v2di) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_storeu_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqudi256_mask ((__v4di *) __P,
             (__v4di) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_storeu_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqusi128_mask ((__v4si *) __P,
             (__v4si) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_storeu_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqusi256_mask ((__v8si *) __P,
             (__v8si) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_storeu_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeupd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_storeu_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeupd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_storeu_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeups128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_storeu_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeups256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}


static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpackhi_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpackhi_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpackhi_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpackhi_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpackhi_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpackhi_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpackhi_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpackhi_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpacklo_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpacklo_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpacklo_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpacklo_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpacklo_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpacklo_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpacklo_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpacklo_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rcp14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rcp14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rcp14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rcp14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rcp14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rcp14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rcp14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rcp14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rcp14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rcp14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rcp14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rcp14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}









































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutevar_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutevar_pd(__mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutevar_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutevar_pd(__mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_permutevar_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_permutevar_ps(__mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutevar_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutevar_ps(__mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)_mm256_setzero_ps());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_test_epi32_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpneq_epi32_mask (_mm_and_si128 (__A, __B), _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_test_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpneq_epi32_mask (__U, _mm_and_si128 (__A, __B),
                                     _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_test_epi32_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpneq_epi32_mask (_mm256_and_si256 (__A, __B),
                                   _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_test_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpneq_epi32_mask (__U, _mm256_and_si256 (__A, __B),
                                        _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_test_epi64_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpneq_epi64_mask (_mm_and_si128 (__A, __B), _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_test_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpneq_epi64_mask (__U, _mm_and_si128 (__A, __B),
                                     _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_test_epi64_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpneq_epi64_mask (_mm256_and_si256 (__A, __B),
                                   _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_test_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpneq_epi64_mask (__U, _mm256_and_si256 (__A, __B),
                                        _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_testn_epi32_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpeq_epi32_mask (_mm_and_si128 (__A, __B), _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_testn_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpeq_epi32_mask (__U, _mm_and_si128 (__A, __B),
                                    _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_testn_epi32_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpeq_epi32_mask (_mm256_and_si256 (__A, __B),
                                  _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_testn_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpeq_epi32_mask (__U, _mm256_and_si256 (__A, __B),
                                       _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_testn_epi64_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpeq_epi64_mask (_mm_and_si128 (__A, __B), _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_testn_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpeq_epi64_mask (__U, _mm_and_si128 (__A, __B),
                                    _mm_setzero_di());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_testn_epi64_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpeq_epi64_mask (_mm256_and_si256 (__A, __B),
                                  _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_testn_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpeq_epi64_mask (__U, _mm256_and_si256 (__A, __B),
                                       _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpackhi_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpackhi_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpackhi_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpackhi_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpackhi_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpackhi_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpackhi_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpackhi_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpacklo_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpacklo_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpacklo_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpacklo_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_unpacklo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_unpacklo_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)_mm_setzero_di());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_unpacklo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_unpacklo_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sra_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sra_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sra_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sra_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srai_epi32(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srai_epi32(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srai_epi32(__m256i __W, __mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srai_epi32(__mmask8 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_sra_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psraq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_sra_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,                                               (__v2di)_mm_sra_epi64(__A, __B),                                               (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_sra_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,                                               (__v2di)_mm_sra_epi64(__A, __B),                                               (__v2di)_mm_setzero_di());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_sra_epi64(__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psraq256((__v4di) __A, (__v2di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_sra_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,                                             (__v4di)_mm256_sra_epi64(__A, __B),                                             (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_sra_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,                                             (__v4di)_mm256_sra_epi64(__A, __B),                                             (__v4di)_mm256_setzero_si256());


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_srai_epi64(__m128i __A, int __imm)
{
  return (__m128i)__builtin_ia32_psraqi128((__v2di)__A, __imm);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_srai_epi64(__m128i __W, __mmask8 __U, __m128i __A, int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,                                             (__v2di)_mm_srai_epi64(__A, __imm),                                             (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_srai_epi64(__mmask8 __U, __m128i __A, int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,                                             (__v2di)_mm_srai_epi64(__A, __imm),                                             (__v2di)_mm_setzero_di());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_srai_epi64(__m256i __A, int __imm)
{
  return (__m256i)__builtin_ia32_psraqi256((__v4di)__A, __imm);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_srai_epi64(__m256i __W, __mmask8 __U, __m256i __A, int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,                                          (__v4di)_mm256_srai_epi64(__A, __imm),                                          (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_srai_epi64(__mmask8 __U, __m256i __A, int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,                                          (__v4di)_mm256_srai_epi64(__A, __imm),                                          (__v4di)_mm256_setzero_si256());


}












































































# 6599 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4










































































































static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rsqrt14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rsqrt14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df) __W,
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rsqrt14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rsqrt14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rsqrt14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df) __W,
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rsqrt14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_rsqrt14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_rsqrt14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_rsqrt14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_rsqrt14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_rsqrt14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_rsqrt14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_broadcast_f32x4(__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcast_f32x4(__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcast_f32x4 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_broadcast_i32x4(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcast_i32x4(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcast_i32x4(__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcastsd_pd (__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) __O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) _mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_broadcastss_ps (__m128 __O, __mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) __O);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) _mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcastss_ps (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) __O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) _mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_broadcastd_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcastd_epi32 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_broadcastq_epi64 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_broadcastq_epi64 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtsepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtsepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtsepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtsepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtsepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)_mm_setzero_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtsepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtsepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtsepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtsepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtsepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtsepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtsepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtsepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtsepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtsepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtsepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtsepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtsepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtsepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtsepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtusepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtusepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtusepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtusepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdb256mem_mask ((__v16qi*) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtusepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtusepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtusepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtusepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtusepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtusepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtusepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtusepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtusepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtusepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtusepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtusepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtusepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtusepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtusepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtusepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  return __builtin_ia32_pmovusqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi)_mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi)_mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) _mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi)_mm_setzero_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi32_storeu_epi16 (void *  __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si)_mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqd256_mask ((__v4di) __A,
              (__v4si) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_cvtepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi)__O,
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_cvtepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi)_mm_undefined_si128(),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}






































# 7892 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4












# 7914 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4



















































































































































































































static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutexvar_pd (__m256i __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
                 (__v4di) __X,
                 (__v4df) _mm256_undefined_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutexvar_pd (__m256d __W, __mmask8 __U, __m256i __X,
          __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
                 (__v4di) __X,
                 (__v4df) __W,
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutexvar_pd (__mmask8 __U, __m256i __X, __m256d __Y)
{
  return (__m256d) __builtin_ia32_permvardf256_mask ((__v4df) __Y,
                 (__v4di) __X,
                 (__v4df) _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutexvar_epi64 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
                 (__v4di) __X,
                 (__v4di) _mm256_setzero_si256 (),
                 (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutexvar_epi64 ( __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
                 (__v4di) __X,
                 (__v4di) _mm256_undefined_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutexvar_epi64 (__m256i __W, __mmask8 __M, __m256i __X,
             __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvardi256_mask ((__v4di) __Y,
                 (__v4di) __X,
                 (__v4di) __W,
                 __M);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutexvar_ps (__m256 __W, __mmask8 __U, __m256i __X,
          __m256 __Y)
{
  return (__m256) __builtin_ia32_permvarsf256_mask ((__v8sf) __Y,
                (__v8si) __X,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutexvar_ps (__mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256) __builtin_ia32_permvarsf256_mask ((__v8sf) __Y,
                (__v8si) __X,
                (__v8sf) _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutexvar_ps (__m256i __X, __m256 __Y)
{
  return (__m256) __builtin_ia32_permvarsf256_mask ((__v8sf) __Y,
                (__v8si) __X,
                (__v8sf) _mm256_undefined_si256 (),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_permutexvar_epi32 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
                 (__v8si) __X,
                 (__v8si) _mm256_setzero_si256 (),
                 __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_permutexvar_epi32 (__m256i __W, __mmask8 __M, __m256i __X,
             __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
                 (__v8si) __X,
                 (__v8si) __W,
                 (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_permutexvar_epi32 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_permvarsi256_mask ((__v8si) __Y,
                 (__v8si) __X,
                 (__v8si) _mm256_undefined_si256(),
                 (__mmask8) -1);
}




















# 8266 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlintrin.h" 3 4













































static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_movehdup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_movehdup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_movehdup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_movehdup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_moveldup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_moveldup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_moveldup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_moveldup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}





















static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mov_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mov_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) _mm_setzero_pd ());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mov_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) __W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mov_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) _mm256_setzero_pd ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_mov_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_mov_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) _mm_setzero_ps ());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_mov_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) __W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_mov_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) _mm256_setzero_ps ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtph_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf) __W,
             (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtph_ps (__m256 __W, __mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_mask_cvtps_ph (__m128i __W, __mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf) __A, 0x04,
                                                  (__v8hi) __W,
                                                  (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm_maskz_cvtps_ph (__mmask8 __U, __m128 __A)
{
  return (__m128i) __builtin_ia32_vcvtps2ph_mask ((__v4sf) __A, 0x04,
                                                  (__v8hi) _mm_setzero_si128 (),
                                                  (__mmask8) __U);
}











static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_mask_cvtps_ph (__m128i __W, __mmask8 __U, __m256 __A)
{
  return (__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf) __A, 0x04,
                                                      (__v8hi) __W,
                                                      (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl")))
_mm256_maskz_cvtps_ph ( __mmask8 __U, __m256 __A)
{
  return (__m128i) __builtin_ia32_vcvtps2ph256_mask ((__v8sf) __A, 0x04,
                                                      (__v8hi) _mm_setzero_si128(),
                                                      (__mmask8) __U);
}














# 147 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4








typedef unsigned int __mmask32;
typedef unsigned long long __mmask64;




static  __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_setzero_qi(void) {
  return (__m512i)(__v64qi){ 0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0 };
}

static  __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_setzero_hi(void) {
  return (__m512i)(__v32hi){ 0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0 };
}












































# 123 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4


# 148 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4


# 173 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4


# 198 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_add_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A + (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_add_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_add_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sub_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A - (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_sub_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_sub_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_add_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A + (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_add_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_add_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sub_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A - (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_sub_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_sub_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mullo_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A * (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mullo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mullo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_blend_epi8 (__mmask64 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
              (__v64qi) __W,
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_blend_epi16 (__mmask32 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
              (__v32hi) __W,
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_abs_epi8 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_abs_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
              (__v64qi) __W,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_abs_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsb512_mask ((__v64qi) __A,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_abs_epi16 (__m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_abs_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_abs_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_pabsw512_mask ((__v32hi) __A,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_packs_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packssdw512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_packs_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_packs_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_packs_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packsswb512((__v32hi)__A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_packs_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_packs_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_packus_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packusdw512((__v16si) __A, (__v16si) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_packus_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_packus_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_packus_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packuswb512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_packus_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_packus_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_adds_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_adds_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_adds_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_adds_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_adds_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_adds_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_adds_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_adds_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_adds_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_adds_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_adds_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_adds_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_paddusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_avg_epu8 (__m512i __A, __m512i __B)
{
  typedef unsigned short __v64hu __attribute__((__vector_size__(128)));
  return (__m512i)__builtin_convertvector(
              ((__builtin_convertvector((__v64qu) __A, __v64hu) +
                __builtin_convertvector((__v64qu) __B, __v64hu)) + 1)
                >> 1, __v64qu);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_avg_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
          __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_avg_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_avg_epu16 (__m512i __A, __m512i __B)
{
  typedef unsigned int __v32su __attribute__((__vector_size__(128)));
  return (__m512i)__builtin_convertvector(
              ((__builtin_convertvector((__v32hu) __A, __v32su) +
                __builtin_convertvector((__v32hu) __B, __v32su)) + 1)
                >> 1, __v32hu);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_avg_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_avg_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi) _mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_max_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_max_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_max_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_max_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_max_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_max_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_max_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_max_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_max_epu8 (__m512i __W, __mmask64 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_max_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_max_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_max_epu16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_pmaxuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_min_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_min_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_min_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_min_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_min_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_min_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_pminsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_min_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_min_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_min_epu8 (__m512i __W, __mmask64 __M, __m512i __A,
          __m512i __B)
{
  return (__m512i) __builtin_ia32_pminub512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_min_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_min_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_min_epu16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_pminuw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_shuffle_epi8(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pshufb512((__v64qi)__A,(__v64qi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_shuffle_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_shuffle_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_subs_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_subs_epi8 (__m512i __W, __mmask64 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_subs_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_subs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_subs_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_subs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubsw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_subs_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_subs_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) __W,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_subs_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusb512_mask ((__v64qi) __A,
              (__v64qi) __B,
              (__v64qi) _mm512_setzero_qi(),
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_subs_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_subs_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
      __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_subs_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_psubusw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask2_permutex2var_epi16 (__m512i __A, __m512i __I,
         __mmask32 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varhi512_mask ((__v32hi) __A,
              (__v32hi) __I  ,
              (__v32hi) __B,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_permutex2var_epi16 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_mask ((__v32hi) __I ,
              (__v32hi) __A,
              (__v32hi) __B,
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_permutex2var_epi16 (__m512i __A, __mmask32 __U,
        __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_mask ((__v32hi) __I ,
              (__v32hi) __A,
              (__v32hi) __B,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_permutex2var_epi16 (__mmask32 __U, __m512i __A,
         __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varhi512_maskz ((__v32hi) __I
               ,
              (__v32hi) __A,
              (__v32hi) __B,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mulhrs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
                (__v32hi) __B,
                (__v32hi) _mm512_setzero_hi(),
                (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mulhrs_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
                (__v32hi) __B,
                (__v32hi) __W,
                (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mulhrs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhrsw512_mask ((__v32hi) __A,
                (__v32hi) __B,
                (__v32hi) _mm512_setzero_hi(),
                (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mulhi_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mulhi_epi16 (__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) __W,
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mulhi_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhw512_mask ((__v32hi) __A,
              (__v32hi) __B,
              (__v32hi) _mm512_setzero_hi(),
              (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mulhi_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v32hi) _mm512_setzero_hi(),
               (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mulhi_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v32hi) __W,
               (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mulhi_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_pmulhuw512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v32hi) _mm512_setzero_hi(),
               (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maddubs_epi16 (__m512i __X, __m512i __Y) {
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
                 (__v64qi) __Y,
                 (__v32hi) _mm512_setzero_hi(),
                 (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_maddubs_epi16 (__m512i __W, __mmask32 __U, __m512i __X,
         __m512i __Y) {
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
                 (__v64qi) __Y,
                 (__v32hi) __W,
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_maddubs_epi16 (__mmask32 __U, __m512i __X, __m512i __Y) {
  return (__m512i) __builtin_ia32_pmaddubsw512_mask ((__v64qi) __X,
                 (__v64qi) __Y,
                 (__v32hi) _mm512_setzero_hi(),
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_madd_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v16si) _mm512_setzero_si512(),
               (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_madd_epi16 (__m512i __W, __mmask16 __U, __m512i __A,
      __m512i __B) {
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v16si) __W,
               (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_madd_epi16 (__mmask16 __U, __m512i __A, __m512i __B) {
  return (__m512i) __builtin_ia32_pmaddwd512_mask ((__v32hi) __A,
               (__v32hi) __B,
               (__v16si) _mm512_setzero_si512(),
               (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_cvtsepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)_mm256_setzero_si256(),
               (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtsepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)__O,
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_cvtsepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi) _mm256_setzero_si256(),
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_cvtusepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtusepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_cvtusepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_cvtepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_setzero_si256(),
              (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) __O,
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_cvtepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_setzero_si256(),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovwb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovuswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_unpackhi_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          8,  64+8,   9, 64+9,
                                          10, 64+10, 11, 64+11,
                                          12, 64+12, 13, 64+13,
                                          14, 64+14, 15, 64+15,
                                          24, 64+24, 25, 64+25,
                                          26, 64+26, 27, 64+27,
                                          28, 64+28, 29, 64+29,
                                          30, 64+30, 31, 64+31,
                                          40, 64+40, 41, 64+41,
                                          42, 64+42, 43, 64+43,
                                          44, 64+44, 45, 64+45,
                                          46, 64+46, 47, 64+47,
                                          56, 64+56, 57, 64+57,
                                          58, 64+58, 59, 64+59,
                                          60, 64+60, 61, 64+61,
                                          62, 64+62, 63, 64+63);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_unpackhi_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_unpackhi_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_unpackhi_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          4,  32+4,   5, 32+5,
                                          6,  32+6,   7, 32+7,
                                          12, 32+12, 13, 32+13,
                                          14, 32+14, 15, 32+15,
                                          20, 32+20, 21, 32+21,
                                          22, 32+22, 23, 32+23,
                                          28, 32+28, 29, 32+29,
                                          30, 32+30, 31, 32+31);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_unpackhi_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_unpackhi_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_unpacklo_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          0,  64+0,   1, 64+1,
                                          2,  64+2,   3, 64+3,
                                          4,  64+4,   5, 64+5,
                                          6,  64+6,   7, 64+7,
                                          16, 64+16, 17, 64+17,
                                          18, 64+18, 19, 64+19,
                                          20, 64+20, 21, 64+21,
                                          22, 64+22, 23, 64+23,
                                          32, 64+32, 33, 64+33,
                                          34, 64+34, 35, 64+35,
                                          36, 64+36, 37, 64+37,
                                          38, 64+38, 39, 64+39,
                                          48, 64+48, 49, 64+49,
                                          50, 64+50, 51, 64+51,
                                          52, 64+52, 53, 64+53,
                                          54, 64+54, 55, 64+55);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_unpacklo_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_unpacklo_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_qi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_unpacklo_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          0,  32+0,   1, 32+1,
                                          2,  32+2,   3, 32+3,
                                          8,  32+8,   9, 32+9,
                                          10, 32+10, 11, 32+11,
                                          16, 32+16, 17, 32+17,
                                          18, 32+18, 19, 32+19,
                                          24, 32+24, 25, 32+25,
                                          26, 32+26, 27, 32+27);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_unpacklo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_unpacklo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_cvtepi8_epi16(__m256i __A)
{
  

  return (__m512i)__builtin_convertvector((__v32qs)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtepi8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_cvtepi8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_cvtepu8_epi16(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v32qu)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_cvtepu8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_cvtepu8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)_mm512_setzero_hi());
}



# 1419 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4














# 1455 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sllv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psllv32hi((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_sllv_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_sllv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sll_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_sll_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_sll_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_slli_epi16(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psllwi512((__v32hi)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_slli_epi16(__m512i __W, __mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_slli_epi16(__mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_hi());
}


# 1604 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_srlv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrlv32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_srlv_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_srlv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_srav_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrav32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_srav_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_srav_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sra_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_sra_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_sra_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_srai_epi16(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psrawi512((__v32hi)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_srai_epi16(__m512i __W, __mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_srai_epi16(__mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_srl_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_srl_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_srl_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_hi());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_srli_epi16(__m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_psrlwi512((__v32hi)__A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_srli_epi16(__m512i __W, __mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_srli_epi16(__mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_hi());
}


# 1805 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bwintrin.h" 3 4

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mov_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mov_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) _mm512_setzero_hi ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_mov_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_mov_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) _mm512_setzero_hi ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_set1_epi8 (__m512i __O, __mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi)_mm512_set1_epi8(__A),
                                              (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_set1_epi8 (__mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi) _mm512_set1_epi8(__A),
                                              (__v64qi) _mm512_setzero_si512());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_kunpackd (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kunpckdi ((__mmask64) __A,
                (__mmask64) __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_kunpackw (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kunpcksi ((__mmask32) __A,
                (__mmask32) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_loadu_epi16 (__m512i __W, __mmask32 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((__v32hi *) __P,
                 (__v32hi) __W,
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_loadu_epi16 (__mmask32 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((__v32hi *) __P,
                 (__v32hi)
                 _mm512_setzero_hi (),
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((__v64qi *) __P,
                 (__v64qi) __W,
                 (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_loadu_epi8 (__mmask64 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((__v64qi *) __P,
                 (__v64qi)
                 _mm512_setzero_hi (),
                 (__mmask64) __U);
}
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_storeu_epi16 (void *__P, __mmask32 __U, __m512i __A)
{
  __builtin_ia32_storedquhi512_mask ((__v32hi *) __P,
             (__v32hi) __A,
             (__mmask32) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_storeu_epi8 (void *__P, __mmask64 __U, __m512i __A)
{
  __builtin_ia32_storedquqi512_mask ((__v64qi *) __P,
             (__v64qi) __A,
             (__mmask64) __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_test_epi8_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpneq_epi8_mask (_mm512_and_epi32 (__A, __B),
                                  _mm512_setzero_qi());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_test_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpneq_epi8_mask (__U, _mm512_and_epi32 (__A, __B),
                                       _mm512_setzero_qi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_test_epi16_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpneq_epi16_mask (_mm512_and_epi32 (__A, __B),
                                   _mm512_setzero_qi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_test_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpneq_epi16_mask (__U, _mm512_and_epi32 (__A, __B),
                                        _mm512_setzero_qi());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_testn_epi8_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpeq_epi8_mask (_mm512_and_epi32 (__A, __B), _mm512_setzero_qi());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_testn_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpeq_epi8_mask (__U, _mm512_and_epi32 (__A, __B),
                                      _mm512_setzero_qi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_testn_epi16_mask (__m512i __A, __m512i __B)
{
  return _mm512_cmpeq_epi16_mask (_mm512_and_epi32 (__A, __B),
                                  _mm512_setzero_qi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_testn_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_cmpeq_epi16_mask (__U, _mm512_and_epi32 (__A, __B),
                                       _mm512_setzero_qi());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_movepi8_mask (__m512i __A)
{
  return (__mmask64) __builtin_ia32_cvtb2mask512 ((__v64qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_movepi16_mask (__m512i __A)
{
  return (__mmask32) __builtin_ia32_cvtw2mask512 ((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_movm_epi8 (__mmask64 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2b512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_movm_epi16 (__mmask32 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2w512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_broadcastb_epi8 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v16qi) __A,
                                          (__v16qi)_mm_undefined_si128(),
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_broadcastb_epi8 (__m512i __O, __mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_broadcastb_epi8 (__mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_set1_epi16 (__m512i __O, __mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_set1_epi16 (__mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_broadcastw_epi16 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v8hi) __A,
                                          (__v8hi)_mm_undefined_si128(),
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_broadcastw_epi16 (__m512i __O, __mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_broadcastw_epi16 (__mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_permutexvar_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
                 (__v32hi) __A,
                 (__v32hi) _mm512_undefined_epi32 (),
                 (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_maskz_permutexvar_epi16 (__mmask32 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
                 (__v32hi) __A,
                 (__v32hi) _mm512_setzero_hi(),
                 (__mmask32) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_mask_permutexvar_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarhi512_mask ((__v32hi) __B,
                 (__v32hi) __A,
                 (__v32hi) __W,
                 (__mmask32) __M);
}





































static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_sad_epu8 (__m512i __A, __m512i __B)
{
 return (__m512i) __builtin_ia32_psadbw512 ((__v64qi) __A,
               (__v64qi) __B);
}






# 151 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bitalgintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512bitalgintrin.h" 3 4











static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_popcnt_epi16(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntw_512((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_mask_popcnt_epi16(__m512i __A, __mmask32 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectw_512((__mmask32) __U,
              (__v32hi) _mm512_popcnt_epi16(__B),
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_maskz_popcnt_epi16(__mmask32 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi16((__m512i) _mm512_setzero_hi(),
              __U,
              __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_popcnt_epi8(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntb_512((__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_mask_popcnt_epi8(__m512i __A, __mmask64 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512((__mmask64) __U,
              (__v64qi) _mm512_popcnt_epi8(__B),
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_maskz_popcnt_epi8(__mmask64 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi8((__m512i) _mm512_setzero_qi(),
              __U,
              __B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_mask_bitshuffle_epi64_mask(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_vpshufbitqmb512_mask((__v64qi) __A,
              (__v64qi) __B,
              __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg")))
_mm512_bitshuffle_epi64_mask(__m512i __A, __m512i __B)
{
  return _mm512_mask_bitshuffle_epi64_mask((__mmask64) -1,
              __A,
              __B);
}





# 155 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512cdintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512cdintrin.h" 3 4











static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_conflict_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
                 (__v8di) _mm512_setzero_si512 (),
                 (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_mask_conflict_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
               (__v8di) __W,
               (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_maskz_conflict_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512_mask ((__v8di) __A,
                 (__v8di) _mm512_setzero_si512 (),
                 (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_conflict_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
                 (__v16si) _mm512_setzero_si512 (),
                 (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_mask_conflict_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
               (__v16si) __W,
               (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_maskz_conflict_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512_mask ((__v16si) __A,
                 (__v16si) _mm512_setzero_si512 (),
                 (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_lzcnt_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
             (__v16si) _mm512_setzero_si512 (),
             (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_mask_lzcnt_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
                 (__v16si) __W,
                 (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_maskz_lzcnt_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512_mask ((__v16si) __A,
             (__v16si) _mm512_setzero_si512 (),
             (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_lzcnt_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
             (__v8di) _mm512_setzero_si512 (),
             (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_mask_lzcnt_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
                 (__v8di) __W,
                 (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_maskz_lzcnt_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512_mask ((__v8di) __A,
             (__v8di) _mm512_setzero_si512 (),
             (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m512i) _mm512_set1_epi64((long long) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd")))
_mm512_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m512i) _mm512_set1_epi32((int) __A);

}




# 159 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vpopcntdqintrin.h" 1 3 4
# 24 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vpopcntdqintrin.h" 3 4














static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q"))) _mm512_popcnt_epi64(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntq_512((__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q")))
_mm512_mask_popcnt_epi64(__m512i __W, __mmask8 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectq_512(
      (__mmask8)__U, (__v8di)_mm512_popcnt_epi64(__A), (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q")))
_mm512_maskz_popcnt_epi64(__mmask8 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi64((__m512i)_mm512_setzero_si512(), __U, __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q"))) _mm512_popcnt_epi32(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntd_512((__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q")))
_mm512_mask_popcnt_epi32(__m512i __W, __mmask16 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectd_512(
      (__mmask16)__U, (__v16si)_mm512_popcnt_epi32(__A), (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntd" "q")))
_mm512_maskz_popcnt_epi32(__mmask16 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi32((__m512i)_mm512_setzero_si512(), __U, __A);
}




# 163 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vpopcntdqvlintrin.h" 1 3 4
# 24 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vpopcntdqvlintrin.h" 3 4













static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"))) _mm_popcnt_epi64(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntq_128((__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm_mask_popcnt_epi64(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128(
      (__mmask8)__U, (__v2di)_mm_popcnt_epi64(__A), (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm_maskz_popcnt_epi64(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi64((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"))) _mm_popcnt_epi32(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntd_128((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm_mask_popcnt_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128(
      (__mmask8)__U, (__v4si)_mm_popcnt_epi32(__A), (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm_maskz_popcnt_epi32(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi32((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"))) _mm256_popcnt_epi64(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntq_256((__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm256_mask_popcnt_epi64(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256(
      (__mmask8)__U, (__v4di)_mm256_popcnt_epi64(__A), (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm256_maskz_popcnt_epi64(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi64((__m256i)_mm256_setzero_si256(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"))) _mm256_popcnt_epi32(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntd_256((__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm256_mask_popcnt_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256(
      (__mmask8)__U, (__v8si)_mm256_popcnt_epi32(__A), (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl")))
_mm256_maskz_popcnt_epi32(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi32((__m256i)_mm256_setzero_si256(), __U, __A);
}




# 168 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vnniintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vnniintrin.h" 3 4












static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_mask_dpbusd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_maskz_dpbusd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusd512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_dpbusd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_mask_dpbusds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusds512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_maskz_dpbusds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusds512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_dpbusds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpbusds512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_mask_dpwssd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_maskz_dpwssd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssd512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_dpwssd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_mask_dpwssds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssds512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_maskz_dpwssds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssds512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni")))
_mm512_dpwssds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpdpwssds512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}





# 172 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlvnniintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlvnniintrin.h" 3 4












static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_mask_dpbusd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_maskz_dpbusd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusd256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_dpbusd_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_mask_dpbusds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusds256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_maskz_dpbusds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusds256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_dpbusds_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpbusds256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_mask_dpwssd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_maskz_dpwssd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssd256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_dpwssd_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_mask_dpwssds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssds256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_maskz_dpwssds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssds256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm256_dpwssds_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpdpwssds256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_mask_dpbusd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_maskz_dpbusd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusd128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_dpbusd_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_mask_dpbusds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusds128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_maskz_dpbusds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusds128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_dpbusds_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpbusds128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_mask_dpwssd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_maskz_dpwssd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssd128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_dpwssd_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_mask_dpwssds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssds128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_maskz_dpwssds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssds128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni")))
_mm128_dpwssds_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpdpwssds128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}





# 177 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4












static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mullo_epi64 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_mullo_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_mullo_epi64(__mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_xor_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A ^ (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_xor_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_xor_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_xor_ps (__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A ^ (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_xor_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_xor_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_or_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A | (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_or_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_or_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_or_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A | (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_or_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_or_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_and_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_and_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_and_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_and_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_and_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_and_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_andnot_pd(__m512d __A, __m512d __B) {
  return (__m512d)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_andnot_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_andnot_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_andnot_ps(__m512 __A, __m512 __B) {
  return (__m512)(~(__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_andnot_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_andnot_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}

















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtepi64_pd (__m512i __A) {
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
                (__v8df) _mm512_setzero_pd(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtepi64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtepi64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d) __builtin_ia32_cvtqq2pd512_mask ((__v8di) __A,
                (__v8df) _mm512_setzero_pd(),
                (__mmask8) __U,
                0x04);
}
















static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtepi64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) -1,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtepi64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) __W,
               (__mmask8) __U,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtepi64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) __U,
               0x04);
}

















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvttpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvttpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvttpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvttpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvttpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvttpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvttps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvttps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvttps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
















static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvttps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvttps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvttps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
















static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtepu64_pd (__m512i __A) {
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
                 (__v8df) _mm512_setzero_pd(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtepu64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
                 (__v8df) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtepu64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d) __builtin_ia32_cvtuqq2pd512_mask ((__v8di) __A,
                 (__v8df) _mm512_setzero_pd(),
                 (__mmask8) __U,
                 0x04);
}


















static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_cvtepu64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_cvtepu64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_cvtepu64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) __U,
                0x04);
}






























































































































































































































































































                     
static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_movepi32_mask (__m512i __A)
{
  return (__mmask16) __builtin_ia32_cvtd2mask512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_movm_epi32 (__mmask16 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2d512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_movm_epi64 (__mmask8 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2q512 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_movepi64_mask (__m512i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask512 ((__v8di) __A);
}


static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_f32x2 (__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A,
                                         (__v4sf)_mm_undefined_ps(),
                                         0, 1, 0, 1, 0, 1, 0, 1,
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_f32x2 (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_f32x2 (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_f32x8(__m256 __A)
{
  return (__m512)__builtin_shufflevector((__v8sf)__A, (__v8sf)__A,
                                         0, 1, 2, 3, 4, 5, 6, 7,
                                         0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_f32x8(__m512 __O, __mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask8)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_f32x8(__mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask8)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_f64x2(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_f64x2(__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_f64x2(__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_i32x2 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A,
                                          (__v4si)_mm_undefined_si128(),
                                          0, 1, 0, 1, 0, 1, 0, 1,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_i32x2 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_i32x2 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_i32x8(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v8si)__A, (__v8si)__A,
                                          0, 1, 2, 3, 4, 5, 6, 7,
                                          0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_i32x8(__m512i __O, __mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask8)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_i32x8(__mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask8)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_broadcast_i64x2(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_mask_broadcast_i64x2(__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_mm512_maskz_broadcast_i64x2(__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)_mm512_setzero_si512());
}


# 1128 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4




























# 1166 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4




























# 1212 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4












# 1234 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4












# 1264 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4












# 1286 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512dqintrin.h" 3 4














































# 181 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbitalgintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbitalgintrin.h" 3 4











static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_popcnt_epi16(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntw_256((__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_mask_popcnt_epi16(__m256i __A, __mmask16 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectw_256((__mmask16) __U,
              (__v16hi) _mm256_popcnt_epi16(__B),
              (__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_maskz_popcnt_epi16(__mmask16 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi16((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_popcnt_epi16(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntw_128((__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_mask_popcnt_epi16(__m128i __A, __mmask8 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectw_128((__mmask8) __U,
              (__v8hi) _mm128_popcnt_epi16(__B),
              (__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_maskz_popcnt_epi16(__mmask8 __U, __m128i __B)
{
  return _mm128_mask_popcnt_epi16((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_popcnt_epi8(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntb_256((__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_mask_popcnt_epi8(__m256i __A, __mmask32 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256((__mmask32) __U,
              (__v32qi) _mm256_popcnt_epi8(__B),
              (__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_maskz_popcnt_epi8(__mmask32 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi8((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_popcnt_epi8(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntb_128((__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_mask_popcnt_epi8(__m128i __A, __mmask16 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128((__mmask16) __U,
              (__v16qi) _mm128_popcnt_epi8(__B),
              (__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_maskz_popcnt_epi8(__mmask16 __U, __m128i __B)
{
  return _mm128_mask_popcnt_epi8((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_mask_bitshuffle_epi32_mask(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_vpshufbitqmb256_mask((__v32qi) __A,
              (__v32qi) __B,
              __U);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm256_bitshuffle_epi32_mask(__m256i __A, __m256i __B)
{
  return _mm256_mask_bitshuffle_epi32_mask((__mmask32) -1,
              __A,
              __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_mask_bitshuffle_epi16_mask(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_vpshufbitqmb128_mask((__v16qi) __A,
              (__v16qi) __B,
              __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg")))
_mm128_bitshuffle_epi16_mask(__m128i __A, __m128i __B)
{
  return _mm128_mask_bitshuffle_epi16_mask((__mmask16) -1,
              __A,
              __B);
}





# 186 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4












static  __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_setzero_hi(void){
    return (__m128i)(__v8hi){ 0, 0, 0, 0, 0, 0, 0, 0 };
}




















































































# 145 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 170 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 195 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 220 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 245 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 270 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 295 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4


# 320 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlbwintrin.h" 3 4

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_add_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B){
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_add_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_add_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_add_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_sub_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_sub_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_sub_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_sub_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_add_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_add_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_add_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_add_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_sub_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_sub_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_sub_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_sub_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mullo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mullo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mullo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mullo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_blend_epi8 (__mmask16 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
              (__v16qi) __W,
              (__v16qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_blend_epi8 (__mmask32 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
               (__v32qi) __W,
               (__v32qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_blend_epi16 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
               (__v8hi) __W,
               (__v8hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_blend_epi16 (__mmask16 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
               (__v16hi) __W,
               (__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_abs_epi8(__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_abs_epi8(__mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_abs_epi8(__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_abs_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_abs_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_abs_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_abs_epi16(__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_abs_epi16(__mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_packs_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_packs_epi32(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_packs_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_packs_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_packs_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_packs_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_packs_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_packs_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_packus_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_packus_epi32(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_packus_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_packus_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_packus_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_packus_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_packus_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_packus_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_adds_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_adds_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_adds_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_adds_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_adds_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_adds_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_adds_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_adds_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_adds_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_adds_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_adds_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_adds_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_adds_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_adds_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_adds_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_adds_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_avg_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_avg_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_avg_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_avg_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_avg_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_avg_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_avg_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_avg_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_max_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_max_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_max_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_max_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_max_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_max_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_max_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_max_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_max_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_max_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_max_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_max_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_max_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_max_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_max_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_max_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_min_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_min_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_min_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_min_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_min_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_min_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_min_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_min_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_min_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_min_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_min_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_min_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_min_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_min_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_min_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_min_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_shuffle_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_shuffle_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_shuffle_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_shuffle_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_subs_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_subs_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_subs_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_subs_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_subs_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_subs_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_subs_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_subs_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_subs_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_subs_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_subs_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_subs_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_subs_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_subs_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_subs_epu16(__m256i __W, __mmask16 __U, __m256i __A,
      __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_subs_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask2_permutex2var_epi16 (__m128i __A, __m128i __I, __mmask8 __U,
            __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2varhi128_mask ((__v8hi) __A,
               (__v8hi) __I  ,
               (__v8hi) __B,
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask2_permutex2var_epi16 (__m256i __A, __m256i __I,
         __mmask16 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2varhi256_mask ((__v16hi) __A,
               (__v16hi) __I  ,
               (__v16hi) __B,
               (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_permutex2var_epi16 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_mask ((__v8hi) __I,
               (__v8hi) __A,
               (__v8hi) __B,
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_permutex2var_epi16 (__m128i __A, __mmask8 __U, __m128i __I,
           __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_mask ((__v8hi) __I,
               (__v8hi) __A,
               (__v8hi) __B,
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_permutex2var_epi16 (__mmask8 __U, __m128i __A, __m128i __I,
            __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varhi128_maskz ((__v8hi) __I,
               (__v8hi) __A,
               (__v8hi) __B,
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_permutex2var_epi16 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_mask ((__v16hi) __I,
               (__v16hi) __A,
               (__v16hi) __B,
               (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_permutex2var_epi16 (__m256i __A, __mmask16 __U,
        __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_mask ((__v16hi) __I,
               (__v16hi) __A,
               (__v16hi) __B,
               (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_permutex2var_epi16 (__mmask16 __U, __m256i __A,
         __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varhi256_maskz ((__v16hi) __I,
               (__v16hi) __A,
               (__v16hi) __B,
               (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_maddubs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_maddubs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_maddubs_epi16(__m256i __W, __mmask16 __U, __m256i __X,
                          __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_maddubs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_madd_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_madd_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_madd_epi16(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_madd_epi16(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_cvtsepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtsepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_cvtsepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_cvtsepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtsepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_cvtsepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_cvtusepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtusepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_cvtusepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_cvtusepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtusepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_cvtusepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_cvtepi16_epi8 (__m128i __A) {

  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) __O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_cvtepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovwb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovuswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_cvtepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
               (__v16qi) __O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_cvtepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovwb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovwb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovswb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovuswb256mem_mask ((__v16qi*) __P, (__v16hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mulhrs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mulhrs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mulhrs_epi16(__m256i __W, __mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mulhrs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mulhi_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mulhi_epu16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mulhi_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mulhi_epu16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mulhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mulhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mulhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mulhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_unpackhi_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_unpackhi_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_unpackhi_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_unpackhi_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_unpackhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_unpackhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_unpackhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_unpackhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_unpacklo_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_unpacklo_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_unpacklo_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_unpacklo_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_unpacklo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_unpacklo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_unpacklo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_unpacklo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtepi8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_cvtepi8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtepi8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_cvtepi8_epi16(__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_cvtepu8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_cvtepu8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_cvtepu8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_cvtepu8_epi16 (__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}












































static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_sllv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psllv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_sllv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_sllv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_sllv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psllv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_sllv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_sllv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_sll_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_sll_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_sll_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_sll_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_slli_epi16(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_slli_epi16(__m256i __W, __mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_slli_epi16(__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_srlv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrlv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_srlv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_srlv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_srlv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrlv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_srlv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_srlv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_srav_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrav16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_srav_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_srav_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_srav_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrav8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_srav_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_srav_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_sra_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_sra_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_sra_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_sra_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_srai_epi16(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_srai_epi16(__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_srai_epi16(__m256i __W, __mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_srai_epi16(__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_srl_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_srl_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_srl_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_srl_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_srli_epi16(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_srli_epi16 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_srli_epi16(__m256i __W, __mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_srli_epi16(__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mov_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mov_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) _mm_setzero_hi ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mov_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mov_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) _mm256_setzero_si256 ());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_mov_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_mov_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) _mm_setzero_hi ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_mov_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_mov_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) _mm256_setzero_si256 ());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_set1_epi8 (__m128i __O, __mmask16 __M, char __A)
{
  return (__m128i) __builtin_ia32_selectb_128(__M,
                                              (__v16qi) _mm_set1_epi8(__A),
                                              (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_set1_epi8 (__mmask16 __M, char __A)
{
 return (__m128i) __builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_set1_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_set1_epi8 (__m256i __O, __mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_set1_epi8 (__mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_loadu_epi16 (__m128i __W, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((__v8hi *) __P,
                 (__v8hi) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_loadu_epi16 (__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((__v8hi *) __P,
                 (__v8hi)
                 _mm_setzero_hi (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_loadu_epi16 (__m256i __W, __mmask16 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((__v16hi *) __P,
                 (__v16hi) __W,
                 (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_loadu_epi16 (__mmask16 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((__v16hi *) __P,
                 (__v16hi)
                 _mm256_setzero_si256 (),
                 (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((__v16qi *) __P,
                 (__v16qi) __W,
                 (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_loadu_epi8 (__mmask16 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((__v16qi *) __P,
                 (__v16qi)
                 _mm_setzero_si128 (),
                 (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((__v32qi *) __P,
                 (__v32qi) __W,
                 (__mmask32) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_loadu_epi8 (__mmask32 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((__v32qi *) __P,
                 (__v32qi)
                 _mm256_setzero_si256 (),
                 (__mmask32) __U);
}
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_storeu_epi16 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedquhi128_mask ((__v8hi *) __P,
             (__v8hi) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_storeu_epi16 (void *__P, __mmask16 __U, __m256i __A)
{
  __builtin_ia32_storedquhi256_mask ((__v16hi *) __P,
             (__v16hi) __A,
             (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_storeu_epi8 (void *__P, __mmask16 __U, __m128i __A)
{
  __builtin_ia32_storedquqi128_mask ((__v16qi *) __P,
             (__v16qi) __A,
             (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_storeu_epi8 (void *__P, __mmask32 __U, __m256i __A)
{
  __builtin_ia32_storedquqi256_mask ((__v32qi *) __P,
             (__v32qi) __A,
             (__mmask32) __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_test_epi8_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpneq_epi8_mask (_mm_and_si128(__A, __B), _mm_setzero_hi());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_test_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpneq_epi8_mask (__U, _mm_and_si128 (__A, __B),
                                    _mm_setzero_hi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_test_epi8_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpneq_epi8_mask (_mm256_and_si256(__A, __B),
                                  _mm256_setzero_si256());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_test_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpneq_epi8_mask (__U, _mm256_and_si256(__A, __B),
                                       _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_test_epi16_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpneq_epi16_mask (_mm_and_si128 (__A, __B), _mm_setzero_hi());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_test_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpneq_epi16_mask (__U, _mm_and_si128 (__A, __B),
                                     _mm_setzero_hi());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_test_epi16_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpneq_epi16_mask (_mm256_and_si256 (__A, __B),
                                   _mm256_setzero_si256 ());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_test_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpneq_epi16_mask (__U, _mm256_and_si256(__A, __B),
                                        _mm256_setzero_si256());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_testn_epi8_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpeq_epi8_mask (_mm_and_si128 (__A, __B), _mm_setzero_hi());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_testn_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpeq_epi8_mask (__U, _mm_and_si128 (__A, __B),
                                  _mm_setzero_hi());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_testn_epi8_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpeq_epi8_mask (_mm256_and_si256 (__A, __B),
                                 _mm256_setzero_si256());
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_testn_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpeq_epi8_mask (__U, _mm256_and_si256 (__A, __B),
                                      _mm256_setzero_si256());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_testn_epi16_mask (__m128i __A, __m128i __B)
{
  return _mm_cmpeq_epi16_mask (_mm_and_si128 (__A, __B), _mm_setzero_hi());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_testn_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_cmpeq_epi16_mask (__U, _mm_and_si128(__A, __B), _mm_setzero_hi());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_testn_epi16_mask (__m256i __A, __m256i __B)
{
  return _mm256_cmpeq_epi16_mask (_mm256_and_si256(__A, __B),
                                  _mm256_setzero_si256());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_testn_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_cmpeq_epi16_mask (__U, _mm256_and_si256 (__A, __B),
                                       _mm256_setzero_si256());
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_movepi8_mask (__m128i __A)
{
  return (__mmask16) __builtin_ia32_cvtb2mask128 ((__v16qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_movepi8_mask (__m256i __A)
{
  return (__mmask32) __builtin_ia32_cvtb2mask256 ((__v32qi) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_movepi16_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtw2mask128 ((__v8hi) __A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_movepi16_mask (__m256i __A)
{
  return (__mmask16) __builtin_ia32_cvtw2mask256 ((__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_movm_epi8 (__mmask16 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2b128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_movm_epi8 (__mmask32 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2b256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_movm_epi16 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2w128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_movm_epi16 (__mmask16 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2w256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_broadcastb_epi8 (__m128i __O, __mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_broadcastb_epi8 (__mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_broadcastb_epi8 (__m256i __O, __mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_broadcastb_epi8 (__mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_broadcastw_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_broadcastw_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_broadcastw_epi16 (__m256i __O, __mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_broadcastw_epi16 (__mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_set1_epi16 (__m256i __O, __mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256 (__M,
                                               (__v16hi) _mm256_set1_epi16(__A),
                                               (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_set1_epi16 (__mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256(__M,
                                              (__v16hi)_mm256_set1_epi16(__A),
                                              (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_set1_epi16 (__m128i __O, __mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_set1_epi16 (__mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) _mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_permutexvar_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
                 (__v8hi) __A,
                 (__v8hi) _mm_undefined_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_maskz_permutexvar_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
                 (__v8hi) __A,
                 (__v8hi) _mm_setzero_si128 (),
                 (__mmask8) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm_mask_permutexvar_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarhi128_mask ((__v8hi) __B,
                 (__v8hi) __A,
                 (__v8hi) __W,
                 (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_permutexvar_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
                 (__v16hi) __A,
                 (__v16hi) _mm256_undefined_si256 (),
                 (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_maskz_permutexvar_epi16 (__mmask16 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
                 (__v16hi) __A,
                 (__v16hi) _mm256_setzero_si256 (),
                 (__mmask16) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw")))
_mm256_mask_permutexvar_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarhi256_mask ((__v16hi) __B,
                 (__v16hi) __A,
                 (__v16hi) __W,
                 (__mmask16) __M);
}




























































# 191 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlcdintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlcdintrin.h" 3 4












static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_broadcastmb_epi64 (__mmask8 __A)
{ 
  return (__m128i) _mm_set1_epi64x((long long) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m256i) _mm256_set1_epi64x((long long)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m128i) _mm_set1_epi32((int)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m256i) _mm256_set1_epi32((int)__A);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_conflict_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
               (__v2di) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_mask_conflict_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
               (__v2di) __W,
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_maskz_conflict_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128_mask ((__v2di) __A,
               (__v2di)
               _mm_setzero_di (),
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_conflict_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
               (__v4di)  _mm256_undefined_si256 (),
               (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_mask_conflict_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
               (__v4di) __W,
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_maskz_conflict_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256_mask ((__v4di) __A,
               (__v4di) _mm256_setzero_si256 (),
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_conflict_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
               (__v4si) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_mask_conflict_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
               (__v4si) __W,
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_maskz_conflict_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128_mask ((__v4si) __A,
               (__v4si) _mm_setzero_si128 (),
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_conflict_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
               (__v8si) _mm256_undefined_si256 (),
               (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_mask_conflict_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
               (__v8si) __W,
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_maskz_conflict_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256_mask ((__v8si) __A,
               (__v8si)
               _mm256_setzero_si256 (),
               (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_lzcnt_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_mask_lzcnt_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_maskz_lzcnt_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128_mask ((__v4si) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_lzcnt_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_mask_lzcnt_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_maskz_lzcnt_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256_mask ((__v8si) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_lzcnt_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
                 (__v2di)
                 _mm_setzero_di (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_mask_lzcnt_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
                 (__v2di) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm_maskz_lzcnt_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128_mask ((__v2di) __A,
                 (__v2di)
                 _mm_setzero_di (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_lzcnt_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_mask_lzcnt_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
                 (__v4di) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd")))
_mm256_maskz_lzcnt_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256_mask ((__v4di) __A,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}




# 196 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vldqintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vldqintrin.h" 3 4












static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mullo_epi64 (__m256i __A, __m256i __B) {
  return (__m256i) ((__v4du) __A * (__v4du) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_mullo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_mullo_epi64(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mullo_epi64 (__m128i __A, __m128i __B) {
  return (__m128i) ((__v2du) __A * (__v2du) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_mullo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_mullo_epi64(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_andnot_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_andnot_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_andnot_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_andnot_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_andnot_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_andnot_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_andnot_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_andnot_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_and_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_and_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_and_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_and_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_and_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_and_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_and_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_and_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_xor_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_xor_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_xor_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_xor_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_xor_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_xor_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_xor_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_xor_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_or_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_or_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_or_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_or_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_or_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_or_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_or_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_or_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtepi64_pd (__m128i __A) {
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
                (__v2df) _mm_setzero_pd(),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtepi64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtepi64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d) __builtin_ia32_cvtqq2pd128_mask ((__v2di) __A,
                (__v2df) _mm_setzero_pd(),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtepi64_pd (__m256i __A) {
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
                (__v4df) _mm256_setzero_pd(),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtepi64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtepi64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d) __builtin_ia32_cvtqq2pd256_mask ((__v4di) __A,
                (__v4df) _mm256_setzero_pd(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtepi64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtepi64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtepi64_ps (__m256i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtepi64_ps (__mmask8 __U, __m256i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps256_mask ((__v4di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvttpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvttpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvttpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvttpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvttpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvttpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvttpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvttpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvttpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvttpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvttpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvttpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvttps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvttps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvttps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvttps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvttps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvttps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvttps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvttps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtepu64_pd (__m128i __A) {
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
                (__v2df) _mm_setzero_pd(),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtepu64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtepu64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d) __builtin_ia32_cvtuqq2pd128_mask ((__v2di) __A,
                (__v2df) _mm_setzero_pd(),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtepu64_pd (__m256i __A) {
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
                (__v4df) _mm256_setzero_pd(),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtepu64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtepu64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d) __builtin_ia32_cvtuqq2pd256_mask ((__v4di) __A,
                (__v4df) _mm256_setzero_pd(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_cvtepu64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_cvtepu64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_cvtepu64_ps (__m256i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_cvtepu64_ps (__mmask8 __U, __m256i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps256_mask ((__v4di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}



































































































































static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_movepi32_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask128 ((__v4si) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_movepi32_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask256 ((__v8si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_movm_epi32 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2d128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_movm_epi32 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2d256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_movm_epi64 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2q128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_movm_epi64 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2q256 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_movepi64_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask128 ((__v2di) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_movepi64_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask256 ((__v4di) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_broadcast_f32x2 (__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A,
                                         (__v4sf)_mm_undefined_ps(),
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_broadcast_f32x2 (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_broadcast_f32x2 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_broadcast_f64x2(__m128d __A)
{
  return (__m256d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_broadcast_f64x2(__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)__O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_broadcast_f64x2 (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_broadcast_i32x2 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector((__v4si)__A,
                                          (__v4si)_mm_undefined_si128(),
                                          0, 1, 0, 1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_mask_broadcast_i32x2 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_broadcast_i32x2 (__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A,
                                          (__v4si)_mm_undefined_si128(),
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_broadcast_i32x2 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_broadcast_i64x2(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_mask_broadcast_i64x2(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq")))
_mm256_maskz_broadcast_i64x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)_mm256_setzero_si256());
}








































































































# 201 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512erintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512erintrin.h" 3 4








// exp2a23
















































// rsqrt28






































































































// rcp28







































































































# 205 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512ifmaintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512ifmaintrin.h" 3 4











static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_madd52hi_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_mask ((__v8di) __X,
                   (__v8di) __Y,
                   (__v8di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_mask_madd52hi_epu64 (__m512i __W, __mmask8 __M, __m512i __X,
          __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_mask ((__v8di) __W,
                   (__v8di) __X,
                   (__v8di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_maskz_madd52hi_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52huq512_maskz ((__v8di) __X,
              (__v8di) __Y,
              (__v8di) __Z,
              (__mmask8) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_madd52lo_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_mask ((__v8di) __X,
                   (__v8di) __Y,
                   (__v8di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_mask_madd52lo_epu64 (__m512i __W, __mmask8 __M, __m512i __X,
          __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_mask ((__v8di) __W,
                   (__v8di) __X,
                   (__v8di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma")))
_mm512_maskz_madd52lo_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i) __builtin_ia32_vpmadd52luq512_maskz ((__v8di) __X,
              (__v8di) __Y,
              (__v8di) __Z,
              (__mmask8) __M);
}




# 209 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512ifmavlintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512ifmavlintrin.h" 3 4













static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_madd52hi_epu64 (__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_mask ((__v2di) __X,
                   (__v2di) __Y,
                   (__v2di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_mask_madd52hi_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_mask ((__v2di) __W,
                   (__v2di) __X,
                   (__v2di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_maskz_madd52hi_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52huq128_maskz ((__v2di) __X,
              (__v2di) __Y,
              (__v2di) __Z,
              (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_madd52hi_epu64 (__m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_mask ((__v4di) __X,
                   (__v4di) __Y,
                   (__v4di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_mask_madd52hi_epu64 (__m256i __W, __mmask8 __M, __m256i __X,
          __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_mask ((__v4di) __W,
                   (__v4di) __X,
                   (__v4di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_maskz_madd52hi_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52huq256_maskz ((__v4di) __X,
              (__v4di) __Y,
              (__v4di) __Z,
              (__mmask8) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_madd52lo_epu64 (__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_mask ((__v2di) __X,
                   (__v2di) __Y,
                   (__v2di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_mask_madd52lo_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_mask ((__v2di) __W,
                   (__v2di) __X,
                   (__v2di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm_maskz_madd52lo_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i) __builtin_ia32_vpmadd52luq128_maskz ((__v2di) __X,
              (__v2di) __Y,
              (__v2di) __Z,
              (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_madd52lo_epu64 (__m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_mask ((__v4di) __X,
                   (__v4di) __Y,
                   (__v4di) __Z,
                   (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_mask_madd52lo_epu64 (__m256i __W, __mmask8 __M, __m256i __X,
          __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_mask ((__v4di) __W,
                   (__v4di) __X,
                   (__v4di) __Y,
                   (__mmask8) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl")))
_mm256_maskz_madd52lo_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i) __builtin_ia32_vpmadd52luq256_maskz ((__v4di) __X,
              (__v4di) __Y,
              (__v4di) __Z,
              (__mmask8) __M);
}





# 214 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmiintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmiintrin.h" 3 4












static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_mask2_permutex2var_epi8 (__m512i __A, __m512i __I,
         __mmask64 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermi2varqi512_mask ((__v64qi) __A,
              (__v64qi) __I
               ,
              (__v64qi) __B,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_permutex2var_epi8 (__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_mask ((__v64qi) __I
               ,
              (__v64qi) __A,
              (__v64qi) __B,
              (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_mask_permutex2var_epi8 (__m512i __A, __mmask64 __U,
        __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_mask ((__v64qi) __I
               ,
              (__v64qi) __A,
              (__v64qi) __B,
              (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_maskz_permutex2var_epi8 (__mmask64 __U, __m512i __A,
         __m512i __I, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpermt2varqi512_maskz ((__v64qi) __I
                ,
               (__v64qi) __A,
               (__v64qi) __B,
               (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_permutexvar_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
                 (__v64qi) __A,
                 (__v64qi) _mm512_undefined_epi32 (),
                 (__mmask64) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_maskz_permutexvar_epi8 (__mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
                 (__v64qi) __A,
                 (__v64qi) _mm512_setzero_si512(),
                 (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_mask_permutexvar_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i) __builtin_ia32_permvarqi512_mask ((__v64qi) __B,
                 (__v64qi) __A,
                 (__v64qi) __W,
                 (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_mask_multishift_epi64_epi8 (__m512i __W, __mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
                (__v64qi) __Y,
                (__v64qi) __W,
                (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_maskz_multishift_epi64_epi8 (__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
                (__v64qi) __Y,
                (__v64qi) _mm512_setzero_si512 (),
                (__mmask64) __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi")))
_mm512_multishift_epi64_epi8 (__m512i __X, __m512i __Y)
{
  return (__m512i) __builtin_ia32_vpmultishiftqb512_mask ((__v64qi) __X,
                (__v64qi) __Y,
                (__v64qi) _mm512_undefined_epi32 (),
                (__mmask64) -1);
}





# 218 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmivlintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmivlintrin.h" 3 4












static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_mask2_permutex2var_epi8 (__m128i __A, __m128i __I, __mmask16 __U,
            __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermi2varqi128_mask ((__v16qi) __A,
              (__v16qi) __I
               ,
              (__v16qi) __B,
              (__mmask16)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_mask2_permutex2var_epi8 (__m256i __A, __m256i __I,
         __mmask32 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermi2varqi256_mask ((__v32qi) __A,
              (__v32qi) __I
               ,
              (__v32qi) __B,
              (__mmask32)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_permutex2var_epi8 (__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_mask ((__v16qi) __I
               ,
              (__v16qi) __A,
              (__v16qi) __B,
              (__mmask16) -
              1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_mask_permutex2var_epi8 (__m128i __A, __mmask16 __U, __m128i __I,
           __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_mask ((__v16qi) __I
               ,
              (__v16qi) __A,
              (__v16qi) __B,
              (__mmask16)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_maskz_permutex2var_epi8 (__mmask16 __U, __m128i __A, __m128i __I,
            __m128i __B)
{
  return (__m128i) __builtin_ia32_vpermt2varqi128_maskz ((__v16qi) __I
                ,
               (__v16qi) __A,
               (__v16qi) __B,
               (__mmask16)
               __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_permutex2var_epi8 (__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_mask ((__v32qi) __I
               ,
              (__v32qi) __A,
              (__v32qi) __B,
              (__mmask32) -
              1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_mask_permutex2var_epi8 (__m256i __A, __mmask32 __U,
        __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_mask ((__v32qi) __I
               ,
              (__v32qi) __A,
              (__v32qi) __B,
              (__mmask32)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_maskz_permutex2var_epi8 (__mmask32 __U, __m256i __A,
         __m256i __I, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpermt2varqi256_maskz ((__v32qi) __I
                ,
               (__v32qi) __A,
               (__v32qi) __B,
               (__mmask32)
               __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_permutexvar_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
                 (__v16qi) __A,
                 (__v16qi) _mm_undefined_si128 (),
                 (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_maskz_permutexvar_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
                 (__v16qi) __A,
                 (__v16qi) _mm_setzero_si128 (),
                 (__mmask16) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_mask_permutexvar_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i) __builtin_ia32_permvarqi128_mask ((__v16qi) __B,
                 (__v16qi) __A,
                 (__v16qi) __W,
                 (__mmask16) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_permutexvar_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
                 (__v32qi) __A,
                 (__v32qi) _mm256_undefined_si256 (),
                 (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_maskz_permutexvar_epi8 (__mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
                 (__v32qi) __A,
                 (__v32qi) _mm256_setzero_si256 (),
                 (__mmask32) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_mask_permutexvar_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i) __builtin_ia32_permvarqi256_mask ((__v32qi) __B,
                 (__v32qi) __A,
                 (__v32qi) __W,
                 (__mmask32) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_mask_multishift_epi64_epi8 (__m128i __W, __mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
                (__v16qi) __Y,
                (__v16qi) __W,
                (__mmask16) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_maskz_multishift_epi64_epi8 (__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
                (__v16qi) __Y,
                (__v16qi)
                _mm_setzero_si128 (),
                (__mmask16) __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm_multishift_epi64_epi8 (__m128i __X, __m128i __Y)
{
  return (__m128i) __builtin_ia32_vpmultishiftqb128_mask ((__v16qi) __X,
                (__v16qi) __Y,
                (__v16qi)
                _mm_undefined_si128 (),
                (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_mask_multishift_epi64_epi8 (__m256i __W, __mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
                (__v32qi) __Y,
                (__v32qi) __W,
                (__mmask32) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_maskz_multishift_epi64_epi8 (__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
                (__v32qi) __Y,
                (__v32qi)
                _mm256_setzero_si256 (),
                (__mmask32) __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl")))
_mm256_multishift_epi64_epi8 (__m256i __X, __m256i __Y)
{
  return (__m256i) __builtin_ia32_vpmultishiftqb256_mask ((__v32qi) __X,
                (__v32qi) __Y,
                (__v32qi)
                _mm256_undefined_si256 (),
                (__mmask32) -1);
}





# 223 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmi2intrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vbmi2intrin.h" 3 4












static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_compress_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_compress_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_hi(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_compress_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_compress_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_qi(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_compressstoreu_epi16(void *__P, __mmask32 __U, __m512i __D)
{
  __builtin_ia32_compressstorehi512_mask ((__v32hi *) __P, (__v32hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_compressstoreu_epi8(void *__P, __mmask64 __U, __m512i __D)
{
  __builtin_ia32_compressstoreqi512_mask ((__v64qi *) __P, (__v64qi) __D,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_expand_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_expand_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_hi(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_expand_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_expand_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_qi(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_expandloadu_epi16(__m512i __S, __mmask32 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((__const __v32hi *)__P,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_expandloadu_epi16(__mmask32 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((__const __v32hi *)__P,
              (__v32hi) _mm512_setzero_hi(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((__const __v64qi *)__P,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_expandloadu_epi8(__mmask64 __U, void __const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((__const __v64qi *)__P,
              (__v64qi) _mm512_setzero_qi(),
              __U);
}















































































static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shldv_epi64(__m512i __S, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvq512_mask ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shldv_epi64(__mmask8 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvq512_maskz ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shldv_epi64(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvq512_mask ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shldv_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shldv_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvd512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shldv_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shldv_epi16(__m512i __S, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvw512_mask ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shldv_epi16(__mmask32 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvw512_maskz ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shldv_epi16(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshldvw512_mask ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              (__mmask32) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shrdv_epi64(__m512i __S, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvq512_mask ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shrdv_epi64(__mmask8 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvq512_maskz ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shrdv_epi64(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvq512_mask ((__v8di) __S,
              (__v8di) __A,
              (__v8di) __B,
              (__mmask8) -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shrdv_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shrdv_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvd512_maskz ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shrdv_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvd512_mask ((__v16si) __S,
              (__v16si) __A,
              (__v16si) __B,
              (__mmask16) -1);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_mask_shrdv_epi16(__m512i __S, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvw512_mask ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_maskz_shrdv_epi16(__mmask32 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvw512_maskz ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2")))
_mm512_shrdv_epi16(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vpshrdvw512_mask ((__v32hi) __S,
              (__v32hi) __A,
              (__v32hi) __B,
              (__mmask32) -1);
}







# 227 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlvbmi2intrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512vlvbmi2intrin.h" 3 4











static  __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_setzero_hi(void) {
  return (__m128i)(__v8hi){ 0, 0, 0, 0, 0, 0, 0, 0 };
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_compress_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_compress_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) _mm128_setzero_hi(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_compress_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_compress_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) _mm128_setzero_hi(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_compressstoreu_epi16(void *__P, __mmask8 __U, __m128i __D)
{
  __builtin_ia32_compressstorehi128_mask ((__v8hi *) __P, (__v8hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_compressstoreu_epi8(void *__P, __mmask16 __U, __m128i __D)
{
  __builtin_ia32_compressstoreqi128_mask ((__v16qi *) __P, (__v16qi) __D,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_expand_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_expand_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) _mm128_setzero_hi(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_expand_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_expand_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) _mm128_setzero_hi(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_expandloadu_epi16(__m128i __S, __mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((__const __v8hi *)__P,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_expandloadu_epi16(__mmask8 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((__const __v8hi *)__P,
              (__v8hi) _mm128_setzero_hi(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_expandloadu_epi8(__m128i __S, __mmask16 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((__const __v16qi *)__P,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_expandloadu_epi8(__mmask16 __U, void __const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((__const __v16qi *)__P,
              (__v16qi) _mm128_setzero_hi(),
              __U);
}

static  __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_setzero_hi(void) {
  return (__m256i)(__v16hi){ 0, 0, 0, 0, 0, 0, 0, 0,
                             0, 0, 0, 0, 0, 0, 0, 0 };
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_compress_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_compress_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_hi(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_compress_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_compress_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_hi(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_compressstoreu_epi16(void *__P, __mmask16 __U, __m256i __D)
{
  __builtin_ia32_compressstorehi256_mask ((__v16hi *) __P, (__v16hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_compressstoreu_epi8(void *__P, __mmask32 __U, __m256i __D)
{
  __builtin_ia32_compressstoreqi256_mask ((__v32qi *) __P, (__v32qi) __D,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_expand_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_expand_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_hi(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_expand_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_expand_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_hi(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_expandloadu_epi16(__m256i __S, __mmask16 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((__const __v16hi *)__P,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_expandloadu_epi16(__mmask16 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((__const __v16hi *)__P,
              (__v16hi) _mm256_setzero_hi(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((__const __v32qi *)__P,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_expandloadu_epi8(__mmask32 __U, void __const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((__const __v32qi *)__P,
              (__v32qi) _mm256_setzero_hi(),
              __U);
}





























































































































































static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shldv_epi64(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvq256_mask ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shldv_epi64(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvq256_maskz ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shldv_epi64(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvq256_mask ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shldv_epi64(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvq128_mask ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shldv_epi64(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvq128_maskz ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shldv_epi64(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvq128_mask ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shldv_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shldv_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvd256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shldv_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shldv_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shldv_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvd128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shldv_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shldv_epi16(__m256i __S, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvw256_mask ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shldv_epi16(__mmask16 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvw256_maskz ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shldv_epi16(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshldvw256_mask ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shldv_epi16(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvw128_mask ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shldv_epi16(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvw128_maskz ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shldv_epi16(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshldvw128_mask ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shrdv_epi64(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvq256_mask ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shrdv_epi64(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvq256_maskz ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shrdv_epi64(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvq256_mask ((__v4di) __S,
              (__v4di) __A,
              (__v4di) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shrdv_epi64(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvq128_mask ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shrdv_epi64(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvq128_maskz ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shrdv_epi64(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvq128_mask ((__v2di) __S,
              (__v2di) __A,
              (__v2di) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shrdv_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shrdv_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvd256_maskz ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shrdv_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvd256_mask ((__v8si) __S,
              (__v8si) __A,
              (__v8si) __B,
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shrdv_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shrdv_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvd128_maskz ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shrdv_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvd128_mask ((__v4si) __S,
              (__v4si) __A,
              (__v4si) __B,
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_mask_shrdv_epi16(__m256i __S, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvw256_mask ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_maskz_shrdv_epi16(__mmask16 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvw256_maskz ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm256_shrdv_epi16(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vpshrdvw256_mask ((__v16hi) __S,
              (__v16hi) __A,
              (__v16hi) __B,
              (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_mask_shrdv_epi16(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvw128_mask ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_maskz_shrdv_epi16(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvw128_maskz ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2")))
_mm128_shrdv_epi16(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vpshrdvw128_mask ((__v8hi) __S,
              (__v8hi) __A,
              (__v8hi) __B,
              (__mmask8) -1);
}





# 232 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512pfintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/avx512pfintrin.h" 3 4















              
























              
















































# 236 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/pkuintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/pkuintrin.h" 3 4











static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_rdpkru_u32(void)
{
  return __builtin_ia32_rdpkru();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_wrpkru(unsigned int __val)
{
  return __builtin_ia32_wrpkru(__val);
}




# 240 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/vaesintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/vaesintrin.h" 3 4















static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes")))
 _mm256_aesenc_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenc256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes")))
 _mm512_aesenc_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenc512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes")))
 _mm256_aesdec_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdec256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes")))
 _mm512_aesdec_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdec512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes")))
 _mm256_aesenclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenclast256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes")))
 _mm512_aesenclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenclast512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes")))
 _mm256_aesdeclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdeclast256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes")))
 _mm512_aesdeclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdeclast512((__v8di) __A,
              (__v8di) __B);
}






# 244 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/gfniintrin.h" 1 3 4
# 23 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/gfniintrin.h" 3 4












































































































static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("gfni")))
_mm_gf2p8mul_epi8(__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi((__v16qi) __A,
              (__v16qi) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni")))
_mm_mask_gf2p8mul_epi8(__m128i __S, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128(__U,
              (__v16qi) _mm_gf2p8mul_epi8(__A, __B),
              (__v16qi) __S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni")))
_mm_maskz_gf2p8mul_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_gf2p8mul_epi8((__m128i)_mm_setzero_si128(),
              __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("gfni")))
_mm256_gf2p8mul_epi8(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi((__v32qi) __A,
              (__v32qi) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni")))
_mm256_mask_gf2p8mul_epi8(__m256i __S, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256(__U,
              (__v32qi) _mm256_gf2p8mul_epi8(__A, __B),
              (__v32qi) __S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni")))
_mm256_maskz_gf2p8mul_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_gf2p8mul_epi8((__m256i)_mm256_setzero_si256(),
              __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,gfni")))
_mm512_gf2p8mul_epi8(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi((__v64qi) __A,
              (__v64qi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,gfni")))
_mm512_mask_gf2p8mul_epi8(__m512i __S, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512(__U,
              (__v64qi) _mm512_gf2p8mul_epi8(__A, __B),
              (__v64qi) __S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,gfni")))
_mm512_maskz_gf2p8mul_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_gf2p8mul_epi8((__m512i)_mm512_setzero_qi(),
              __U, __A, __B);
}








# 248 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4



static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand16_step(unsigned short *__p)
{
  return __builtin_ia32_rdrand16_step(__p);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand32_step(unsigned int *__p)
{
  return __builtin_ia32_rdrand32_step(__p);
}


static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand64_step(unsigned long long *__p)
{
  return __builtin_ia32_rdrand64_step(__p);
}




static __inline__ int __attribute__((__always_inline__, __nodebug__))
_bit_scan_forward(int __A) {
  return __builtin_ctz(__A);
}


static __inline__ int __attribute__((__always_inline__, __nodebug__))
_bit_scan_reverse(int __A) {
  return 31 - __builtin_clz(__A);
}



static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u32(void)
{
  return __builtin_ia32_rdfsbase32();
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u64(void)
{
  return __builtin_ia32_rdfsbase64();
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u32(void)
{
  return __builtin_ia32_rdgsbase32();
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u64(void)
{
  return __builtin_ia32_rdgsbase64();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u32(unsigned int __V)
{
  return __builtin_ia32_wrfsbase32(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u64(unsigned long long __V)
{
  return __builtin_ia32_wrfsbase64(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u32(unsigned int __V)
{
  return __builtin_ia32_wrgsbase32(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u64(unsigned long long __V)
{
  return __builtin_ia32_wrgsbase64(__V);
}






# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/rtmintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/rtmintrin.h" 3 4





















static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xbegin(void)
{
  return __builtin_ia32_xbegin();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xend(void)
{
  __builtin_ia32_xend();
}






# 339 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4
# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xtestintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xtestintrin.h" 3 4













static __inline__ int
    __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
    _xtest(void) {
  return __builtin_ia32_xtest();
}


# 340 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/shaintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/shaintrin.h" 3 4















static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha1nexte_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1nexte((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha1msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg1((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha1msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg2((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha256rnds2_epu32(__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_sha256rnds2((__v4si)__X, (__v4si)__Y, (__v4si)__Z);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha256msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg1((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha")))
_mm_sha256msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg2((__v4si)__X, (__v4si)__Y);
}




# 344 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/fxsrintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/fxsrintrin.h" 3 4











/// \brief Saves the XMM, MMX, MXCSR and x87 FPU registers into a 512-byte
///    memory region pointed to by the input parameter \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> FXSAVE </c> instruction.
///
/// \param __p
///    A pointer to a 512-byte memory region. The beginning of this memory
///    region should be aligned on a 16-byte boundary.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave(void *__p)
{
  return __builtin_ia32_fxsave(__p);
}

/// \brief Restores the XMM, MMX, MXCSR and x87 FPU registers from the 512-byte
///    memory region pointed to by the input parameter \a __p. The contents of
///    this memory region should have been written to by a previous \c _fxsave
///    or \c _fxsave64 intrinsic.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> FXRSTOR </c> instruction.
///
/// \param __p
///    A pointer to a 512-byte memory region. The beginning of this memory
///    region should be aligned on a 16-byte boundary.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor(void *__p)
{
  return __builtin_ia32_fxrstor(__p);
}


/// \brief Saves the XMM, MMX, MXCSR and x87 FPU registers into a 512-byte
///    memory region pointed to by the input parameter \a __p.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> FXSAVE64 </c> instruction.
///
/// \param __p
///    A pointer to a 512-byte memory region. The beginning of this memory
///    region should be aligned on a 16-byte boundary.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave64(void *__p)
{
  return __builtin_ia32_fxsave64(__p);
}

/// \brief Restores the XMM, MMX, MXCSR and x87 FPU registers from the 512-byte
///    memory region pointed to by the input parameter \a __p. The contents of
///    this memory region should have been written to by a previous \c _fxsave
///    or \c _fxsave64 intrinsic.
///
/// \headerfile <x86intrin.h>
///
/// This intrinsic corresponds to the <c> FXRSTOR64 </c> instruction.
///
/// \param __p
///    A pointer to a 512-byte memory region. The beginning of this memory
///    region should be aligned on a 16-byte boundary.
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor64(void *__p)
{
  return __builtin_ia32_fxrstor64(__p);
}





# 348 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsaveintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsaveintrin.h" 3 4












static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave(void *__p, unsigned long long __m) {
  return __builtin_ia32_xsave(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor(void *__p, unsigned long long __m) {
  return __builtin_ia32_xrstor(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave64(void *__p, unsigned long long __m) {
  return __builtin_ia32_xsave64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor64(void *__p, unsigned long long __m) {
  return __builtin_ia32_xrstor64(__p, __m);
}





# 352 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsaveoptintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsaveoptintrin.h" 3 4












static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt(void *__p, unsigned long long __m) {
  return __builtin_ia32_xsaveopt(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt64(void *__p, unsigned long long __m) {
  return __builtin_ia32_xsaveopt64(__p, __m);
}





# 356 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsavecintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsavecintrin.h" 3 4












static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec64(__p, __m);
}





# 360 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsavesintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/xsavesintrin.h" 3 4












static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors64(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves64(__p, __m);
}





# 364 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4




# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/cetintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/cetintrin.h" 3 4













static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspd(int __a) {
  __builtin_ia32_incsspd(__a);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspq(unsigned long long __a) {
  __builtin_ia32_incsspq(__a);
}


static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspd(unsigned int __a) {
  return __builtin_ia32_rdsspd(__a);
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspq(unsigned long long __a) {
  return __builtin_ia32_rdsspq(__a);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _saveprevssp() {
  __builtin_ia32_saveprevssp();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rstorssp(void * __p) {
  __builtin_ia32_rstorssp(__p);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssd(unsigned int __a, void * __p) {
  __builtin_ia32_wrssd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrssq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussd(unsigned int __a, void * __p) {
  __builtin_ia32_wrussd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrussq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _setssbsy() {
  __builtin_ia32_setssbsy();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _clrssbsy(void * __p) {
  __builtin_ia32_clrssbsy(__p);
}




# 368 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4





# 1 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/adxintrin.h" 1 3 4
# 22 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/adxintrin.h" 3 4













static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx")))
_addcarryx_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
               unsigned int *__p)
{
  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx")))
_addcarryx_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long  *__p)
{
  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);
}



static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_addcarry_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
              unsigned int *__p)
{
  return __builtin_ia32_addcarry_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_addcarry_u64(unsigned char __cf, unsigned long long __x,
              unsigned long long __y, unsigned long long  *__p)
{
  return __builtin_ia32_addcarry_u64(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_subborrow_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
              unsigned int *__p)
{
  return __builtin_ia32_subborrow_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_subborrow_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long  *__p)
{
  return __builtin_ia32_subborrow_u64(__cf, __x, __y, __p);
}





# 373 "/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/lib/clang/10.0.0/include/immintrin.h" 2 3 4


# 5 "gemm_8x8_blocking_pack_ab_avx.c" 2



# 1 "./util.h" 1



double dclock();
float* allocate_matrix(float*** arr, int n, int m);
void deallocate_matrix(float*** arr, float* arr_data);
void copy_matrix(int, int, float *, float *);
void random_matrix(int, int, float **);
void print_matrix(int, int, float **);
float* get_cached_packed_a(int size);
float* get_cached_packed_b(int size);
void free_cached_packed_data();
double compare_matrices( int, int, float **, float **);


# 8 "gemm_8x8_blocking_pack_ab_avx.c" 2






typedef __m256 f8;

void block_kernel(int m, 
                  int b_i, int b_j, int b_rows, int b_cols,
                  float *a, float *b, float *c[]);
void pack_b_block(int bi, int bj, int brows, int bcols, int unroll_size,
                  float *b[],
                  float *packed_mem);

void pack_a(int m,
            int bi, int b_rows, float *a[], float *packed_a);

void gemm_my(int m, int n, int k, float *a[], float *b[], float *c[]) {
  int b_i, b_j, b_rows, b_cols;
  float *packed_b = get_cached_packed_b(n*k);
  float *packed_a = get_cached_packed_a(m*k);

  for (b_i = 0; b_i < k; b_i += 224 // blocking over B, rows & cols) {
    b_rows = ( (k-b_i)<( 224 // blocking over B, rows & cols) ? (k-b_i): ( 224 // blocking over B, rows & cols) );
    pack_a(m, b_i, b_rows, a, packed_a);
    for (b_j = 0; b_j < n; b_j += 224) {
      b_cols = ( (n-b_j)<( 224) ? (n-b_j): ( 224) );
      
      

      pack_b_block(b_i, b_j, b_rows, b_cols, 8 , b, packed_b);
      block_kernel(m, b_i, b_j, b_rows, b_cols, packed_a, packed_b, c);
      packed_b += (b_rows * b_cols);
    }
    packed_a += (m*b_rows);
  }
}




void pack_a(int m,
            int bi, int b_rows, float *a[], float *packed_a){
  for (int ai = 0; ai < m; ai++) {
    __builtin___memcpy_chk (packed_a,  &a[ai][bi], b_rows*sizeof(float), __builtin_object_size (packed_a, 0));
    packed_a += b_rows;
  }
}




void pack_b_block(int bi, int bj, int brows, int bcols, int unroll_size,
                  float *b[],
                  float *packed_mem) {
  int i, j, ii;
  for (i = bi; i < bi + brows; i += unroll_size){
    for (j = bj; j < bj + bcols; j += unroll_size) {
      for (ii = i; ii < i+unroll_size; ii++){
        __builtin___memcpy_chk (packed_mem,  &b[ii][j], unroll_size * sizeof(float), __builtin_object_size (packed_mem, 0));
        packed_mem += unroll_size;
      }
    }
  }
}





void block_kernel(int m,
                  int b_i, int b_j,
                  int b_rows, int b_cols,
                  float *a, float *b, float *c[]){
  // tiling to 8x8

  // intel x64 should have 32 XMM registers
  f8 c_f8, 
    a_f8, 
    b0_f8, b1_f8, b2_f8, b3_f8, b4_f8, b5_f8, b6_f8, b7_f8; // b[aj:aj+8,:]

  
  
  
  
  
  
  
  
  

  
  
  
  
  
  
  
  
  


  
  
  
  

  
  
        
  
  
  
  
  
  
  

  

  int b_packed_index, bj_shifted, a_packed_index;
  a_packed_index = 0;
  for (int ai = 0; ai < m; ai++) {
    // init block packsize.
    b_packed_index = 0;
    for (int aj = b_i; aj < b_i + b_rows; aj += 8, a_packed_index += 8) {
      for (int bj = 0; bj < b_cols; bj += 8) {
        // initialize c, we don't load value from mem for further parallizing.
        bj_shifted = bj + b_j;
        c_f8 = _mm256_load_ps(&c[ai][bj_shifted]); 
        
        // init b ymm registers
        b0_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b1_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b2_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b3_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b4_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b5_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b6_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;
        b7_f8 = _mm256_load_ps(&b[b_packed_index]); b_packed_index += 8;


        // vectorized multmul
        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+0]);
        c_f8 += _mm256_mul_ps(a_f8, b0_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+1]);
        c_f8 += _mm256_mul_ps(a_f8, b1_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+2]);
        c_f8 += _mm256_mul_ps(a_f8, b2_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+3]);
        c_f8 += _mm256_mul_ps(a_f8, b3_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+4]);
        c_f8 += _mm256_mul_ps(a_f8, b4_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+5]);
        c_f8 += _mm256_mul_ps(a_f8, b5_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+6]);
        c_f8 += _mm256_mul_ps(a_f8, b6_f8); 

        a_f8 = _mm256_broadcast_ss(&a[a_packed_index+7]);
        c_f8 += _mm256_mul_ps(a_f8, b7_f8); 

        // sum up

        _mm256_store_ps(&c[ai][bj_shifted], c_f8);
      }
    }
  }
}

